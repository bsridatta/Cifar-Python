{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DD2424 deep19 VT19-1 Deep Learning in Data Science - Assignment 4\n",
    "\n",
    "### Table of Content\n",
    "\n",
    "\n",
    "\n",
    "[Exercise 1 - 0.1](#0.1) -  Read in the data   \n",
    "[Exercise 1 - 0.2](#0.2) -  Set hyper-parameters & initialize the RNN’s parameters   \n",
    "[Exercise 1 - 0.3](#0.3) -  Synthesize text from your randomly initialized RNN   \n",
    "[Exercise 1 - 0.4](#0.4) -  Implement the [forward](#0.4) with [loss](#loss) & [backward pass](#back) of the back-prop    \n",
    "[Exercise 1 - 0.4 - b](#check) - Check correctness with [numerical gradient](#num)  \n",
    "[Exercise 1 - 0.5](#0.5) - Train your RNN using AdaGrad   \n",
    "[Training output and Loss Graph!](#output)  \n",
    "[Best model writes !!!](#best)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report\n",
    "1. Computed Numerical Gradient and Analytical gradient and check [error](#check) <10^-7\n",
    "2. Training [Graph](#output) with smoothed loss and loss for +2 epochs  \n",
    "3. Evolution of generated [text](#evolution) sampling every 10,000 for first 100,000 steps of training. \n",
    "4. [Passage](#best) of length 1000 characters synthesized from your best model \n",
    "            Loss - 25.548408663992184\n",
    "            Smoothed Loss - 46.77587470482475\n",
    "            Iteration - 126000 \n",
    "   Interesting words relatable to Harry Potter that the best model generated (from over 1000's of trails)- \n",
    "            \n",
    "            harry peter (Close)\n",
    "            harry  (Very often)\n",
    "            hermione (Yes!)\n",
    "            lord  (You-Know-Who)\n",
    "            magic\n",
    "            death     \n",
    "            crooked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image as Img\n",
    "import pickle\n",
    "from scipy import misc\n",
    "\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pprint\n",
    "from ipywidgets import *\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1  0. 1\n",
    "<a id='0.1'></a>\n",
    "### Read in the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1107542 total characters and 54 unique characters in the data.\n"
     ]
    }
   ],
   "source": [
    "book_data = open('goblet_book.txt', 'r').read()\n",
    "book_data = book_data.lower()\n",
    "book_chars = list(set(book_data))\n",
    "data_size, vocab_size = len(book_data), len(book_chars)\n",
    "K = vocab_size\n",
    "print('There are %d total characters and %d unique characters in the data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\t', 1: '\\n', 2: ' ', 3: '!', 4: '\"', 5: \"'\", 6: '(', 7: ')', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '6', 18: '7', 19: '9', 20: ':', 21: ';', 22: '?', 23: '^', 24: '_', 25: 'a', 26: 'b', 27: 'c', 28: 'd', 29: 'e', 30: 'f', 31: 'g', 32: 'h', 33: 'i', 34: 'j', 35: 'k', 36: 'l', 37: 'm', 38: 'n', 39: 'o', 40: 'p', 41: 'q', 42: 'r', 43: 's', 44: 't', 45: 'u', 46: 'v', 47: 'w', 48: 'x', 49: 'y', 50: 'z', 51: '}', 52: 'ü', 53: '•'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(book_chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(book_chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Set hyper-parameters & initialize the RNN’s parameters\n",
    "<a id='0.2' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100 # Hidden states\n",
    "eta = 0.1 #?\n",
    "seq_length = 25\n",
    "sig = 0.01\n",
    "\n",
    "parameters = {}\n",
    "parameters['b'] = np.zeros((m,1))\n",
    "parameters['c'] = np.zeros((K,1))\n",
    "parameters['U'] = np.random.rand(m,K)*sig\n",
    "parameters['W'] = np.random.rand(m,m)*sig\n",
    "parameters['V'] = np.random.rand(K,m)*sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Synthesize text from your randomly initialized RNN\n",
    "<a id='0.3' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Instead of using np.exp(x)/np.sum(np.exp(x))\n",
    "    I decrease the value of x with the max, \n",
    "    it could be any number as it cancels out when equation is expanded\n",
    "    This is to avoid overflow due to exponential increase\n",
    "    \n",
    "    Argumets:\n",
    "    x -- Product W, X\n",
    "    \n",
    "    Returns:\n",
    "    soft -- softmax(W,X)\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis = 0))\n",
    "    soft = e_x / np.sum(e_x, axis = 0)\n",
    "    return soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_seq(h0, x0, n, parameters):\n",
    "    \"\"\"\n",
    "    synthesize a sequence of characters \n",
    "\n",
    "    Arguments:\n",
    "    h0 -- the hidden state at time 0\n",
    "    x0 -- the first (dummy) input vector  (as one-hot)\n",
    "    n --  the length of the sequence you want to generate\n",
    "    parameters -- RNN parameters weights, bias and stuff\n",
    "    \n",
    "    Returns:\n",
    "    Synthesized sequence\n",
    "    \"\"\" \n",
    "    \n",
    "    synthesized_seq = [] #output\n",
    "    ht = h0 # First state \n",
    "    xt = x0 # First one-hot input of the seq \n",
    "    \n",
    "    for i in range(n):\n",
    "        # 1. from x get discrete probability distribution of labels\n",
    "        at = parameters['W'].dot(ht) + parameters['U'].dot(xt) + parameters['b']\n",
    "        ht = np.tanh(at)\n",
    "        ot = parameters['V'].dot(ht) + parameters['c']\n",
    "        pt = softmax(ot)\n",
    "        # 2. Pick one out of them as next input\n",
    "        char_ix = np.random.choice(list(range(K)), p = pt.ravel())\n",
    "        synthesized_seq.append(char_ix)\n",
    "        xt = np.zeros((K, 1)) # one-hot\n",
    "        xt[char_ix] = 1  #xnext from current \n",
    "\n",
    "    return synthesized_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(chars, book_chars):\n",
    "    '''\n",
    "    Convert string to idx and then to one-hot representation\n",
    "    \n",
    "    Arguments:\n",
    "    chars -- string of characters\n",
    "    book_chars -- list of all chars in the whole dataset\n",
    "    \n",
    "    Returns:\n",
    "    hot_chars -- one hot reps of the chars \n",
    "    '''\n",
    "    # One char -> 1 coloum of one-hot of size len(unique chars) i.e K\n",
    "    hot_chars = np.zeros((K, len(chars))) \n",
    "    char_to_ix = { ch:i for i,ch in enumerate(sorted(book_chars)) }\n",
    "    \n",
    "    for i, char in enumerate(chars):\n",
    "        ix = char_to_ix[char]\n",
    "        hot_chars[ix,i] = 1\n",
    "    return hot_chars\n",
    "\n",
    "def to_string(ixs, book_chars):\n",
    "    '''\n",
    "    Convert ixs to string representation\n",
    "    \n",
    "    Arguments:\n",
    "    ixs -- index of each char\n",
    "    book_chars -- list of all chars in the whole dataset\n",
    "    \n",
    "    Returns:\n",
    "    string_seq  \n",
    "    '''\n",
    "    # One char -> 1 coloum of one-hot of size len(unique chars) i.e K\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(sorted(book_chars)) }\n",
    "    chars = []\n",
    "    for ix in ixs:\n",
    "        char = ix_to_char[ix]\n",
    "        chars.append(char)\n",
    "    string_seq = \"\".join(chars)\n",
    "    \n",
    "    return string_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_chars = book_data[:seq_length]\n",
    "Y_chars = book_data[1:seq_length + 1]\n",
    "X = to_one_hot(X_chars, book_chars)\n",
    "Y = to_one_hot(Y_chars, book_chars)\n",
    "\n",
    "h0 = np.zeros((m, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tp1s•,17'}3}bmtl6w:)mr.^}\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_seq = to_string(synthesize_seq(h0, X[:,0].reshape(-1,1), 25, parameters), book_chars)\n",
    "string_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Implement the forward & backward pass of back-prop\n",
    "<a id='0.4' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(h0, X, parameters):\n",
    "    '''\n",
    "    the forward-pass of the back-prop algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    h0 -- Initial hidden state\n",
    "    X -- Input / labelled sequence \n",
    "    \n",
    "    Returns:\n",
    "    cache -- final and intermediary output vectors at each time step needed by the backward-pass of the algorithm \n",
    "    '''\n",
    "    seq_len = X.shape[1]\n",
    "    cache = {}\n",
    "    cache['A'] = np.zeros((m, seq_len)) # a0 to at\n",
    "    cache['H'] = np.zeros((m, seq_len)) # h0 to ht \n",
    "    cache['O'] = np.zeros((X.shape[0], seq_len))\n",
    "    cache['P'] = np.zeros((X.shape[0], seq_len))\n",
    "    h_t = h0\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        cache['A'][:,i] = (parameters['W'].dot(h_t) + parameters['U'].dot(X[:,i].reshape(-1, 1)) + parameters['b']).flatten()\n",
    "        h_t = np.tanh(cache['A'][:,i]).reshape(-1, 1)\n",
    "        cache['H'][:,i] = h_t.flatten()\n",
    "        cache['O'][:,i] = parameters['V'].dot(h_t).flatten() + parameters['c'].flatten()\n",
    "        cache['P'][:,i] = softmax(cache['O'][:,i].reshape(-1, 1))[:,0]\n",
    "    cache['H'] = np.concatenate((h0, cache['H']), axis = 1)\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001101  0.00017123 0.00024448 ... 0.01520924 0.0152407  0.01552264]\n"
     ]
    }
   ],
   "source": [
    "cache = forward_pass(h0, X, parameters)\n",
    "print (np.unique(cache['A']))    #just to check if its not null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Loss\n",
    "<a id='loss' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeCost_CrossEntropy(h0, X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Loss of current state of prediction w.r.t the ground truth\n",
    "    \n",
    "    Arguments:\n",
    "    h0 -- initial hidden state\n",
    "    X -- Input\n",
    "    Y -- Ground truth\n",
    "    parameters -- Weights \n",
    "\n",
    "    Returns:\n",
    "    cost -- sum of crossentropy loss\n",
    "    \"\"\"\n",
    "    #     P = np.reshape(P, (K, -1))\n",
    "#     ground_truth = np.reshape(Y, (K, -1))\n",
    "    P = forward_pass(h0, X, parameters)['P']\n",
    "    \n",
    "    product = np.multiply(Y, P).sum(axis = 0)\n",
    "    \n",
    "    #cross entropy loss - Handling -log0 tending to infinity\n",
    "    product[product == 0] = np.finfo(float).eps    #very low value\n",
    "    crossEntropyLoss = np.sum(-np.log(product)) #.sum() / N # or mean \n",
    "\n",
    "    \n",
    "#     L2_regularization_cost = lambd * (np.power(parameters[\"W1\"], 2).sum()+ \\\n",
    "#                                       np.power(parameters[\"W2\"], 2).sum())\n",
    "    \n",
    "    cost = crossEntropyLoss #+ L2_regularization_cost\n",
    "    return  cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "<a id='back' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X, Y, cache, parameters, clipping = True, clip_max = 5):\n",
    "    '''\n",
    "    the backward-pass of the back-prop algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Input / labelled sequence \n",
    "    Y -- Ground truth of the output seq\n",
    "    cache -- Activation, hidden states, observation, probabilistic distribution of output\n",
    "    \n",
    "    Returns:\n",
    "    grad -- Gradients \n",
    "    '''\n",
    "    grad = {}\n",
    "    \n",
    "    for param in parameters.keys():\n",
    "        grad[param] = np.zeros_like(parameters[param])\n",
    "    _grad = {} # internal gradients H and A which are used to calculate grad W   \n",
    "    _grad['A'] = np.zeros((m, seq_length)) # also grad_a and grad_h\n",
    "\n",
    "    T = Y.shape[1]\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        g = cache['P'][:,t] - Y[:,t] # Derivative with respect to o\n",
    "\n",
    "        # Update gradients\n",
    "        grad['c'][:, 0] += g\n",
    "        grad['V'] += np.outer(g, cache['H'][:,t + 1])\n",
    "\n",
    "        if not (t == T - 1):\n",
    "            grad_h = g.dot(parameters['V']) + grad_a.dot(parameters['W'])\n",
    "        else:\n",
    "            grad_h = g.dot(parameters['V']) \n",
    "        grad_a = grad_h.dot(np.diag(1 - np.tanh(cache['A'][:, t]) ** 2))\n",
    "\n",
    "        _grad['A'][:, t] = grad_a\n",
    "        grad['U'] += np.outer(grad_a, X[:,t])\n",
    "        grad['b'][:,0] += grad_a\n",
    "\n",
    "    for t in range(seq_length):\n",
    "        grad['W'] += np.dot(_grad['A'][:, t].reshape(-1, 1), cache['H'][:, t].reshape(1, -1))\n",
    "\n",
    "    if clipping is True:\n",
    "        for gradient in grad.keys():\n",
    "            np.clip(grad[gradient], -clip_max, clip_max, out=grad[gradient])\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00053904 -0.00053724 -0.00052913 ...  0.00064895  0.00067082\n",
      "  0.00072473]\n"
     ]
    }
   ],
   "source": [
    "grad = backward_pass(X, Y, cache, parameters, clipping = True, clip_max = 5)\n",
    "print (np.unique(grad['W']))    #just to check if its not null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Gradient\n",
    "<a id='num' />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grads_num(X, Y, h0, parameters):\n",
    "    \"\"\"\n",
    "    Numerical gradient descent using finite difference method. \n",
    "    \n",
    "    Arguments:\n",
    "    X -- Input dataset\n",
    "    Y -- Corresponding Ground Truth\n",
    "    parameters:\n",
    "        W, U, V -- Current Weights\n",
    "        b, c -- current bias\n",
    "\n",
    "    Returns:\n",
    "    grad -- for all weights and bias\n",
    "        grad['W']... -- Numerical Gradient of W \n",
    "        grad['b'] -- Numerical Gradient of b\n",
    "    \"\"\"\n",
    "    \n",
    "    grad = {}\n",
    "    \n",
    "    for param in parameters.keys():\n",
    "        grad[param] = np.zeros_like(parameters[param])\n",
    "        \n",
    "    h = 1e-4    #very small number by which you want to vary the params\n",
    "    \n",
    "    for param in parameters:\n",
    "        if param in ['W','U', 'V']:\n",
    "            for i in tqdm(range(parameters[param].shape[0]), desc = param):\n",
    "                for j in range(parameters[param].shape[1]):\n",
    "                    parameters[param][i, j] -= h    #change the weights and see if the cost reduces!\n",
    "                    c1 = ComputeCost_CrossEntropy(h0, X, Y, parameters)\n",
    "                    parameters[param][i,j] += 2 * h\n",
    "                    c2 = ComputeCost_CrossEntropy(h0, X, Y, parameters)\n",
    "                    parameters[param][i, j] -= h    \n",
    "                    grad[param][i, j] = (c2-c1) / (2 * h) #if cost decreases, value negative,\n",
    "                                               #hence add this grad to W, ViceVersa\n",
    "        else:\n",
    "            for i in tqdm(range(parameters[param].shape[0]), desc = param):\n",
    "                parameters[param][i] -= h    #change the weights and see if the cost reduces!\n",
    "                c1 = ComputeCost_CrossEntropy(h0, X, Y, parameters)\n",
    "                parameters[param][i] += 2 * h\n",
    "                c2 = ComputeCost_CrossEntropy(h0, X, Y, parameters)\n",
    "                parameters[param][i] -= h    \n",
    "                grad[param][i] = (c2-c1) / (2 * h) #if cost decreases, value negative,\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3ea70ca3d74d068fdabb3c44e86293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='W', style=ProgressStyle(description_width='initial')), HTML(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7f35231d374468b73c87ed94f19f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='V', max=54, style=ProgressStyle(description_width='initial'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60fccad1e4d416fb47c311af569ee23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='b', style=ProgressStyle(description_width='initial')), HTML(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76360cbd44cd41adb27945bba90e458b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='c', max=54, style=ProgressStyle(description_width='initial'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f4993358ed4f18848f052394614a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='U', style=ProgressStyle(description_width='initial')), HTML(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gradNum = compute_grads_num(X, Y, h0, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(grad, gradNum, epsilon):\n",
    "    \"\"\"\n",
    "    Calculating realtive error between analytically and numerically computed gradient\n",
    "    and checking if they are small. (smaller than epsilon)\n",
    "    \n",
    "    Order of the gradients does not matter\n",
    "    \n",
    "    Arguments:\n",
    "    grad -- Analytical gradient\n",
    "    gradNum -- Numerical gradient\n",
    "    epsilon -- very small value by which gradNum and gradAnly could differ\n",
    "    \n",
    "    Returns:\n",
    "    None -- Prints relative error of gradAnly and gradNum\n",
    "    \"\"\"\n",
    "    for param in parameters:\n",
    "        #Weights\n",
    "        grad1 = grad[param]\n",
    "        grad2 = gradNum[param]\n",
    "        difference = np.linalg.norm(grad1 - grad2)  #Could simply use np.abs(grad1 - grad2).sum()      \n",
    "        summation = np.linalg.norm(grad1) + np.linalg.norm(grad2)  #varitaion - 2.0988034043225143e-08 only\n",
    "        denominator = max(epsilon, summation)    #to avoid division by 0\n",
    "        relative_error = difference / denominator                                                     \n",
    "\n",
    "        if relative_error < 1e-6:\n",
    "            print(u'\\u2714  ' + \"The gradient for \" + param +\" is correct! \" + \n",
    "                  str(relative_error))\n",
    "        else:\n",
    "            print(\"Relative_error for \" + param + \" :\" + str(relative_error))\n",
    "            print(u'\\u2718  ' + \"The gradient is wrong!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='check' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  The gradient for b is correct! 2.0531720652715897e-09\n",
      "✔  The gradient for c is correct! 3.9356187798901504e-10\n",
      "✔  The gradient for U is correct! 6.579877163699171e-09\n",
      "✔  The gradient for V is correct! 2.9858111136044654e-09\n",
      "✔  The gradient for W is correct! 1.3578320328523312e-07\n"
     ]
    }
   ],
   "source": [
    "gradient_check(grad, gradNum, 1e-6)   #check all gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Train your RNN using AdaGrad\n",
    "<a id='0.5' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, h0, parameters, epochs=2):\n",
    "    \"\"\"\n",
    "    Perform training with adagrad!!!\n",
    "    \n",
    "    Arguments:\n",
    "    X,Y -- One-hot rep of whole training data\n",
    "    h0 -- initial hidden state (zero vector)\n",
    "    parameters -- Weights\n",
    "    epochs --  number of epochs to train ¯\\_(ツ)_/¯\n",
    "    \"\"\"\n",
    "    #Set up the canvas to do live plotting of the training info\n",
    "    log = {\n",
    "        \"smooth loss\": [],\n",
    "        \"loss\": [],\n",
    "        \"least loss\": np.infty,\n",
    "        \"best h\": None,\n",
    "        \"best parameters\": {}\n",
    "    }  #using terms cost and loss interchangably\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 3))  #must include %matplotlib notebook\n",
    "    Loss = fig.add_subplot(111)\n",
    "\n",
    "    #plt.ion() #iteractive mode on\n",
    "    plt.subplot(111).set_title(\"Smooth Loss\")\n",
    "    plt.xlabel('steps', axes=Loss)\n",
    "    plt.ylabel('Loss', axes=Loss)\n",
    "    fig.show()\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    total_len = X.shape[1]\n",
    "\n",
    "    adaGrad = {}\n",
    "    for param in parameters:\n",
    "        adaGrad[param] = np.zeros_like(parameters[param])\n",
    "\n",
    "    smooth_loss = ComputeCost_CrossEntropy(h0, X[:, :seq_length],\n",
    "                                           Y[:, :seq_length], parameters)\n",
    "\n",
    "    step = 0\n",
    "    for i in tqdm(range(epochs)):  #progress bar\n",
    "        e = 0\n",
    "        hprev = np.copy(h0)  # as e=0\n",
    "\n",
    "        while e + seq_length <= total_len - 1:  # as long as there is input\n",
    "\n",
    "            X_batch = X[:, e:e + seq_length]\n",
    "            Y_batch = Y[:, e:e + seq_length]\n",
    "\n",
    "            # back-prop\n",
    "            cache = forward_pass(hprev, X_batch, parameters)\n",
    "            grad = backward_pass(X_batch,\n",
    "                                 Y_batch,\n",
    "                                 cache,\n",
    "                                 parameters,\n",
    "                                 clipping=True,\n",
    "                                 clip_max=5)\n",
    "\n",
    "            # adaGrad parameter update\n",
    "            for param in parameters:\n",
    "                # Step 6\n",
    "                # mθ,t` = mθ,t`−1 + g2t`\n",
    "                adaGrad[param] += np.power(grad[param], 2)\n",
    "                # Step 7\n",
    "                #θt`+1 = θt` − (η/√mθ,t` + epsilon) gt`\n",
    "                parameters[param] += -(eta * grad[param]) / np.sqrt(\n",
    "                    adaGrad[param] + epsilon)\n",
    "\n",
    "            # Smoothed loss\n",
    "            loss = ComputeCost_CrossEntropy(hprev, X_batch, Y_batch,\n",
    "                                            parameters)\n",
    "            smooth_loss = .999 * smooth_loss + .001 * loss\n",
    "            \n",
    "            if step > 100:\n",
    "                log[\"smooth loss\"].append(smooth_loss)\n",
    "                log[\"loss\"].append(loss)\n",
    "            \n",
    "            if (loss < log['least loss']):\n",
    "                log['least loss'] = loss\n",
    "                log['best h'] = hprev\n",
    "                log['best parameters'] = parameters\n",
    "                np.save(\"train_log.npy\", log)\n",
    "                # Save Model!!!!\n",
    "            # e>1 hprev the last computed hidden state by the forward pass in the previous iteration\n",
    "            hprev = cache['H'][:, -1].reshape(-1, 1)\n",
    "\n",
    "            # Shift the cursor!\n",
    "            e += seq_length\n",
    "            step += 1\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                #display the log\n",
    "                Loss.plot(log[\"loss\"], 'm', label=\"training\")\n",
    "                Loss.plot(log[\"smooth loss\"], 'c', label=\"training\")\n",
    "                if (step == 0):  #cant get it to work anyother way\n",
    "                    Loss.legend()\n",
    "                    Accuracy.legend()\n",
    "                fig.canvas.draw()  # draw\n",
    "\n",
    "            if step % 500 == 0:\n",
    "                seq = to_string(\n",
    "                    synthesize_seq(hprev, X_batch[:, 0].reshape(-1, 1), 200,\n",
    "                                   parameters), book_chars)\n",
    "                print(\"Step : \", step, \"Smooth loss :\",smooth_loss, '\\n', seq)\n",
    "                print('\\n')\n",
    "\n",
    "            if step % 10000 == 0:\n",
    "                with open('syntesized.txt', 'a+') as f:\n",
    "                    print(\"Step: \", step, file=f)\n",
    "                    print(\"Smooth Loss: \", smooth_loss, file=f)\n",
    "\n",
    "                    seq = to_string(\n",
    "                        synthesize_seq(hprev, X_batch[:, 0].reshape(-1, 1),\n",
    "                                       200, parameters), book_chars)\n",
    "                    print(seq, file=f)\n",
    "                    print(\"\\n\", file=f)\n",
    "            \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "X_chars = book_data[:len(book_data)-2]\n",
    "Y_chars = book_data[1:len(book_data)-1]\n",
    "X = to_one_hot(X_chars, book_chars)\n",
    "Y = to_one_hot(Y_chars, book_chars)\n",
    "h0 = np.zeros((m, 1))\n",
    "\n",
    "book_chars = list(set(book_data))\n",
    "\n",
    "epsilon = 1e-8\n",
    "m = 100 # Hidden states\n",
    "eta = 0.1 #?\n",
    "seq_length = 25\n",
    "sig = 0.01\n",
    "\n",
    "parameters = {}\n",
    "parameters['b'] = np.zeros((m,1))\n",
    "parameters['c'] = np.zeros((K,1))\n",
    "parameters['U'] = np.random.rand(m,K)*sig\n",
    "parameters['W'] = np.random.rand(m,m)*sig\n",
    "parameters['V'] = np.random.rand(K,m)*sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 7\n",
    "train_log = train(X, Y, h0, parameters, epochs=epochs) \n",
    "Image(\"train plot 126k steps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evolution' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of Text sampling every 10,000 for first 100,000 steps of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step:  10000  \n",
    "Smooth Loss:  52.970881146613905  \n",
    "thevls asleying angece cirove hoatsoroun learidly antt the shad buin tory,  urkettle, ak gog, sard a-som burrby midit is tow rrfplyoraminvid, mampo?\" dolling so teren fyos rup thr.\n",
    "\" ind, i srurt herr\n",
    "\n",
    "\n",
    "Step:  10000  \n",
    "Smooth Loss:  54.744094131001305  \n",
    "roalge hal lyllanl, ru?fikl rdon, wnoulk abdy bdand mrris uhe cay, thare thiabot wheuron thef tore breots. rry the eh trarul gol sins shee lle ling the roget on pri peng, are boving mesing askad cadft\n",
    "\n",
    "\n",
    "Step:  20000   \n",
    "Smooth Loss:  52.26947371263958  \n",
    " sartor wagd ghike beats simoktis-e, singe to ssluid sunt i'butht bope itf baouer bisk - a the the waced esteant les ak, har he .\"\n",
    "\"m atky . .  hans her'sisde k and kard louged he whe fumi on's in's f\n",
    "\n",
    "\n",
    "Step:  30000 \n",
    "Smooth Loss:  50.965779813453  \n",
    "at hamusc, withevede igouss gow fotmen mer tandiul, wicf tha hohg malig and onyy ttis oped thooes od itf he thorilst, ang sarlomy shat herry in  hack squng ther let the as sryand dumet gerwan warle su\n",
    "\n",
    "\n",
    "Step:  40000  \n",
    "Smooth Loss:  49.81367501514544  \n",
    "hos tamked- gottestint, ing, fanthe to ser wobes, to war shourny ling amamly morlt. horrellly bosteosingt havly seimed mats ald stored sslakewnged, noufvelly mintly.\" \"outhe ther yout mistenme jusrwag\n",
    "\n",
    "\n",
    "Step:  50000  \n",
    "Smooth Loss:  50.643127865800025  \n",
    "suwh toustmas.  nod iche to horest. his'n agrel the samrodiy'lt beazy hes prows srowceny son the dirt topistlly.  he lertesten,\" tuce coselre, t\"ase gop.   nod hepertalsebe, to far teor -s, brate hars\n",
    "\n",
    "\n",
    "Step:  60000  \n",
    "Smooth Loss:  49.529689141300615  \n",
    "on thiy louss of sas evers.  \"exupled harryenln ther asker ather a drace so geifes sarl.\n",
    "\" \"bach sit soid\"\n",
    "\"blapifls, sherowt.  \"ke sowont serriisiols therigh'r homer veare, the the gras was mlen hes \n",
    "\n",
    "\n",
    "Step:  70000  \n",
    "Smooth Loss:  49.08102524969884  \n",
    "has bastond siink fixe dere saredf hor, nibed, rood shears noge higry hadry'y teyis a sked wokef to ro jinnent thaxe wning arwo harrobsiledty har whe spookerrmy it . . . yot the mingantun fond doors h\n",
    "\n",
    "\n",
    "Step:  80000  \n",
    "Smooth Loss:  47.75493535593699  \n",
    "ky butt and prome slougre. . . . . .\". horre, thanes; . upwaid soich loot regast mariml hoir roat as worc of can at nof crouched ever wold rine aldeetein his spenone casefring to sof horrile sarred- g\n",
    "\n",
    "\n",
    "Step:  90000  \n",
    "Smooth Loss:  47.86315262079798  \n",
    "y blitsed.\n",
    "\tnbuize, a\tcssitne ovanne not.  toif sseadlinne he he whind stilen hay the exsus of a crund.  it... i ghaig ost at compe he bedt a modent scaser was samews.\n",
    "\"\t\"a suth harry aid ron aveer cw\n",
    "\n",
    "\n",
    "Step:  100000  \n",
    "Smooth Loss:  48.75981124130462\n",
    "laturbut on agrones ougs, but neck thandlt nobl ware to sceads, the yoik hisce, bull the wet hand mors, and becomey and ichon arans  heo, as the ghard puok'm..\n",
    "\"weat croune he for, mole and zhoanatly \n",
    "\n",
    "\n",
    "Step:  110000  \n",
    "Smooth Loss:  47.22446744571248\n",
    "tous maseref snool-y od for audnem to hakry as ang it git, was oling if contted yin his to har?\" \"hon, you've joucwed some handy. . . yon ge stond nacisesoun toouspioneto belich he spond, as the mutt \n",
    "\n",
    "\n",
    "Step:  120000  \n",
    "Smooth Loss:  47.23760778776166\n",
    " pign theirs.\n",
    "\"ous aboun hal ands to to to she posni , tadloo was an ort fight torcoutly fils fe dechon's - shat and ifbear sain  bermoke antming their lepteing fist you he fremainden bageccy cameacar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='best' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Writes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep one hot hit bect thaad an thas gidh weved briae a betching the death fwer the hech abubl, the'; the gharring the harmsed mut to vtont notw crooked he suad lich, nones was as nood thizam harry vereennnacinghed to the could pot,\" snoorsunf stond ere or be harry, the cark powns.\n",
      "\"eotcit.  he shasslemand harrdutes. \"luffrome fware!\"  had his dentt theppant ordi led himmeny be to intanted termait armorring und reat he he cuntedly, arkiwing when heat and nully ther apave alssed ind.\n",
      "hadlly aid wovend ite slaum. \"ain cuppye pinichind dow her cligh fircne the emantly.\" harting in havessione freate!  ge butiatdo seall, bug lacchean demrrotkascargred hald tor the wheroounffor the and of thind nower to ssiits ughod hin harry.  a poby shic haver kantarswrrack wagh dook, they he the sed. \"ow.  one mow pard nood.\n",
      "\"ot thit ux the beting con.\n",
      "he whick there. \"it clattergicert sat goods, hombbepcled trowas mapint of at himkeyids it wive the wallic!\"\n",
      "\"med  higny her, finns ugh sulby.\n",
      "he and use hil b\n"
     ]
    }
   ],
   "source": [
    "# load from npy\n",
    "trained_log = np.load(\"train_log.npy\")\n",
    "best_parameters = trained_log.item().get('best parameters')\n",
    "best_h = trained_log.item().get('best h')\n",
    "seq = to_string(synthesize_seq(best_h, X[:, 0].reshape(-1, 1),1000, best_parameters), book_chars)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.548408663992184"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_log.item().get('least loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_parameters = train_log['best parameters']\n",
    "# best_h = train_log['best h']\n",
    "# seq = to_string(synthesize_seq(best_h, X[:, 0].reshape(-1, 1),1000, best_parameters), book_chars)\n",
    "# print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac749370008e4bc3ab6fd3bfe6edcce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harry\n",
      "hermione\n",
      "magic\n",
      "\n",
      "{'slytherin': 0, 'gryffindor': 0, 'hogwarts': 0, 'hermione': 55, 'dumbledore': 0, 'magic': 13, 'voldemort': 0, 'harry': 1000}\n"
     ]
    }
   ],
   "source": [
    "words_of_interest = {\n",
    "    \"hermione\" : 0, \"harry\" : 0, \"dumbledore\" : 0, \"magic\" : 0, \"slytherin\" : 0, \"gryffindor\" : 0,\n",
    "    \"voldemort\": 0, \"hogwarts\" : 0, \"spell\" :0,\n",
    "}\n",
    "for i in tqdm(range(1000)):\n",
    "    seq = to_string(\n",
    "        synthesize_seq(best_h, X[:, 0].reshape(-1, 1), 10000, best_parameters),\n",
    "        book_chars)\n",
    "    for x in words_of_interest.keys(): \n",
    "        if x in seq: \n",
    "            if words_of_interest[x] == 0:\n",
    "                print(x)\n",
    "            words_of_interest[x] +=1\n",
    "print(words_of_interest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
