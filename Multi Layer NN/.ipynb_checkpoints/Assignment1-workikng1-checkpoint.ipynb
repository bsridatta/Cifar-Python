{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from utils_assignment1 import LoadBatch, show_image, update_params\n",
    "import pickle\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "from IPython.html.widgets import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Training a multi-linear classifier\n",
    "\n",
    "#### Section 1\n",
    "\n",
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data ✓\n",
      "Image data to Double ✓\n",
      "Standardizing image ✓\n",
      "one hot encoding ✓\n",
      "Reshaping ✓\n"
     ]
    }
   ],
   "source": [
    "#get the data\n",
    "train_set_x, train_set_y, valid_set_x, valid_set_y, test_set_x, test_set_y, classes = LoadBatch('cifar-10')\n",
    "\n",
    "#reshaping labels\n",
    "train_x = train_set_x.transpose()\n",
    "valid_x = valid_set_x.transpose()\n",
    "test_x  = test_set_x.transpose()\n",
    "train_y = train_set_y.transpose()\n",
    "valid_y = valid_set_y.transpose()\n",
    "test_y  = test_set_y.transpose()\n",
    "print(\"Reshaping \" +u'\\u2713' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2\n",
    "\n",
    "## Sneak peak  into the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 10000\n",
      "Number of valid examples: 10000\n",
      "Number of testing examples: 10000\n",
      "Each image is of size: 3072\n",
      "train_x shape: (3072, 10000)\n",
      "train_y shape: (10, 10000)\n",
      "valid_x shape: (3072, 10000)\n",
      "valid_y shape: (10, 10000)\n",
      "test_x shape: (3072, 10000)\n",
      "test_y shape: (10, 10000)\n",
      "classes shape :(10,)\n"
     ]
    }
   ],
   "source": [
    "num_train = train_x.shape[1]\n",
    "num_valid = valid_x.shape[1]\n",
    "num_test = test_x.shape[1]\n",
    "num_px = train_x.shape[0]\n",
    "\n",
    "\n",
    "print (\"Number of training examples: \" + str(num_train))\n",
    "print (\"Number of valid examples: \" + str(num_valid))\n",
    "print (\"Number of testing examples: \" + str(num_test))\n",
    "print (\"Each image is of size: \"+str(num_px))\n",
    "print (\"train_x shape: \" + str(train_x.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"valid_x shape: \" + str(valid_x.shape))\n",
    "print (\"valid_y shape: \" + str(valid_y.shape))\n",
    "print (\"test_x shape: \" + str(test_x.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))\n",
    "print (\"classes shape :\"+str(classes.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out some pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a truck\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index=1\n",
    "show_image(train_set_x,index)\n",
    "#using train_set_x keeping the show_image dimensions in mind\n",
    "label=np.where(train_y[:,index]==1)\n",
    "print (\"It is a \"+str(classes[label][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2\n",
    "\n",
    "\n",
    "\n",
    "## Paramater Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number for images to train\n",
    "N=10000\n",
    "#Number of possible predictions\n",
    "K=len(classes)\n",
    "#Dimensions of the input image\n",
    "d=num_px\n",
    "#Subset of data to be trained on\n",
    "X=train_x[:,:N]\n",
    "Y=train_y[:,:N]\n",
    "#Gausssian parameters \n",
    "mu=0\n",
    "sigma=0.01\n",
    "W = np.random.normal(loc=mu,scale=sigma,size=(K,d)) \n",
    "b = np.random.normal(loc=mu,scale=sigma,size=(K,1))\n",
    "\n",
    "lambd=0\n",
    "GDparams={}\n",
    "#n_batch=batch_size NOT the number\n",
    "GDparams['n_batch']=100\n",
    "GDparams['eta']=0.001\n",
    "GDparams['n_epochs']=200\n",
    "\n",
    "#Get the shapes right\n",
    "assert(X.shape == (d, N))\n",
    "assert(Y.shape == (K, N))\n",
    "assert(W.shape == (K, d))\n",
    "assert(b.shape == (K, 1))\n",
    "assert(classes.size==10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3\n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    #Instead of using np.exp(x)/np.sum(np.exp(x))\n",
    "    #I decrease the value of x with the max, \n",
    "    #it could be any number as it cancels out when equation is expanded\n",
    "    #This is to avoid overflow due to exponential increase\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    soft= e_x / np.sum(e_x,axis=0)\n",
    "    return soft\n",
    "\n",
    "def EvaluateClassifier(X, W, b):\n",
    "    \"\"\"\n",
    "    s = W x + b\n",
    "    p = SOFTMAX (s) \n",
    "    \"\"\"\n",
    "    s = np.dot(W,X) + b\n",
    "    P = softmax(s)\n",
    "    #Dimensional check\n",
    "    assert(X.shape == (d,N))\n",
    "    assert(P.shape == (K,N))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Prediction')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/pandas/core/computation/check.py:17: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Ground Truth')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Correct Matches')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Differences')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# N set to 100\n",
    "P=EvaluateClassifier(X,W,b)\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "# plt.subplot(221).set_title(\"Probabilities\")\n",
    "# ax = sns.heatmap(P)\n",
    "plt.subplot(222).set_title(\"Prediction\")\n",
    "predictions=(P == P.max(axis=0, keepdims=1)).astype(float)\n",
    "ax = sns.heatmap(predictions)\n",
    "plt.subplot(221).set_title(\"Ground Truth\")\n",
    "ax=sns.heatmap(Y)\n",
    "plt.subplot(224).set_title(\"Correct Matches\")\n",
    "matches=(np.multiply(Y,predictions)).astype(float)\n",
    "ax=sns.heatmap(matches)\n",
    "plt.subplot(223).set_title(\"Differences\")\n",
    "matches= Y+predictions\n",
    "ax=sns.heatmap(matches)\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 4\n",
    "\n",
    "## Computing the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeCost_CrossEntropy(P,Y,W):\n",
    "    \n",
    "    #instead of using function J = ComputeCost(X, Y, W, b, lambda)\n",
    "    # and calculating P again lets use it from the from the prev. output\n",
    "    \"\"\"\n",
    "    P - Softmax predictions\n",
    "    Y - Ground Truth\n",
    "    \"\"\"\n",
    "    #cross entropy loss - Handling -log0 tending to infinity\n",
    "    #crossEntropyLoss =  -1*np.log(np.matmul(Y,P))\n",
    "    D = Y.shape[1]\n",
    "\n",
    "    crossEntropyLoss = -1*(np.multiply(Y, np.log(P)) + np.multiply(1 - Y, np.log(1 - P)))\n",
    "    #L2 regularization term\n",
    "    L2_regularization_cost = lambd * np.sum(np.square(W)) \n",
    "    Jacobian = np.sum(crossEntropyLoss)/float(D) + L2_regularization_cost\n",
    "    cost = np.squeeze(Jacobian) # just to make sure the output is a value and not an array\n",
    "    assert(Y.shape == (K,N))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost :3.308936409770906\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cost_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-06ec73a4be4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cost :\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_CE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcost_Check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcost_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcost_CE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mComputeCost_CrossEntropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mcost_Check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cost_1' is not defined"
     ]
    }
   ],
   "source": [
    "cost_CE=ComputeCost_CrossEntropy(P,Y,W)\n",
    "print(\"cost :\"+str(cost_CE))\n",
    "\n",
    "cost_Check=cost_1(P,Y)\n",
    "cost_CE=ComputeCost_CrossEntropy(P,Y,W)\n",
    "print cost_Check\n",
    "print cost_CE\n",
    "print cost_CE-cost_Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 5\n",
    "\n",
    "## Accuracy of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccuracy(P,Y):\n",
    "    #instead of using function acc = ComputeAccuracy(X, y, W, b)\n",
    "    # and calculating P again lets use it from the from the prev. output\n",
    "    predictions=np.argmax(P,axis=0)\n",
    "    groundtruth=np.argmax(Y,axis=0)\n",
    "    matches=np.sum(predictions==groundtruth)\n",
    "    total=len(predictions)\n",
    "    accuracy=(matches/float(total))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :0.07\n"
     ]
    }
   ],
   "source": [
    "accuracy=ComputeAccuracy(P,Y)\n",
    "print (\"Accuracy :\"+ str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 6\n",
    "\n",
    "## Computing the Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradients(X, Y, P, W, lambd):\n",
    "    \"\"\" \n",
    "    G -- dJ/dZ - g for Batch\n",
    "    Z -- WX\n",
    "    grad_W1 -- dJ/dW\n",
    "    grad_b -- dJ/db\n",
    "    grad_L -- dJ/dL Regularization term\n",
    "    grad_W -- dJ/dW + Regularization term\n",
    "\n",
    "    lambd - lambda, to differ from the keyword itself\n",
    "    \"\"\"\n",
    "    N=float(X.shape[1])\n",
    "    G= -1*(Y-P)\n",
    "    grad_W1= (1/N)*np.dot(G,X.T)\n",
    "    grad_L = 2*lambd*W\n",
    "    grad_W = grad_W1+grad_L\n",
    "    grad_b = (1/N)*np.dot(G,np.ones((int(N),1)))\n",
    "    grad_b = np.sum(G, axis=1, keepdims=True)/ N\n",
    "    \n",
    "    print grad_b.shape\n",
    "    assert(grad_W.shape == (K,d))\n",
    "    assert(grad_b.shape == (K,1))\n",
    "    \n",
    "    return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "grad_W,grad_b=ComputeGradients(X,Y,P,W,lambd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNum(X, Y, W, b):\n",
    "#function [grad_b, grad_W] = ComputeGradsNum(X, Y, W, b, lambda, h) \n",
    "    h=float(1e-6)\n",
    "    grad_W = np.zeros_like(W)\n",
    "    grad_b = np.zeros_like(b)\n",
    "    \n",
    "    P=EvaluateClassifier(X,W,b)\n",
    "    #c = ComputeCost_CrossEntropy(P,Y,W)\n",
    "    c = cost_1(P,Y)\n",
    "    \n",
    "    for i in range(len(b)):\n",
    "        b_try = b\n",
    "        b_try[i] = b_try[i] + h\n",
    "        P=EvaluateClassifier(X,W,b_try)\n",
    "        #c2 = ComputeCost_CrossEntropy(P,Y,W)\n",
    "        c2 = cost_1(P,Y)\n",
    "        grad_b[i] = (c2-c) / h\n",
    "        \n",
    "    for i,j in np.ndindex(W.shape):\n",
    "        W_try = W\n",
    "        W_try[i,j] = W_try[i,j] + h\n",
    "        P=EvaluateClassifier(X,W_try,b)\n",
    "        #change params for W and b\n",
    "        #c2 = ComputeCost_CrossEntropy(P,Y,W)\n",
    "        c2 = cost_1(P,Y)\n",
    "        grad_W[i,j] = (c2-c) / h\n",
    "        \n",
    "    return grad_W,grad_b\n",
    "\n",
    "\n",
    "def gradient_check(X,Y,W,b, epsilon=1e-6):\n",
    "\n",
    "    Wplus = W + epsilon                               \n",
    "    Wminus = W - epsilon  \n",
    "    P=EvaluateClassifier(X,Wplus,b)\n",
    "    #J_plus = ComputeCost_CrossEntropy(P,Y,Wplus)\n",
    "    J_plus = cost_1(P,Y)\n",
    "    \n",
    "    P=EvaluateClassifier(X,Wminus,b)\n",
    "    #J_minus = ComputeCost_CrossEntropy(P,Y,Wminus)\n",
    "    J_minus = cost_1(P,Y)\n",
    "    \n",
    "    gradapprox = (J_plus - J_minus) / (2 * epsilon)           \n",
    "    gradWnum,gradbnum=ComputeGradsNum(X,Y,W,b)\n",
    "    #gradapprox=grad_w_num\n",
    "    grad,grad_b =ComputeGradients(X,Y,P,W,lambd)\n",
    "    \n",
    "    numerator = np.linalg.norm(grad - gradapprox)                      \n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)    \n",
    "    difference = numerator / denominator                               \n",
    "    \n",
    "    if difference < 1e-6:\n",
    "        print(\"The gradient is correct!\")\n",
    "    else:\n",
    "        print(\"The gradient is wrong!\")\n",
    "        print(\"Variation of :\"+str(difference))\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'cost_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-91d96fa20682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgradient_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-ff5d4df5d553>\u001b[0m in \u001b[0;36mgradient_check\u001b[0;34m(X, Y, W, b, epsilon)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEvaluateClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWplus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#J_plus = ComputeCost_CrossEntropy(P,Y,Wplus)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mJ_plus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEvaluateClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWminus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'cost_1' is not defined"
     ]
    }
   ],
   "source": [
    "gradient_check(X,Y,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax1(x):\n",
    "    try:\n",
    "        e = np.exp(x)\n",
    "        return e / np.sum(e, axis=0)\n",
    "    except FloatingPointError:\n",
    "        # What happens here is that we get some of the values in x that are too big\n",
    "        # so the exp overflows. In that case we check column by column where the\n",
    "        # problem is\n",
    "        return softmax_column_by_column(x)\n",
    "\n",
    "def softmax_column_by_column( x):\n",
    "    cols = x.shape[1]\n",
    "    res = np.finfo(float).eps * np.ones_like(x)\n",
    "    for c in range(cols):\n",
    "        try:\n",
    "            e = np.exp(x[:, c])\n",
    "            res[:, c] = e / np.sum(e, axis=0)\n",
    "        except FloatingPointError:\n",
    "            res[np.argmax(x[:, c]), :] = 1 - np.finfo(float).eps\n",
    "    return res\n",
    "\n",
    "def evaluate(input_data):\n",
    "    \"\"\"\n",
    "    input_data must be a matrix containing one input per column (dimensions: input_size * N)\n",
    "\n",
    "    output will be:\n",
    "    - a matrix containing the probabilities of each class, one image per column \n",
    "      (dimensions: output_size * N)\n",
    "    - a row vector containing the numerical index of the class with the highest probability,\n",
    "      one image per column (dimensions: 1 * N)\n",
    "    \"\"\"\n",
    "    X = np.reshape(input_data, (d, -1))\n",
    "    probabilities = softmax1(np.dot(W, X) + b)\n",
    "    predictions = np.argmax(probabilities, axis=0)\n",
    "    return probabilities, predictions\n",
    "\n",
    "def cost_1( probabilities, ground_truth):\n",
    "    \"\"\"\n",
    "    probabilities should be the same as the first output of the evaluate function\n",
    "    (dimensions: output_size * N)\n",
    "\n",
    "    ground_truth is matrix containing the one-hot encoded true labels, one image per column\n",
    "    (dimensions: output_size * N)\n",
    "\n",
    "    l is a scalar used as a weight in the regularization term\n",
    "    \"\"\"\n",
    "    N = probabilities.shape[1]\n",
    "    probabilities = np.reshape(probabilities, (K, -1))\n",
    "    ground_truth = np.reshape(ground_truth, (K, -1))\n",
    "\n",
    "    log_arg = np.multiply(ground_truth, probabilities).sum(axis=0)\n",
    "    log_arg[log_arg == 0] = np.finfo(float).eps\n",
    "\n",
    "    return - np.log(log_arg).sum() / N + lambd * np.power(W, 2).sum()\n",
    "\n",
    "def accuracy(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    predictions should be the same as the second output of the evaluate function\n",
    "    (dimensions: N * 1)\n",
    "\n",
    "    ground_truth is matrix containing the one-hot encoded true labels, one image per column\n",
    "    (dimensions: output_size * N)\n",
    "    \"\"\"\n",
    "    N = predictions.shape[0]\n",
    "    ground_truth = np.argmax(ground_truth, axis=0)\n",
    "    return np.sum(predictions == ground_truth) / N\n",
    "\n",
    "def gradients_slides( input_data, probabilities, ground_truth):\n",
    "    N = probabilities.shape[1]\n",
    "\n",
    "    grad_w = np.zeros_like(W)\n",
    "    grad_b = np.zeros_like(b)\n",
    "\n",
    "    for i in range(N):\n",
    "        x = input_data[:, i]\n",
    "        y = ground_truth[:, i]\n",
    "        p = probabilities[:, i]\n",
    "\n",
    "        # The actual formula, which I broke down for ease of debuggin\n",
    "        # g = - np.dot(y, (np.diag(p) - np.outer(p, p))) / np.dot(y, p)\n",
    "\n",
    "        t1 = np.outer(p, p)\n",
    "        t2 = np.dot(y, (np.diag(p) - t1))\n",
    "        t3 = np.dot(y, p)\n",
    "        if t3 == 0:\n",
    "            t3 = np.finfo(float).eps\n",
    "        g = - t2 / t3\n",
    "\n",
    "        grad_w += np.outer(g, x)\n",
    "        grad_b += np.reshape(g, grad_b.shape)\n",
    "\n",
    "    grad_w = grad_w / N + 2 * lambd * W\n",
    "    grad_b /= N\n",
    "\n",
    "    return grad_w, grad_b\n",
    "\n",
    "def gradients_simple( input_data, probabilities, ground_truth):\n",
    "    N = probabilities.shape[1]\n",
    "\n",
    "    grad_w = np.zeros_like(W)\n",
    "    grad_b = np.zeros_like(b)\n",
    "\n",
    "    for i in range(N):\n",
    "        x = input_data[:, i]\n",
    "        y = ground_truth[:, i]\n",
    "        p = probabilities[:, i]\n",
    "\n",
    "        g = p - y\n",
    "\n",
    "        grad_w += np.outer(g, x)\n",
    "        grad_b += np.reshape(g, grad_b.shape)\n",
    "\n",
    "    grad_w = grad_w / N + 2 * lambd * W\n",
    "    grad_b /= N\n",
    "\n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2184697695261093e-14\n"
     ]
    }
   ],
   "source": [
    "#probs check\n",
    "\n",
    "probabilities, predictions=evaluate(X)\n",
    "print np.sum(np.abs(probabilities-P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3544753109271412\n",
      "3.308936409770906\n",
      "0.954461098843765\n"
     ]
    }
   ],
   "source": [
    "cost_Check=cost_1(P,Y)\n",
    "cost_CE=ComputeCost_CrossEntropy(P,Y,W)\n",
    "print cost_Check\n",
    "print cost_CE\n",
    "print cost_CE-cost_Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997984874705198\n"
     ]
    }
   ],
   "source": [
    "grad_w_num,grad_b_num=ComputeGradsNum(X,Y,W,b)\n",
    "difference=np.sum(np.abs(grad_W-grad_w_num))\n",
    "total=np.sum(np.abs(grad_W)+np.abs(grad_w_num))\n",
    "epsilon=np.finfo(float).eps\n",
    "\n",
    "print difference/max(epsilon,total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "Step 0: Cost:3.308936409770906 Accuracy: 0.07\n",
      "(10, 1)\n",
      "Step 1: Cost:3.291247277444087 Accuracy: 0.07\n",
      "(10, 1)\n",
      "Step 2: Cost:3.2756961435082554 Accuracy: 0.09\n",
      "(10, 1)\n",
      "Step 3: Cost:3.261969992059477 Accuracy: 0.14\n",
      "(10, 1)\n",
      "Step 4: Cost:3.2498098058077924 Accuracy: 0.13\n",
      "(10, 1)\n",
      "Step 5: Cost:3.2389988184462664 Accuracy: 0.14\n",
      "(10, 1)\n",
      "Step 6: Cost:3.229353853259543 Accuracy: 0.14\n",
      "(10, 1)\n",
      "Step 7: Cost:3.220718857103496 Accuracy: 0.16\n",
      "(10, 1)\n",
      "Step 8: Cost:3.212960009726294 Accuracy: 0.14\n",
      "(10, 1)\n",
      "Step 9: Cost:3.2059619712444682 Accuracy: 0.12\n",
      "(10, 1)\n",
      "Step 10: Cost:3.199624956788534 Accuracy: 0.15\n",
      "(10, 1)\n",
      "Step 11: Cost:3.1938624160504117 Accuracy: 0.16\n",
      "(10, 1)\n",
      "Step 12: Cost:3.1885991586779223 Accuracy: 0.16\n",
      "(10, 1)\n",
      "Step 13: Cost:3.1837698118589945 Accuracy: 0.16\n",
      "(10, 1)\n",
      "Step 14: Cost:3.1793175291113154 Accuracy: 0.18\n",
      "(10, 1)\n",
      "Step 15: Cost:3.175192892732807 Accuracy: 0.17\n",
      "(10, 1)\n",
      "Step 16: Cost:3.1713529690518034 Accuracy: 0.18\n",
      "(10, 1)\n",
      "Step 17: Cost:3.1677604873538807 Accuracy: 0.17\n",
      "(10, 1)\n",
      "Step 18: Cost:3.1643831215047875 Accuracy: 0.17\n",
      "(10, 1)\n",
      "Step 19: Cost:3.161192858851389 Accuracy: 0.17\n",
      "(10, 1)\n",
      "Step 20: Cost:3.158165444727582 Accuracy: 0.17\n",
      "(10, 1)\n",
      "Step 21: Cost:3.1552798933863255 Accuracy: 0.2\n",
      "(10, 1)\n",
      "Step 22: Cost:3.1525180578357777 Accuracy: 0.19\n",
      "(10, 1)\n",
      "Step 23: Cost:3.149864252171765 Accuracy: 0.19\n",
      "(10, 1)\n",
      "Step 24: Cost:3.1473049207741948 Accuracy: 0.19\n",
      "(10, 1)\n",
      "Step 25: Cost:3.1448283493077827 Accuracy: 0.19\n",
      "(10, 1)\n",
      "Step 26: Cost:3.1424244129247128 Accuracy: 0.2\n",
      "(10, 1)\n",
      "Step 27: Cost:3.140084357461112 Accuracy: 0.21\n",
      "(10, 1)\n",
      "Step 28: Cost:3.137800609780101 Accuracy: 0.22\n",
      "(10, 1)\n",
      "Step 29: Cost:3.1355666137561413 Accuracy: 0.22\n",
      "(10, 1)\n",
      "Step 30: Cost:3.133376688724046 Accuracy: 0.22\n",
      "(10, 1)\n",
      "Step 31: Cost:3.1312259075319946 Accuracy: 0.22\n",
      "(10, 1)\n",
      "Step 32: Cost:3.1291099916393694 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 33: Cost:3.127025220984839 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 34: Cost:3.124968356615562 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 35: Cost:3.122936574312945 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 36: Cost:3.1209274076732867 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 37: Cost:3.1189386993025634 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 38: Cost:3.1169685589642286 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 39: Cost:3.115015327677961 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 40: Cost:3.1130775469073035 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 41: Cost:3.111153932096506 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 42: Cost:3.1092433499232253 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 43: Cost:3.1073447987258374 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 44: Cost:3.105457391643382 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 45: Cost:3.1035803420743027 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 46: Cost:3.101712951118444 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 47: Cost:3.0998545967166056 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 48: Cost:3.0980047242443858 Accuracy: 0.23\n",
      "(10, 1)\n",
      "Step 49: Cost:3.096162838353209 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 50: Cost:3.094328495882164 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 51: Cost:3.0925012996903463 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 52: Cost:3.090680893281591 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 53: Cost:3.0888669561122377 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 54: Cost:3.087059199488574 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 55: Cost:3.0852573629740836 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 56: Cost:3.083461211238188 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 57: Cost:3.081670531287874 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 58: Cost:3.079885130031945 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 59: Cost:3.078104832134681 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 60: Cost:3.0763294781217168 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 61: Cost:3.07455892270606 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 62: Cost:3.0727930333065854 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 63: Cost:3.0710316887350415 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 64: Cost:3.0692747780308514 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 65: Cost:3.0675221994257065 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 66: Cost:3.0657738594223356 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 67: Cost:3.064029671973823 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 68: Cost:3.0622895577516456 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 69: Cost:3.0605534434920556 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 70: Cost:3.058821261411765 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 71: Cost:3.057092948685014 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 72: Cost:3.055368446975054 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 73: Cost:3.0536477020139596 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 74: Cost:3.051930663225393 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 75: Cost:3.0502172833855887 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 76: Cost:3.0485075183184036 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 77: Cost:3.0468013266207303 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 78: Cost:3.045098669415043 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 79: Cost:3.0433995101261724 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 80: Cost:3.041703814279775 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 81: Cost:3.0400115493202184 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 82: Cost:3.0383226844458773 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 83: Cost:3.0366371904600515 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 84: Cost:3.0349550396359035 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 85: Cost:3.0332762055940146 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 86: Cost:3.031600663191274 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 87: Cost:3.0299283884199832 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 88: Cost:3.028259358316163 Accuracy: 0.24\n",
      "(10, 1)\n",
      "Step 89: Cost:3.0265935508761608 Accuracy: 0.25\n",
      "(10, 1)\n",
      "Step 90: Cost:3.0249309449807367 Accuracy: 0.25\n",
      "(10, 1)\n",
      "Step 91: Cost:3.023271520325934 Accuracy: 0.25\n",
      "(10, 1)\n",
      "Step 92: Cost:3.0216152573600477 Accuracy: 0.26\n",
      "(10, 1)\n",
      "Step 93: Cost:3.019962137226133 Accuracy: 0.26\n",
      "(10, 1)\n",
      "Step 94: Cost:3.0183121417095253 Accuracy: 0.27\n",
      "(10, 1)\n",
      "Step 95: Cost:3.016665253189887 Accuracy: 0.27\n",
      "(10, 1)\n",
      "Step 96: Cost:3.015021454597378 Accuracy: 0.27\n",
      "(10, 1)\n",
      "Step 97: Cost:3.013380729372559 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 98: Cost:3.011743061429671 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 99: Cost:3.010108435123012 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 100: Cost:3.008476835216096 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 101: Cost:3.0068482468533704 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 102: Cost:3.00522265553425 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 103: Cost:3.003600047089253 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 104: Cost:3.001980407658085 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 105: Cost:3.000363723669457 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 106: Cost:2.998749981822519 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 107: Cost:2.9971391690697624 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 108: Cost:2.9955312726012515 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 109: Cost:2.993926279830088 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 110: Cost:2.992324178378993 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 111: Cost:2.9907249560679166 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 112: Cost:2.989128600902593 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 113: Cost:2.9875351010639526 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 114: Cost:2.985944444898325 Accuracy: 0.28\n",
      "(10, 1)\n",
      "Step 115: Cost:2.9843566209083803 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 116: Cost:2.982771617744724 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 117: Cost:2.9811894241981203 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 118: Cost:2.979610029192262 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 119: Cost:2.9780334217770847 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 120: Cost:2.9764595911225378 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 121: Cost:2.974888526512806 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 122: Cost:2.9733202173409454 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 123: Cost:2.971754653103874 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 124: Cost:2.9701918233977347 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 125: Cost:2.968631717913558 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 126: Cost:2.96707432643323 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 127: Cost:2.965519638825732 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 128: Cost:2.9639676450436343 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 129: Cost:2.96241833511982 Accuracy: 0.29\n",
      "(10, 1)\n",
      "Step 130: Cost:2.960871699164434 Accuracy: 0.3\n",
      "(10, 1)\n",
      "Step 131: Cost:2.959327727362021 Accuracy: 0.3\n",
      "(10, 1)\n",
      "Step 132: Cost:2.9577864099688633 Accuracy: 0.3\n",
      "(10, 1)\n",
      "Step 133: Cost:2.9562477373104823 Accuracy: 0.3\n",
      "(10, 1)\n",
      "Step 134: Cost:2.9547116997793017 Accuracy: 0.3\n",
      "(10, 1)\n",
      "Step 135: Cost:2.9531782878324644 Accuracy: 0.3\n",
      "(10, 1)\n",
      "Step 136: Cost:2.9516474919897835 Accuracy: 0.3\n",
      "(10, 1)\n",
      "Step 137: Cost:2.950119302831821 Accuracy: 0.3\n",
      "(10, 1)\n",
      "Step 138: Cost:2.948593710998092 Accuracy: 0.3\n",
      "(10, 1)\n",
      "Step 139: Cost:2.9470707071853717 Accuracy: 0.3\n",
      "(10, 1)\n",
      "Step 140: Cost:2.9455502821461166 Accuracy: 0.31\n",
      "(10, 1)\n",
      "Step 141: Cost:2.9440324266869684 Accuracy: 0.31\n",
      "(10, 1)\n",
      "Step 142: Cost:2.942517131667366 Accuracy: 0.31\n",
      "(10, 1)\n",
      "Step 143: Cost:2.9410043879982197 Accuracy: 0.31\n",
      "(10, 1)\n",
      "Step 144: Cost:2.9394941866406805 Accuracy: 0.31\n",
      "(10, 1)\n",
      "Step 145: Cost:2.937986518604979 Accuracy: 0.31\n",
      "(10, 1)\n",
      "Step 146: Cost:2.936481374949322 Accuracy: 0.32\n",
      "(10, 1)\n",
      "Step 147: Cost:2.9349787467788646 Accuracy: 0.32\n",
      "(10, 1)\n",
      "Step 148: Cost:2.933478625244736 Accuracy: 0.32\n",
      "(10, 1)\n",
      "Step 149: Cost:2.931981001543122 Accuracy: 0.32\n",
      "(10, 1)\n",
      "Step 150: Cost:2.930485866914398 Accuracy: 0.32\n",
      "(10, 1)\n",
      "Step 151: Cost:2.9289932126423097 Accuracy: 0.32\n",
      "(10, 1)\n",
      "Step 152: Cost:2.9275030300532023 Accuracy: 0.33\n",
      "(10, 1)\n",
      "Step 153: Cost:2.926015310515289 Accuracy: 0.33\n",
      "(10, 1)\n",
      "Step 154: Cost:2.9245300454379595 Accuracy: 0.33\n",
      "(10, 1)\n",
      "Step 155: Cost:2.923047226271126 Accuracy: 0.34\n",
      "(10, 1)\n",
      "Step 156: Cost:2.9215668445046044 Accuracy: 0.35\n",
      "(10, 1)\n",
      "Step 157: Cost:2.9200888916675263 Accuracy: 0.35\n",
      "(10, 1)\n",
      "Step 158: Cost:2.918613359327784 Accuracy: 0.35\n",
      "(10, 1)\n",
      "Step 159: Cost:2.9171402390915007 Accuracy: 0.35\n",
      "(10, 1)\n",
      "Step 160: Cost:2.9156695226025295 Accuracy: 0.35\n",
      "(10, 1)\n",
      "Step 161: Cost:2.914201201541977 Accuracy: 0.37\n",
      "(10, 1)\n",
      "Step 162: Cost:2.912735267627753 Accuracy: 0.38\n",
      "(10, 1)\n",
      "Step 163: Cost:2.911271712614136 Accuracy: 0.38\n",
      "(10, 1)\n",
      "Step 164: Cost:2.909810528291365 Accuracy: 0.38\n",
      "(10, 1)\n",
      "Step 165: Cost:2.908351706485249 Accuracy: 0.38\n",
      "(10, 1)\n",
      "Step 166: Cost:2.906895239056794 Accuracy: 0.38\n",
      "(10, 1)\n",
      "Step 167: Cost:2.9054411179018484 Accuracy: 0.38\n",
      "(10, 1)\n",
      "Step 168: Cost:2.9039893349507633 Accuracy: 0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "Step 169: Cost:2.9025398821680675 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 170: Cost:2.9010927515521603 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 171: Cost:2.899647935135013 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 172: Cost:2.898205424981883 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 173: Cost:2.896765213191047 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 174: Cost:2.895327291893535 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 175: Cost:2.893891653252883 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 176: Cost:2.892458289464889 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 177: Cost:2.891027192757383 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 178: Cost:2.8895983553900084 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 179: Cost:2.8881717696539977 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 180: Cost:2.8867474278719736 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 181: Cost:2.8853253223977493 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 182: Cost:2.8839054456161297 Accuracy: 0.39\n",
      "(10, 1)\n",
      "Step 183: Cost:2.8824877899427306 Accuracy: 0.4\n",
      "(10, 1)\n",
      "Step 184: Cost:2.8810723478237947 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 185: Cost:2.879659111736022 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 186: Cost:2.8782480741863936 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 187: Cost:2.876839227712014 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 188: Cost:2.8754325648799477 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 189: Cost:2.8740280782870666 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 190: Cost:2.872625760559899 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 191: Cost:2.87122560435448 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 192: Cost:2.8698276023562164 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 193: Cost:2.8684317472797374 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 194: Cost:2.867038031868768 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 195: Cost:2.865646448895991 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 196: Cost:2.8642569911629177 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 197: Cost:2.862869651499763 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 198: Cost:2.8614844227653218 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 199: Cost:2.8601012978468443 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 200: Cost:2.8587202696599183 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 201: Cost:2.8573413311483535 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 202: Cost:2.855964475284066 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 203: Cost:2.8545896950669634 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 204: Cost:2.8532169835248373 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 205: Cost:2.8518463337132505 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 206: Cost:2.850477738715432 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 207: Cost:2.849111191642172 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 208: Cost:2.847746685631713 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 209: Cost:2.8463842138496522 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 210: Cost:2.8450237694888356 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 211: Cost:2.8436653457692613 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 212: Cost:2.8423089359379774 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 213: Cost:2.8409545332689845 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 214: Cost:2.8396021310631414 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 215: Cost:2.838251722648065 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 216: Cost:2.836903301378038 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 217: Cost:2.835556860633916 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 218: Cost:2.8342123938230306 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 219: Cost:2.832869894379099 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 220: Cost:2.831529355762134 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 221: Cost:2.8301907714583505 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 222: Cost:2.8288541349800753 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 223: Cost:2.827519439865662 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 224: Cost:2.8261866796793935 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 225: Cost:2.8248558480114037 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 226: Cost:2.8235269384775807 Accuracy: 0.41\n",
      "(10, 1)\n",
      "Step 227: Cost:2.8221999447194865 Accuracy: 0.42\n",
      "(10, 1)\n",
      "Step 228: Cost:2.8208748604042664 Accuracy: 0.42\n",
      "(10, 1)\n",
      "Step 229: Cost:2.8195516792245634 Accuracy: 0.43\n",
      "(10, 1)\n",
      "Step 230: Cost:2.818230394898433 Accuracy: 0.43\n",
      "(10, 1)\n",
      "Step 231: Cost:2.816911001169254 Accuracy: 0.43\n",
      "(10, 1)\n",
      "Step 232: Cost:2.815593491805655 Accuracy: 0.43\n",
      "(10, 1)\n",
      "Step 233: Cost:2.814277860601413 Accuracy: 0.43\n",
      "(10, 1)\n",
      "Step 234: Cost:2.812964101375382 Accuracy: 0.43\n",
      "(10, 1)\n",
      "Step 235: Cost:2.8116522079714037 Accuracy: 0.43\n",
      "(10, 1)\n",
      "Step 236: Cost:2.810342174258226 Accuracy: 0.43\n",
      "(10, 1)\n",
      "Step 237: Cost:2.8090339941294165 Accuracy: 0.43\n",
      "(10, 1)\n",
      "Step 238: Cost:2.8077276615032862 Accuracy: 0.43\n",
      "(10, 1)\n",
      "Step 239: Cost:2.8064231703227955 Accuracy: 0.43\n",
      "(10, 1)\n",
      "Step 240: Cost:2.8051205145554854 Accuracy: 0.43\n",
      "(10, 1)\n",
      "Step 241: Cost:2.8038196881933843 Accuracy: 0.43\n",
      "(10, 1)\n",
      "Step 242: Cost:2.802520685252932 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 243: Cost:2.8012234997748964 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 244: Cost:2.799928125824291 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 245: Cost:2.7986345574902964 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 246: Cost:2.797342788886175 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 247: Cost:2.7960528141491943 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 248: Cost:2.7947646274405447 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 249: Cost:2.793478222945258 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 250: Cost:2.792193594872131 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 251: Cost:2.790910737453638 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 252: Cost:2.7896296449458577 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 253: Cost:2.788350311628392 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 254: Cost:2.787072731804284 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 255: Cost:2.78579689979994 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 256: Cost:2.78452280996505 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 257: Cost:2.7832504566725103 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 258: Cost:2.781979834318339 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 259: Cost:2.780710937321606 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 260: Cost:2.7794437601243436 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 261: Cost:2.7781782971914772 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 262: Cost:2.776914543010739 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 263: Cost:2.7756524920925982 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 264: Cost:2.7743921389701725 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 265: Cost:2.7731334781991586 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 266: Cost:2.771876504357749 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 267: Cost:2.770621212046556 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 268: Cost:2.769367595888535 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 269: Cost:2.7681156505289017 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 270: Cost:2.7668653706350614 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 271: Cost:2.7656167508965264 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 272: Cost:2.7643697860248393 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 273: Cost:2.7631244707534997 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 274: Cost:2.761880799837879 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 275: Cost:2.760638768055153 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 276: Cost:2.7593983702042157 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 277: Cost:2.7581596011056098 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 278: Cost:2.7569224556014444 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 279: Cost:2.7556869285553205 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 280: Cost:2.754453014852256 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 281: Cost:2.753220709398607 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 282: Cost:2.751990007121991 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 283: Cost:2.750760902971212 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 284: Cost:2.749533391916184 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 285: Cost:2.7483074689478544 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 286: Cost:2.747083129078128 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 287: Cost:2.745860367339792 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 288: Cost:2.74463917878644 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 289: Cost:2.743419558492394 Accuracy: 0.44\n",
      "(10, 1)\n",
      "Step 290: Cost:2.742201501552631 Accuracy: 0.45\n",
      "(10, 1)\n",
      "Step 291: Cost:2.7409850030827103 Accuracy: 0.45\n",
      "(10, 1)\n",
      "Step 292: Cost:2.7397700582186895 Accuracy: 0.45\n",
      "(10, 1)\n",
      "Step 293: Cost:2.73855666211706 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 294: Cost:2.737344809954662 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 295: Cost:2.736134496928617 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 296: Cost:2.734925718256249 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 297: Cost:2.73371846917501 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 298: Cost:2.7325127449424054 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 299: Cost:2.7313085408359203 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 300: Cost:2.730105852152945 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 301: Cost:2.7289046742106984 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 302: Cost:2.727705002346157 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 303: Cost:2.7265068319159758 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 304: Cost:2.725310158296422 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 305: Cost:2.724114976883293 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 306: Cost:2.7229212830918472 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 307: Cost:2.7217290723567293 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 308: Cost:2.7205383401318977 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 309: Cost:2.7193490818905475 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 310: Cost:2.718161293125042 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 311: Cost:2.716974969346835 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 312: Cost:2.715790106086402 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 313: Cost:2.7146066988931636 Accuracy: 0.46\n",
      "(10, 1)\n",
      "Step 314: Cost:2.7134247433354144 Accuracy: 0.47\n",
      "(10, 1)\n",
      "Step 315: Cost:2.7122442350002505 Accuracy: 0.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "Step 316: Cost:2.711065169493496 Accuracy: 0.48\n",
      "(10, 1)\n",
      "Step 317: Cost:2.7098875424396307 Accuracy: 0.48\n",
      "(10, 1)\n",
      "Step 318: Cost:2.7087113494817197 Accuracy: 0.48\n",
      "(10, 1)\n",
      "Step 319: Cost:2.707536586281338 Accuracy: 0.48\n",
      "(10, 1)\n",
      "Step 320: Cost:2.7063632485184996 Accuracy: 0.48\n",
      "(10, 1)\n",
      "Step 321: Cost:2.705191331891588 Accuracy: 0.48\n",
      "(10, 1)\n",
      "Step 322: Cost:2.7040208321172816 Accuracy: 0.48\n",
      "(10, 1)\n",
      "Step 323: Cost:2.702851744930482 Accuracy: 0.49\n",
      "(10, 1)\n",
      "Step 324: Cost:2.701684066084244 Accuracy: 0.49\n",
      "(10, 1)\n",
      "Step 325: Cost:2.700517791349704 Accuracy: 0.49\n",
      "(10, 1)\n",
      "Step 326: Cost:2.699352916516009 Accuracy: 0.49\n",
      "(10, 1)\n",
      "Step 327: Cost:2.6981894373902433 Accuracy: 0.49\n",
      "(10, 1)\n",
      "Step 328: Cost:2.6970273497973607 Accuracy: 0.49\n",
      "(10, 1)\n",
      "Step 329: Cost:2.69586664958011 Accuracy: 0.49\n",
      "(10, 1)\n",
      "Step 330: Cost:2.6947073325989686 Accuracy: 0.49\n",
      "(10, 1)\n",
      "Step 331: Cost:2.6935493947320697 Accuracy: 0.5\n",
      "(10, 1)\n",
      "Step 332: Cost:2.6923928318751327 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 333: Cost:2.691237639941388 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 334: Cost:2.6900838148615174 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 335: Cost:2.6889313525835745 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 336: Cost:2.6877802490729197 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 337: Cost:2.6866305003121482 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 338: Cost:2.6854821023010227 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 339: Cost:2.6843350510564035 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 340: Cost:2.6831893426121773 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 341: Cost:2.6820449730191918 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 342: Cost:2.680901938345183 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 343: Cost:2.6797602346747094 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 344: Cost:2.6786198581090828 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 345: Cost:2.6774808047662977 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 346: Cost:2.6763430707809666 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 347: Cost:2.675206652304249 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 348: Cost:2.674071545503787 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 349: Cost:2.672937746563632 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 350: Cost:2.6718052516841824 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 351: Cost:2.6706740570821133 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 352: Cost:2.6695441589903113 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 353: Cost:2.6684155536578045 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 354: Cost:2.667288237349698 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 355: Cost:2.6661622063471078 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 356: Cost:2.665037456947091 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 357: Cost:2.6639139854625817 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 358: Cost:2.662791788222324 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 359: Cost:2.661670861570808 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 360: Cost:2.6605512018681985 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 361: Cost:2.659432805490277 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 362: Cost:2.6583156688283687 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 363: Cost:2.6571997882892817 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 364: Cost:2.6560851602952407 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 365: Cost:2.6549717812838214 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 366: Cost:2.6538596477078853 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 367: Cost:2.6527487560355154 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 368: Cost:2.651639102749955 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 369: Cost:2.6505306843495355 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 370: Cost:2.6494234973476205 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 371: Cost:2.648317538272536 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 372: Cost:2.6472128036675118 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 373: Cost:2.646109290090611 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 374: Cost:2.645006994114674 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 375: Cost:2.64390591232725 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 376: Cost:2.6428060413305348 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 377: Cost:2.641707377741311 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 378: Cost:2.640609918190881 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 379: Cost:2.639513659325007 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 380: Cost:2.6384185978038484 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 381: Cost:2.637324730301899 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 382: Cost:2.6362320535079244 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 383: Cost:2.6351405641249035 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 384: Cost:2.6340502588699617 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 385: Cost:2.6329611344743125 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 386: Cost:2.6318731876831962 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 387: Cost:2.630786415255821 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 388: Cost:2.629700813965296 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 389: Cost:2.628616380598575 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 390: Cost:2.627533111956397 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 391: Cost:2.6264510048532212 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 392: Cost:2.6253700561171716 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 393: Cost:2.6242902625899744 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 394: Cost:2.6232116211268988 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 395: Cost:2.6221341285966973 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 396: Cost:2.621057781881547 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 397: Cost:2.6199825778769896 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 398: Cost:2.6189085134918724 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 399: Cost:2.61783558564829 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 400: Cost:2.616763791281526 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 401: Cost:2.615693127339993 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 402: Cost:2.6146235907851763 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 403: Cost:2.613555178591574 Accuracy: 0.51\n",
      "(10, 1)\n",
      "Step 404: Cost:2.612487887746639 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 405: Cost:2.611421715250726 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 406: Cost:2.6103566581170266 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 407: Cost:2.6092927133715165 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 408: Cost:2.6082298780528985 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 409: Cost:2.6071681492125447 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 410: Cost:2.606107523914439 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 411: Cost:2.605047999235121 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 412: Cost:2.6039895722636306 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 413: Cost:2.602932240101452 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 414: Cost:2.6018759998624548 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 415: Cost:2.600820848672843 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 416: Cost:2.5997667836710945 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 417: Cost:2.5987138020079095 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 418: Cost:2.597661900846153 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 419: Cost:2.596611077360801 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 420: Cost:2.595561328738886 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 421: Cost:2.5945126521794397 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 422: Cost:2.593465044893443 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 423: Cost:2.5924185041037675 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 424: Cost:2.5913730270451243 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 425: Cost:2.590328610964009 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 426: Cost:2.589285253118649 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 427: Cost:2.5882429507789477 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 428: Cost:2.5872017012264337 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 429: Cost:2.5861615017542072 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 430: Cost:2.585122349666886 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 431: Cost:2.584084242280554 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 432: Cost:2.5830471769227086 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 433: Cost:2.5820111509322072 Accuracy: 0.52\n",
      "(10, 1)\n",
      "Step 434: Cost:2.580976161659217 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 435: Cost:2.579942206465161 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 436: Cost:2.5789092827226683 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 437: Cost:2.5778773878155214 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 438: Cost:2.5768465191386047 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 439: Cost:2.5758166740978536 Accuracy: 0.53\n",
      "(10, 1)\n",
      "Step 440: Cost:2.5747878501102046 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 441: Cost:2.5737600446035422 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 442: Cost:2.5727332550166504 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 443: Cost:2.5717074787991625 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 444: Cost:2.5706827134115087 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 445: Cost:2.5696589563248677 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 446: Cost:2.568636205021117 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 447: Cost:2.567614456992784 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 448: Cost:2.5665937097429925 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 449: Cost:2.5655739607854184 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 450: Cost:2.5645552076442395 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 451: Cost:2.5635374478540847 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 452: Cost:2.562520678959985 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 453: Cost:2.56150489851733 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 454: Cost:2.5604901040918135 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 455: Cost:2.559476293259388 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 456: Cost:2.5584634636062176 Accuracy: 0.54\n",
      "(10, 1)\n",
      "Step 457: Cost:2.55745161272863 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 458: Cost:2.5564407382330665 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 459: Cost:2.5554308377360386 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 460: Cost:2.554421908864078 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 461: Cost:2.5534139492536894 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 462: Cost:2.5524069565513066 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 463: Cost:2.5514009284132424 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 464: Cost:2.550395862505645 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 465: Cost:2.5493917565044506 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 466: Cost:2.5483886080953364 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 467: Cost:2.547386414973676 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 468: Cost:2.5463851748444943 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 469: Cost:2.54538488542242 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 470: Cost:2.544385544431642 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 471: Cost:2.5433871496058646 Accuracy: 0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "Step 472: Cost:2.542389698688261 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 473: Cost:2.5413931894314286 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 474: Cost:2.540397619597345 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 475: Cost:2.539402986957326 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 476: Cost:2.538409289291977 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 477: Cost:2.5374165243911517 Accuracy: 0.55\n",
      "(10, 1)\n",
      "Step 478: Cost:2.536424690053907 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 479: Cost:2.5354337840884624 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 480: Cost:2.5344438043121515 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 481: Cost:2.5334547485513834 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 482: Cost:2.5324666146415953 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 483: Cost:2.5314794004272145 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 484: Cost:2.5304931037616116 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 485: Cost:2.5295077225070597 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 486: Cost:2.528523254534691 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 487: Cost:2.527539697724457 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 488: Cost:2.5265570499650822 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 489: Cost:2.525575309154027 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 490: Cost:2.5245944731974435 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 491: Cost:2.5236145400101337 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 492: Cost:2.5226355075155085 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 493: Cost:2.5216573736455485 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 494: Cost:2.5206801363407587 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 495: Cost:2.519703793550134 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 496: Cost:2.5187283432311114 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 497: Cost:2.517753783349536 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 498: Cost:2.5167801118796165 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 499: Cost:2.5158073268038867 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 500: Cost:2.514835426113166 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 501: Cost:2.5138644078065178 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 502: Cost:2.5128942698912122 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 503: Cost:2.5119250103826865 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 504: Cost:2.510956627304503 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 505: Cost:2.509989118688313 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 506: Cost:2.5090224825738177 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 507: Cost:2.5080567170087273 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 508: Cost:2.5070918200487244 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 509: Cost:2.506127789757426 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 510: Cost:2.505164624206343 Accuracy: 0.56\n",
      "(10, 1)\n",
      "Step 511: Cost:2.504202321474845 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 512: Cost:2.5032408796501193 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 513: Cost:2.5022802968271365 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 514: Cost:2.501320571108611 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 515: Cost:2.5003617006049623 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 516: Cost:2.4994036834342825 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 517: Cost:2.4984465177222943 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 518: Cost:2.4974902016023157 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 519: Cost:2.496534733215226 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 520: Cost:2.495580110709424 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 521: Cost:2.494626332240797 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 522: Cost:2.4936733959726807 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 523: Cost:2.492721300075826 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 524: Cost:2.49177004272836 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 525: Cost:2.490819622115753 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 526: Cost:2.4898700364307826 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 527: Cost:2.4889212838734966 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 528: Cost:2.487973362651179 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 529: Cost:2.4870262709783146 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 530: Cost:2.4860800070765556 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 531: Cost:2.485134569174684 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 532: Cost:2.484189955508578 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 533: Cost:2.4832461643211796 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 534: Cost:2.482303193862458 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 535: Cost:2.481361042389376 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 536: Cost:2.480419708165856 Accuracy: 0.57\n",
      "(10, 1)\n",
      "Step 537: Cost:2.479479189462748 Accuracy: 0.58\n",
      "(10, 1)\n",
      "Step 538: Cost:2.4785394845577917 Accuracy: 0.58\n",
      "(10, 1)\n",
      "Step 539: Cost:2.477600591735588 Accuracy: 0.58\n",
      "(10, 1)\n",
      "Step 540: Cost:2.4766625092875616 Accuracy: 0.58\n",
      "(10, 1)\n",
      "Step 541: Cost:2.4757252355119306 Accuracy: 0.58\n",
      "(10, 1)\n",
      "Step 542: Cost:2.474788768713672 Accuracy: 0.58\n",
      "(10, 1)\n",
      "Step 543: Cost:2.473853107204489 Accuracy: 0.58\n",
      "(10, 1)\n",
      "Step 544: Cost:2.4729182493027775 Accuracy: 0.58\n",
      "(10, 1)\n",
      "Step 545: Cost:2.4719841933335966 Accuracy: 0.59\n",
      "(10, 1)\n",
      "Step 546: Cost:2.4710509376286325 Accuracy: 0.59\n",
      "(10, 1)\n",
      "Step 547: Cost:2.4701184805261676 Accuracy: 0.59\n",
      "(10, 1)\n",
      "Step 548: Cost:2.4691868203710485 Accuracy: 0.59\n",
      "(10, 1)\n",
      "Step 549: Cost:2.4682559555146555 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 550: Cost:2.467325884314866 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 551: Cost:2.46639660513603 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 552: Cost:2.465468116348932 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 553: Cost:2.464540416330762 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 554: Cost:2.463613503465086 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 555: Cost:2.4626873761418127 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 556: Cost:2.461762032757163 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 557: Cost:2.46083747171364 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 558: Cost:2.4599136914199966 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 559: Cost:2.4589906902912078 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 560: Cost:2.458068466748437 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 561: Cost:2.4571470192190095 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 562: Cost:2.456226346136378 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 563: Cost:2.455306445940096 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 564: Cost:2.454387317075789 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 565: Cost:2.4534689579951188 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 566: Cost:2.4525513671557615 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 567: Cost:2.4516345430213726 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 568: Cost:2.4507184840615617 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 569: Cost:2.4498031887518605 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 570: Cost:2.448888655573694 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 571: Cost:2.447974883014354 Accuracy: 0.6\n",
      "(10, 1)\n",
      "Step 572: Cost:2.4470618695669692 Accuracy: 0.61\n",
      "(10, 1)\n",
      "Step 573: Cost:2.446149613730475 Accuracy: 0.61\n",
      "(10, 1)\n",
      "Step 574: Cost:2.4452381140095873 Accuracy: 0.61\n",
      "(10, 1)\n",
      "Step 575: Cost:2.4443273689147738 Accuracy: 0.61\n",
      "(10, 1)\n",
      "Step 576: Cost:2.4434173769622247 Accuracy: 0.61\n",
      "(10, 1)\n",
      "Step 577: Cost:2.4425081366738266 Accuracy: 0.61\n",
      "(10, 1)\n",
      "Step 578: Cost:2.441599646577133 Accuracy: 0.61\n",
      "(10, 1)\n",
      "Step 579: Cost:2.4406919052053353 Accuracy: 0.61\n",
      "(10, 1)\n",
      "Step 580: Cost:2.439784911097242 Accuracy: 0.61\n",
      "(10, 1)\n",
      "Step 581: Cost:2.438878662797241 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 582: Cost:2.4379731588552804 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 583: Cost:2.4370683978268377 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 584: Cost:2.4361643782728937 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 585: Cost:2.4352610987599053 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 586: Cost:2.4343585578597784 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 587: Cost:2.4334567541498417 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 588: Cost:2.432555686212821 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 589: Cost:2.431655352636811 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 590: Cost:2.4307557520152487 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 591: Cost:2.42985688294689 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 592: Cost:2.4289587440357825 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 593: Cost:2.428061333891238 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 594: Cost:2.427164651127807 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 595: Cost:2.4262686943652563 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 596: Cost:2.425373462228539 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 597: Cost:2.424478953347773 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 598: Cost:2.4235851663582118 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 599: Cost:2.422692099900223 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 600: Cost:2.421799752619262 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 601: Cost:2.420908123165845 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 602: Cost:2.4200172101955286 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 603: Cost:2.41912701236888 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 604: Cost:2.4182375283514568 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 605: Cost:2.4173487568137806 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 606: Cost:2.4164606964313116 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 607: Cost:2.415573345884429 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 608: Cost:2.4146867038583992 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 609: Cost:2.4138007690433594 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 610: Cost:2.412915540134291 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 611: Cost:2.4120310158309928 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 612: Cost:2.4111471948380636 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 613: Cost:2.4102640758648732 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 614: Cost:2.409381657625543 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 615: Cost:2.408499938838919 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 616: Cost:2.4076189182285517 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 617: Cost:2.4067385945226722 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 618: Cost:2.4058589664541694 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 619: Cost:2.404980032760566 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 620: Cost:2.4041017921839978 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 621: Cost:2.4032242434711897 Accuracy: 0.62\n",
      "(10, 1)\n",
      "Step 622: Cost:2.402347385373434 Accuracy: 0.63\n",
      "(10, 1)\n",
      "Step 623: Cost:2.4014712166465686 Accuracy: 0.63\n",
      "(10, 1)\n",
      "Step 624: Cost:2.400595736050952 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 625: Cost:2.3997209423514447 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 626: Cost:2.3988468343173857 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 627: Cost:2.397973410722571 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 628: Cost:2.3971006703452304 Accuracy: 0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "Step 629: Cost:2.3962286119680067 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 630: Cost:2.3953572343779364 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 631: Cost:2.3944865363664247 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 632: Cost:2.393616516729225 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 633: Cost:2.39274717426642 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 634: Cost:2.391878507782398 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 635: Cost:2.3910105160858337 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 636: Cost:2.3901431979896643 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 637: Cost:2.3892765523110735 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 638: Cost:2.3884105778714666 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 639: Cost:2.3875452734964506 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 640: Cost:2.3866806380158168 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 641: Cost:2.3858166702635164 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 642: Cost:2.384953369077642 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 643: Cost:2.3840907333004093 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 644: Cost:2.383228761778131 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 645: Cost:2.3823674533612054 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 646: Cost:2.381506806904089 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 647: Cost:2.380646821265281 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 648: Cost:2.3797874953073026 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 649: Cost:2.378928827896676 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 650: Cost:2.3780708179039074 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 651: Cost:2.3772134642034652 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 652: Cost:2.3763567656737634 Accuracy: 0.64\n",
      "(10, 1)\n",
      "Step 653: Cost:2.375500721197139 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 654: Cost:2.374645329659835 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 655: Cost:2.373790589951984 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 656: Cost:2.3729365009675827 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 657: Cost:2.372083061604479 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 658: Cost:2.3712302707643507 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 659: Cost:2.3703781273526867 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 660: Cost:2.369526630278772 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 661: Cost:2.3686757784556627 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 662: Cost:2.367825570800174 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 663: Cost:2.366976006232859 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 664: Cost:2.3661270836779904 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 665: Cost:2.3652788020635436 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 666: Cost:2.364431160321178 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 667: Cost:2.363584157386219 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 668: Cost:2.3627377921976414 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 669: Cost:2.361892063698051 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 670: Cost:2.361046970833666 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 671: Cost:2.360202512554301 Accuracy: 0.65\n",
      "(10, 1)\n",
      "Step 672: Cost:2.3593586878133492 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 673: Cost:2.358515495567765 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 674: Cost:2.357672934778046 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 675: Cost:2.3568310044082184 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 676: Cost:2.3559897034258155 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 677: Cost:2.355149030801866 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 678: Cost:2.354308985510871 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 679: Cost:2.3534695665307965 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 680: Cost:2.3526307728430442 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 681: Cost:2.3517926034324477 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 682: Cost:2.350955057287245 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 683: Cost:2.3501181333990697 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 684: Cost:2.3492818307629317 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 685: Cost:2.3484461483772 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 686: Cost:2.3476110852435887 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 687: Cost:2.346776640367139 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 688: Cost:2.345942812756205 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 689: Cost:2.3451096014224357 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 690: Cost:2.344277005380762 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 691: Cost:2.343445023649376 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 692: Cost:2.342613655249721 Accuracy: 0.66\n",
      "(10, 1)\n",
      "Step 693: Cost:2.3417828992064735 Accuracy: 0.67\n",
      "(10, 1)\n",
      "Step 694: Cost:2.340952754547526 Accuracy: 0.67\n",
      "(10, 1)\n",
      "Step 695: Cost:2.340123220303974 Accuracy: 0.67\n",
      "(10, 1)\n",
      "Step 696: Cost:2.3392942955100984 Accuracy: 0.67\n",
      "(10, 1)\n",
      "Step 697: Cost:2.338465979203354 Accuracy: 0.67\n",
      "(10, 1)\n",
      "Step 698: Cost:2.3376382704243492 Accuracy: 0.67\n",
      "(10, 1)\n",
      "Step 699: Cost:2.336811168216835 Accuracy: 0.67\n",
      "(10, 1)\n",
      "Step 700: Cost:2.335984671627689 Accuracy: 0.67\n",
      "(10, 1)\n",
      "Step 701: Cost:2.3351587797068967 Accuracy: 0.67\n",
      "(10, 1)\n",
      "Step 702: Cost:2.334333491507544 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 703: Cost:2.3335088060857943 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 704: Cost:2.33268472250088 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 705: Cost:2.331861239815084 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 706: Cost:2.3310383570937283 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 707: Cost:2.330216073405156 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 708: Cost:2.329394387820719 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 709: Cost:2.328573299414763 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 710: Cost:2.327752807264614 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 711: Cost:2.326932910450563 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 712: Cost:2.326113608055853 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 713: Cost:2.325294899166664 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 714: Cost:2.3244767828720994 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 715: Cost:2.3236592582641706 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 716: Cost:2.3228423244377865 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 717: Cost:2.3220259804907366 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 718: Cost:2.321210225523679 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 719: Cost:2.320395058640126 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 720: Cost:2.3195804789464307 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 721: Cost:2.318766485551774 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 722: Cost:2.317953077568149 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 723: Cost:2.3171402541103525 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 724: Cost:2.3163280142959666 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 725: Cost:2.315516357245347 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 726: Cost:2.314705282081612 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 727: Cost:2.3138947879306273 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 728: Cost:2.313084873920992 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 729: Cost:2.312275539184029 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 730: Cost:2.31146678285377 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 731: Cost:2.3106586040669423 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 732: Cost:2.309851001962957 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 733: Cost:2.309043975683897 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 734: Cost:2.3082375243745026 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 735: Cost:2.3074316471821588 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 736: Cost:2.3066263432568865 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 737: Cost:2.3058216117513255 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 738: Cost:2.3050174518207247 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 739: Cost:2.3042138626229294 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 740: Cost:2.3034108433183693 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 741: Cost:2.302608393070045 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 742: Cost:2.3018065110435177 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 743: Cost:2.301005196406897 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 744: Cost:2.3002044483308275 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 745: Cost:2.299404265988477 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 746: Cost:2.2986046485555276 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 747: Cost:2.29780559521016 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 748: Cost:2.2970071051330443 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 749: Cost:2.296209177507328 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 750: Cost:2.2954118115186235 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 751: Cost:2.294615006354997 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 752: Cost:2.293818761206958 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 753: Cost:2.2930230752674454 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 754: Cost:2.2922279477318197 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 755: Cost:2.29143337779785 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 756: Cost:2.2906393646657004 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 757: Cost:2.289845907537923 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 758: Cost:2.2890530056194445 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 759: Cost:2.2882606581175553 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 760: Cost:2.2874688642418985 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 761: Cost:2.28667762320446 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 762: Cost:2.285886934219556 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 763: Cost:2.285096796503824 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 764: Cost:2.2843072092762093 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 765: Cost:2.283518171757959 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 766: Cost:2.2827296831726054 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 767: Cost:2.281941742745959 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 768: Cost:2.281154349706101 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 769: Cost:2.2803675032833635 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 770: Cost:2.279581202710329 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 771: Cost:2.278795447221814 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 772: Cost:2.2780102360548624 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 773: Cost:2.2772255684487304 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 774: Cost:2.276441443644882 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 775: Cost:2.2756578608869744 Accuracy: 0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "Step 776: Cost:2.2748748194208512 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 777: Cost:2.2740923184945294 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 778: Cost:2.273310357358191 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 779: Cost:2.272528935264175 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 780: Cost:2.271748051466961 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 781: Cost:2.2709677052231685 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 782: Cost:2.2701878957915387 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 783: Cost:2.2694086224329313 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 784: Cost:2.26862988441031 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 785: Cost:2.267851680988736 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 786: Cost:2.2670740114353563 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 787: Cost:2.2662968750193953 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 788: Cost:2.2655202710121474 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 789: Cost:2.264744198686961 Accuracy: 0.68\n",
      "(10, 1)\n",
      "Step 790: Cost:2.263968657319238 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 791: Cost:2.263193646186417 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 792: Cost:2.2624191645679685 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 793: Cost:2.2616452117453827 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 794: Cost:2.2608717870021646 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 795: Cost:2.260098889623818 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 796: Cost:2.259326518897844 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 797: Cost:2.258554674113727 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 798: Cost:2.2577833545629264 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 799: Cost:2.2570125595388704 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 800: Cost:2.2562422883369435 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 801: Cost:2.2554725402544804 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 802: Cost:2.254703314590755 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 803: Cost:2.2539346106469735 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 804: Cost:2.253166427726265 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 805: Cost:2.2523987651336714 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 806: Cost:2.251631622176144 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 807: Cost:2.2508649981625264 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 808: Cost:2.250098892403553 Accuracy: 0.69\n",
      "(10, 1)\n",
      "Step 809: Cost:2.2493333042118384 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 810: Cost:2.248568232901869 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 811: Cost:2.247803677789993 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 812: Cost:2.2470396381944147 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 813: Cost:2.2462761134351847 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 814: Cost:2.2455131028341917 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 815: Cost:2.2447506057151556 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 816: Cost:2.243988621403616 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 817: Cost:2.2432271492269287 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 818: Cost:2.242466188514254 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 819: Cost:2.2417057385965498 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 820: Cost:2.2409457988065644 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 821: Cost:2.2401863684788283 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 822: Cost:2.239427446949644 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 823: Cost:2.2386690335570822 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 824: Cost:2.23791112764097 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 825: Cost:2.2371537285428866 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 826: Cost:2.2363968356061523 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 827: Cost:2.2356404481758227 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 828: Cost:2.2348845655986818 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 829: Cost:2.2341291872232314 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 830: Cost:2.233374312399688 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 831: Cost:2.232619940479969 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 832: Cost:2.2318660708176927 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 833: Cost:2.2311127027681645 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 834: Cost:2.2303598356883736 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 835: Cost:2.2296074689369814 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 836: Cost:2.2288556018743204 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 837: Cost:2.2281042338623793 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 838: Cost:2.227353364264804 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 839: Cost:2.2266029924468818 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 840: Cost:2.2258531177755416 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 841: Cost:2.225103739619342 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 842: Cost:2.224354857348467 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 843: Cost:2.223606470334716 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 844: Cost:2.2228585779514995 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 845: Cost:2.2221111795738318 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 846: Cost:2.2213642745783226 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 847: Cost:2.220617862343171 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 848: Cost:2.219871942248157 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 849: Cost:2.2191265136746394 Accuracy: 0.7\n",
      "(10, 1)\n",
      "Step 850: Cost:2.2183815760055423 Accuracy: 0.71\n",
      "(10, 1)\n",
      "Step 851: Cost:2.2176371286253533 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 852: Cost:2.2168931709201147 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 853: Cost:2.2161497022774173 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 854: Cost:2.215406722086394 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 855: Cost:2.214664229737713 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 856: Cost:2.2139222246235697 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 857: Cost:2.2131807061376825 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 858: Cost:2.212439673675285 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 859: Cost:2.21169912663312 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 860: Cost:2.2109590644094324 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 861: Cost:2.2102194864039633 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 862: Cost:2.2094803920179427 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 863: Cost:2.208741780654085 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 864: Cost:2.2080036517165813 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 865: Cost:2.207266004611092 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 866: Cost:2.2065288387447444 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 867: Cost:2.2057921535261213 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 868: Cost:2.20505594836526 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 869: Cost:2.204320222673641 Accuracy: 0.72\n",
      "(10, 1)\n",
      "Step 870: Cost:2.203584975864186 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 871: Cost:2.2028502073512497 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 872: Cost:2.202115916550616 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 873: Cost:2.201382102879487 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 874: Cost:2.2006487657564837 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 875: Cost:2.1999159046016343 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 876: Cost:2.1991835188363713 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 877: Cost:2.1984516078835243 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 878: Cost:2.197720171167316 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 879: Cost:2.1969892081133553 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 880: Cost:2.1962587181486284 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 881: Cost:2.195528700701498 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 882: Cost:2.1947991552016957 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 883: Cost:2.194070081080314 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 884: Cost:2.193341477769805 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 885: Cost:2.1926133447039713 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 886: Cost:2.1918856813179604 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 887: Cost:2.1911584870482614 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 888: Cost:2.1904317613326976 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 889: Cost:2.1897055036104205 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 890: Cost:2.188979713321908 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 891: Cost:2.1882543899089533 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 892: Cost:2.1875295328146627 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 893: Cost:2.186805141483452 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 894: Cost:2.1860812153610367 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 895: Cost:2.185357753894429 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 896: Cost:2.184634756531933 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 897: Cost:2.183912222723138 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 898: Cost:2.183190151918915 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 899: Cost:2.182468543571409 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 900: Cost:2.1817473971340355 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 901: Cost:2.1810267120614752 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 902: Cost:2.1803064878096685 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 903: Cost:2.17958672383581 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 904: Cost:2.178867419598343 Accuracy: 0.73\n",
      "(10, 1)\n",
      "Step 905: Cost:2.1781485745569573 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 906: Cost:2.1774301881725795 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 907: Cost:2.1767122599073727 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 908: Cost:2.1759947892247267 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 909: Cost:2.175277775589258 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 910: Cost:2.174561218466801 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 911: Cost:2.173845117324405 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 912: Cost:2.173129471630327 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 913: Cost:2.1724142808540314 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 914: Cost:2.1716995444661804 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 915: Cost:2.170985261938631 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 916: Cost:2.170271432744432 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 917: Cost:2.169558056357814 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 918: Cost:2.1688451322541917 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 919: Cost:2.168132659910154 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 920: Cost:2.16742063880346 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 921: Cost:2.1667090684130375 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 922: Cost:2.1659979482189735 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 923: Cost:2.1652872777025127 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 924: Cost:2.1645770563460545 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 925: Cost:2.163867283633143 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 926: Cost:2.1631579590484677 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 927: Cost:2.1624490820778557 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 928: Cost:2.1617406522082696 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 929: Cost:2.1610326689278003 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 930: Cost:2.160325131725665 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 931: Cost:2.1596180400922016 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 932: Cost:2.158911393518864 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 933: Cost:2.15820519149822 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 934: Cost:2.1574994335239417 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 935: Cost:2.156794119090807 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 936: Cost:2.1560892476946925 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 937: Cost:2.1553848188325695 Accuracy: 0.74\n",
      "(10, 1)\n",
      "Step 938: Cost:2.1546808320024984 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 939: Cost:2.1539772867036278 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 940: Cost:2.1532741824361863 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 941: Cost:2.152571518701482 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 942: Cost:2.151869295001894 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 943: Cost:2.1511675108408737 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 944: Cost:2.1504661657229356 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 945: Cost:2.149765259153656 Accuracy: 0.76\n",
      "(10, 1)\n",
      "Step 946: Cost:2.149064790639667 Accuracy: 0.76\n",
      "(10, 1)\n",
      "Step 947: Cost:2.148364759688656 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 948: Cost:2.147665165809357 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 949: Cost:2.1469660085115487 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 950: Cost:2.146267287306052 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 951: Cost:2.145569001704725 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 952: Cost:2.1448711512204546 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 953: Cost:2.1441737353671604 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 954: Cost:2.1434767536597854 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 955: Cost:2.1427802056142937 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 956: Cost:2.1420840907476664 Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "Step 957: Cost:2.1413884085778974 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 958: Cost:2.1406931586239906 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 959: Cost:2.139998340405953 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 960: Cost:2.1393039534447973 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 961: Cost:2.1386099972625305 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 962: Cost:2.1379164713821557 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 963: Cost:2.137223375327664 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 964: Cost:2.136530708624036 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 965: Cost:2.1358384707972324 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 966: Cost:2.135146661374194 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 967: Cost:2.1344552798828373 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 968: Cost:2.1337643258520513 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 969: Cost:2.133073798811691 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 970: Cost:2.1323836982925775 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 971: Cost:2.1316940238264923 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 972: Cost:2.131004774946174 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 973: Cost:2.1303159511853154 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 974: Cost:2.1296275520785586 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 975: Cost:2.1289395771614936 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 976: Cost:2.1282520259706526 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 977: Cost:2.1275648980435067 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 978: Cost:2.126878192918465 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 979: Cost:2.1261919101348674 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 980: Cost:2.1255060492329845 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 981: Cost:2.1248206097540123 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 982: Cost:2.124135591240068 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 983: Cost:2.123450993234189 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 984: Cost:2.1227668152803294 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 985: Cost:2.1220830569233526 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 986: Cost:2.1213997177090334 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 987: Cost:2.1207167971840506 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 988: Cost:2.1200342948959867 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 989: Cost:2.119352210393323 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 990: Cost:2.1186705432254347 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 991: Cost:2.117989292942592 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 992: Cost:2.117308459095953 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 993: Cost:2.1166280412375618 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 994: Cost:2.1159480389203456 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 995: Cost:2.1152684516981117 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 996: Cost:2.1145892791255427 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 997: Cost:2.113910520758195 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 998: Cost:2.113232176152496 Accuracy: 0.75\n",
      "(10, 1)\n",
      "Step 999: Cost:2.112554244865739 Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "        P=EvaluateClassifier(X,W,b)\n",
    "        # Get the cost of the predictions\n",
    "        cost=ComputeCost_CrossEntropy(P, Y, W)\n",
    "        # Get accuracy of predictions\n",
    "        acc=ComputeAccuracy(P,Y)\n",
    "        # Learn the gradients of the cost function\n",
    "        grad_W,grad_b=ComputeGradients(X,Y,P,W,lambd)\n",
    "        # update params\n",
    "        W,b=update_params(W,b,grad_W,grad_b,0.001)\n",
    "        print('Step '+str(i)+\": Cost:\"+str(cost)+\" Accuracy: \"+str(acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accordion': ipywidgets.widgets.widget_selectioncontainer.Accordion,\n",
       " 'Audio': ipywidgets.widgets.widget_media.Audio,\n",
       " 'BoundedFloatText': ipywidgets.widgets.widget_float.BoundedFloatText,\n",
       " 'BoundedIntText': ipywidgets.widgets.widget_int.BoundedIntText,\n",
       " 'Box': ipywidgets.widgets.widget_box.Box,\n",
       " 'Button': ipywidgets.widgets.widget_button.Button,\n",
       " 'ButtonStyle': ipywidgets.widgets.widget_button.ButtonStyle,\n",
       " 'CallbackDispatcher': ipywidgets.widgets.widget.CallbackDispatcher,\n",
       " 'Checkbox': ipywidgets.widgets.widget_bool.Checkbox,\n",
       " 'Color': ipywidgets.widgets.trait_types.Color,\n",
       " 'ColorPicker': ipywidgets.widgets.widget_color.ColorPicker,\n",
       " 'ComputeAccuracy': <function __main__.ComputeAccuracy>,\n",
       " 'ComputeCost_CrossEntropy': <function __main__.ComputeCost_CrossEntropy>,\n",
       " 'ComputeGradients': <function __main__.ComputeGradients>,\n",
       " 'ComputeGradsNum': <function __main__.ComputeGradsNum>,\n",
       " 'Controller': ipywidgets.widgets.widget_controller.Controller,\n",
       " 'CoreWidget': ipywidgets.widgets.widget_core.CoreWidget,\n",
       " 'DOMWidget': ipywidgets.widgets.domwidget.DOMWidget,\n",
       " 'DatePicker': ipywidgets.widgets.widget_date.DatePicker,\n",
       " 'Datetime': ipywidgets.widgets.trait_types.Datetime,\n",
       " 'Dropdown': ipywidgets.widgets.widget_selection.Dropdown,\n",
       " 'EvaluateClassifier': <function __main__.EvaluateClassifier>,\n",
       " 'FloatLogSlider': ipywidgets.widgets.widget_float.FloatLogSlider,\n",
       " 'FloatProgress': ipywidgets.widgets.widget_float.FloatProgress,\n",
       " 'FloatRangeSlider': ipywidgets.widgets.widget_float.FloatRangeSlider,\n",
       " 'FloatSlider': ipywidgets.widgets.widget_float.FloatSlider,\n",
       " 'FloatText': ipywidgets.widgets.widget_float.FloatText,\n",
       " 'GDparams': {'eta': 0.001, 'n_batch': 100, 'n_epochs': 200},\n",
       " 'GridBox': ipywidgets.widgets.widget_box.GridBox,\n",
       " 'HBox': ipywidgets.widgets.widget_box.HBox,\n",
       " 'HTML': ipywidgets.widgets.widget_string.HTML,\n",
       " 'HTMLMath': ipywidgets.widgets.widget_string.HTMLMath,\n",
       " 'Image': ipywidgets.widgets.widget_media.Image,\n",
       " 'In': ['',\n",
       "  u\"import matplotlib.pyplot as plt\\nimport numpy as np\\nfrom PIL import Image\\nfrom utils_assignment1 import LoadBatch, show_image, update_params\\nimport pickle\\nimport pprint\\nimport seaborn as sns\\nfrom IPython.html.widgets import *\\n\\nget_ipython().magic(u'matplotlib inline')\\n\\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\\nplt.rcParams['image.interpolation'] = 'nearest'\\nplt.rcParams['image.cmap'] = 'gray'\\n\\nget_ipython().magic(u'load_ext autoreload')\\nget_ipython().magic(u'autoreload 2')\\n\\nnp.random.seed(200)\",\n",
       "  u'#get the data\\ntrain_set_x, train_set_y, valid_set_x, valid_set_y, test_set_x, test_set_y, classes = LoadBatch(\\'cifar-10\\')\\n\\n#reshaping labels\\ntrain_x = train_set_x.transpose()\\nvalid_x = valid_set_x.transpose()\\ntest_x  = test_set_x.transpose()\\ntrain_y = train_set_y.transpose()\\nvalid_y = valid_set_y.transpose()\\ntest_y  = test_set_y.transpose()\\nprint(\"Reshaping \" +u\\'\\\\u2713\\' )',\n",
       "  u'num_train = train_x.shape[1]\\nnum_valid = valid_x.shape[1]\\nnum_test = test_x.shape[1]\\nnum_px = train_x.shape[0]\\n\\n\\nprint (\"Number of training examples: \" + str(num_train))\\nprint (\"Number of valid examples: \" + str(num_valid))\\nprint (\"Number of testing examples: \" + str(num_test))\\nprint (\"Each image is of size: \"+str(num_px))\\nprint (\"train_x shape: \" + str(train_x.shape))\\nprint (\"train_y shape: \" + str(train_y.shape))\\nprint (\"valid_x shape: \" + str(valid_x.shape))\\nprint (\"valid_y shape: \" + str(valid_y.shape))\\nprint (\"test_x shape: \" + str(test_x.shape))\\nprint (\"test_y shape: \" + str(test_y.shape))\\nprint (\"classes shape :\"+str(classes.shape))',\n",
       "  u'index=1\\nshow_image(train_set_x,index)\\n#using train_set_x keeping the show_image dimensions in mind\\nlabel=np.where(train_y[:,index]==1)\\nprint (\"It is a \"+str(classes[label][0]))',\n",
       "  u\"#Number for images to train\\nN=100\\n#Number of possible predictions\\nK=len(classes)\\n#Dimensions of the input image\\nd=num_px\\n#Subset of data to be trained on\\nX=train_x[:,:N]\\nY=train_y[:,:N]\\n#Gausssian parameters \\nmu=0\\nsigma=0.01\\nW = np.random.normal(loc=mu,scale=sigma,size=(K,d)) \\nb = np.random.normal(loc=mu,scale=sigma,size=(K,1))\\n\\nlambd=0\\nGDparams={}\\n#n_batch=batch_size NOT the number\\nGDparams['n_batch']=100\\nGDparams['eta']=0.001\\nGDparams['n_epochs']=200\\n\\n#Get the shapes right\\nassert(X.shape == (d, N))\\nassert(Y.shape == (K, N))\\nassert(W.shape == (K, d))\\nassert(b.shape == (K, 1))\\nassert(classes.size==10)\",\n",
       "  u'def softmax(x):\\n    #Instead of using np.exp(x)/np.sum(np.exp(x))\\n    #I decrease the value of x with the max, \\n    #it could be any number as it cancels out when equation is expanded\\n    #This is to avoid overflow due to exponential increase\\n    e_x = np.exp(x - np.max(x))\\n    soft= e_x / np.sum(e_x,axis=0)\\n    return soft\\n\\ndef EvaluateClassifier(X, W, b):\\n    \"\"\"\\n    s = W x + b\\n    p = SOFTMAX (s) \\n    \"\"\"\\n    s = np.dot(W,X) + b\\n    P = softmax(s)\\n    #Dimensional check\\n    assert(X.shape == (d,N))\\n    assert(P.shape == (K,N))\\n    return P',\n",
       "  u'# N set to 100\\nP=EvaluateClassifier(X,W,b)\\nplt.rcParams[\\'figure.figsize\\'] = [15, 10]\\n# plt.subplot(221).set_title(\"Probabilities\")\\n# ax = sns.heatmap(P)\\nplt.subplot(222).set_title(\"Prediction\")\\npredictions=(P == P.max(axis=0, keepdims=1)).astype(float)\\nax = sns.heatmap(predictions)\\nplt.subplot(221).set_title(\"Ground Truth\")\\nax=sns.heatmap(Y)\\nplt.subplot(224).set_title(\"Correct Matches\")\\nmatches=(np.multiply(Y,predictions)).astype(float)\\nax=sns.heatmap(matches)\\nplt.subplot(223).set_title(\"Differences\")\\nmatches= Y+predictions\\nax=sns.heatmap(matches)\\nplt.show(ax)',\n",
       "  u'def ComputeCost_CrossEntropy(P,Y,W):\\n    \\n    #instead of using function J = ComputeCost(X, Y, W, b, lambda)\\n    # and calculating P again lets use it from the from the prev. output\\n    \"\"\"\\n    P - Softmax predictions\\n    Y - Ground Truth\\n    \"\"\"\\n    #cross entropy loss - Handling -log0 tending to infinity\\n    #crossEntropyLoss =  -1*np.log(np.matmul(Y,P))\\n    D = Y.shape[1]\\n\\n    crossEntropyLoss = -1*(np.multiply(Y, np.log(P)) + np.multiply(1 - Y, np.log(1 - P)))\\n    #L2 regularization term\\n    L2_regularization_cost = lambd * np.sum(np.square(W)) \\n    Jacobian = np.sum(crossEntropyLoss)/float(D) + L2_regularization_cost\\n    cost = np.squeeze(Jacobian) # just to make sure the output is a value and not an array\\n    assert(Y.shape == (K,N))\\n    return cost',\n",
       "  u'cost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint(\"cost :\"+str(cost_CE))\\n\\ncost_Check=cost_1(P,Y)\\ncost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint cost_Check\\nprint cost_CE\\nprint cost_CE-cost_Check',\n",
       "  u'def ComputeAccuracy(P,Y):\\n    #instead of using function acc = ComputeAccuracy(X, y, W, b)\\n    # and calculating P again lets use it from the from the prev. output\\n    predictions=np.argmax(P,axis=0)\\n    groundtruth=np.argmax(Y,axis=0)\\n    matches=np.sum(predictions==groundtruth)\\n    total=len(predictions)\\n    accuracy=(matches/float(total))\\n    return accuracy',\n",
       "  u'accuracy=ComputeAccuracy(P,Y)\\nprint (\"Accuracy :\"+ str(accuracy))',\n",
       "  u'def ComputeGradients(X, Y, P, W, lambd):\\n    \"\"\" \\n    G -- dJ/dZ - g for Batch\\n    Z -- WX\\n    grad_W1 -- dJ/dW\\n    grad_b -- dJ/db\\n    grad_L -- dJ/dL Regularization term\\n    grad_W -- dJ/dW + Regularization term\\n\\n    lambd - lambda, to differ from the keyword itself\\n    \"\"\"\\n    N=float(X.shape[1])\\n    G= -1*(Y-P)\\n    grad_W1= (1/N)*np.dot(G,X.T)\\n    grad_L = 2*lambd*W\\n    grad_W = grad_W1+grad_L\\n    grad_b = (1/N)*np.dot(G,np.ones((int(N),1)))\\n    grad_b = np.sum(G, axis=1, keepdims=True)/ N\\n    \\n    print grad_b.shape\\n    assert(grad_W.shape == (K,d))\\n    assert(grad_b.shape == (K,1))\\n    \\n    return grad_W, grad_b',\n",
       "  u'grad_W,grad_b=ComputeGradients(X,Y,P,W,lambd)',\n",
       "  u'def ComputeGradsNum(X, Y, W, b):\\n#function [grad_b, grad_W] = ComputeGradsNum(X, Y, W, b, lambda, h) \\n    h=float(1e-6)\\n    grad_W = np.zeros_like(W)\\n    grad_b = np.zeros_like(b)\\n    \\n    P=EvaluateClassifier(X,W,b)\\n    c = ComputeCost_CrossEntropy(P,Y,W)\\n    for i in range(len(b)):\\n        b_try = b\\n        b_try[i] = b_try[i] + h\\n        P=EvaluateClassifier(X,W,b_try)\\n        c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        grad_b[i] = (c2-c) / h\\n        \\n    for i,j in np.ndindex(W.shape):\\n        W_try = W\\n        W_try[i,j] = W_try[i,j] + h\\n        P=EvaluateClassifier(X,W_try,b)\\n        c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        grad_W[i,j] = (c2-c) / h\\n        \\n    return grad_W,grad_b\\n\\n\\ndef gradient_check(X,Y,W,b, epsilon=1e-6):\\n\\n    Wplus = W + epsilon                               \\n    Wminus = W - epsilon  \\n    P=EvaluateClassifier(X,Wplus,b)\\n    J_plus = ComputeCost_CrossEntropy(P,Y,Wplus)\\n    P=EvaluateClassifier(X,Wminus,b)\\n    J_minus = ComputeCost_CrossEntropy(P,Y,Wminus)\\n    \\n    gradapprox = (J_plus - J_minus) / (2 * epsilon)           \\n    gradWnum,gradbnum=ComputeGradsNum(X,Y,W,b)\\n    #gradapprox=grad_w_num\\n    grad,grad_b =ComputeGradients(X,Y,P,W,lambd)\\n    \\n    numerator = np.linalg.norm(grad - gradapprox)                      \\n    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)    \\n    difference = numerator / denominator                               \\n    \\n    if difference < 1e-6:\\n        print(\"The gradient is correct!\")\\n    else:\\n        print(\"The gradient is wrong!\")\\n        print(\"Variation of :\"+str(difference))\\n    \\n    return difference',\n",
       "  u'gradient_check(X,Y,W,b)',\n",
       "  u'def softmax1(x):\\n    try:\\n        e = np.exp(x)\\n        return e / np.sum(e, axis=0)\\n    except FloatingPointError:\\n        # What happens here is that we get some of the values in x that are too big\\n        # so the exp overflows. In that case we check column by column where the\\n        # problem is\\n        return softmax_column_by_column(x)\\n\\ndef softmax_column_by_column( x):\\n    cols = x.shape[1]\\n    res = np.finfo(float).eps * np.ones_like(x)\\n    for c in range(cols):\\n        try:\\n            e = np.exp(x[:, c])\\n            res[:, c] = e / np.sum(e, axis=0)\\n        except FloatingPointError:\\n            res[np.argmax(x[:, c]), :] = 1 - np.finfo(float).eps\\n    return res\\n\\ndef evaluate(input_data):\\n    \"\"\"\\n    input_data must be a matrix containing one input per column (dimensions: input_size * N)\\n\\n    output will be:\\n    - a matrix containing the probabilities of each class, one image per column \\n      (dimensions: output_size * N)\\n    - a row vector containing the numerical index of the class with the highest probability,\\n      one image per column (dimensions: 1 * N)\\n    \"\"\"\\n    X = np.reshape(input_data, (d, -1))\\n    probabilities = softmax1(np.dot(W, X) + b)\\n    predictions = np.argmax(probabilities, axis=0)\\n    return probabilities, predictions\\n\\ndef cost_1( probabilities, ground_truth):\\n    \"\"\"\\n    probabilities should be the same as the first output of the evaluate function\\n    (dimensions: output_size * N)\\n\\n    ground_truth is matrix containing the one-hot encoded true labels, one image per column\\n    (dimensions: output_size * N)\\n\\n    l is a scalar used as a weight in the regularization term\\n    \"\"\"\\n    N = probabilities.shape[1]\\n    probabilities = np.reshape(probabilities, (K, -1))\\n    ground_truth = np.reshape(ground_truth, (K, -1))\\n\\n    log_arg = np.multiply(ground_truth, probabilities).sum(axis=0)\\n    log_arg[log_arg == 0] = np.finfo(float).eps\\n\\n    return - np.log(log_arg).sum() / N + lambd * np.power(W, 2).sum()\\n\\ndef accuracy(predictions, ground_truth):\\n    \"\"\"\\n    predictions should be the same as the second output of the evaluate function\\n    (dimensions: N * 1)\\n\\n    ground_truth is matrix containing the one-hot encoded true labels, one image per column\\n    (dimensions: output_size * N)\\n    \"\"\"\\n    N = predictions.shape[0]\\n    ground_truth = np.argmax(ground_truth, axis=0)\\n    return np.sum(predictions == ground_truth) / N\\n\\ndef gradients_slides( input_data, probabilities, ground_truth):\\n    N = probabilities.shape[1]\\n\\n    grad_w = np.zeros_like(W)\\n    grad_b = np.zeros_like(b)\\n\\n    for i in range(N):\\n        x = input_data[:, i]\\n        y = ground_truth[:, i]\\n        p = probabilities[:, i]\\n\\n        # The actual formula, which I broke down for ease of debuggin\\n        # g = - np.dot(y, (np.diag(p) - np.outer(p, p))) / np.dot(y, p)\\n\\n        t1 = np.outer(p, p)\\n        t2 = np.dot(y, (np.diag(p) - t1))\\n        t3 = np.dot(y, p)\\n        if t3 == 0:\\n            t3 = np.finfo(float).eps\\n        g = - t2 / t3\\n\\n        grad_w += np.outer(g, x)\\n        grad_b += np.reshape(g, grad_b.shape)\\n\\n    grad_w = grad_w / N + 2 * lambd * W\\n    grad_b /= N\\n\\n    return grad_w, grad_b\\n\\ndef gradients_simple( input_data, probabilities, ground_truth):\\n    N = probabilities.shape[1]\\n\\n    grad_w = np.zeros_like(W)\\n    grad_b = np.zeros_like(b)\\n\\n    for i in range(N):\\n        x = input_data[:, i]\\n        y = ground_truth[:, i]\\n        p = probabilities[:, i]\\n\\n        g = p - y\\n\\n        grad_w += np.outer(g, x)\\n        grad_b += np.reshape(g, grad_b.shape)\\n\\n    grad_w = grad_w / N + 2 * lambd * W\\n    grad_b /= N\\n\\n    return grad_w, grad_b',\n",
       "  u'cost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint(\"cost :\"+str(cost_CE))\\n\\ncost_Check=cost_1(P,Y)\\ncost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint cost_Check\\nprint cost_CE\\nprint cost_CE-cost_Check',\n",
       "  u'#probs check\\n\\nprobabilities, predictions=evaluate(X)\\nprint np.sum(np.abs(probabilities-P))',\n",
       "  u'cost_Check=cost_1(P,Y)\\ncost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint cost_Check\\nprint cost_CE\\nprint cost_CE-cost_Check',\n",
       "  u'#probs check\\n\\nprobabilities, predictions=evaluate(X)\\nprint np.sum(np.abs(probabilities-P))',\n",
       "  u'#probs check\\n\\nprobabilities, predictions=evaluate(X)\\nprint np.sum(np.abs(probabilities-P))',\n",
       "  u'def ComputeGradsNum(X, Y, W, b):\\n#function [grad_b, grad_W] = ComputeGradsNum(X, Y, W, b, lambda, h) \\n    h=float(1e-6)\\n    grad_W = np.zeros_like(W)\\n    grad_b = np.zeros_like(b)\\n    \\n    P=EvaluateClassifier(X,W,b)\\n    #c = ComputeCost_CrossEntropy(P,Y,W)\\n    c = cost_1(P,Y)\\n    \\n    for i in range(len(b)):\\n        b_try = b\\n        b_try[i] = b_try[i] + h\\n        P=EvaluateClassifier(X,W,b_try)\\n        #c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        c2 = cost_1(P,Y)\\n        grad_b[i] = (c2-c) / h\\n        \\n    for i,j in np.ndindex(W.shape):\\n        W_try = W\\n        W_try[i,j] = W_try[i,j] + h\\n        P=EvaluateClassifier(X,W_try,b)\\n        #change params for W and b\\n        #c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        c2 = cost_1(P,Y)\\n        grad_W[i,j] = (c2-c) / h\\n        \\n    return grad_W,grad_b\\n\\n\\ndef gradient_check(X,Y,W,b, epsilon=1e-6):\\n\\n    Wplus = W + epsilon                               \\n    Wminus = W - epsilon  \\n    P=EvaluateClassifier(X,Wplus,b)\\n    J_plus = ComputeCost_CrossEntropy(P,Y,Wplus)\\n    P=EvaluateClassifier(X,Wminus,b)\\n    J_minus = ComputeCost_CrossEntropy(P,Y,Wminus)\\n    \\n    gradapprox = (J_plus - J_minus) / (2 * epsilon)           \\n    gradWnum,gradbnum=ComputeGradsNum(X,Y,W,b)\\n    #gradapprox=grad_w_num\\n    grad,grad_b =ComputeGradients(X,Y,P,W,lambd)\\n    \\n    numerator = np.linalg.norm(grad - gradapprox)                      \\n    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)    \\n    difference = numerator / denominator                               \\n    \\n    if difference < 1e-6:\\n        print(\"The gradient is correct!\")\\n    else:\\n        print(\"The gradient is wrong!\")\\n        print(\"Variation of :\"+str(difference))\\n    \\n    return difference',\n",
       "  u'gradient_check(X,Y,W,b)',\n",
       "  u'globals',\n",
       "  u'globals()'],\n",
       " 'IntProgress': ipywidgets.widgets.widget_int.IntProgress,\n",
       " 'IntRangeSlider': ipywidgets.widgets.widget_int.IntRangeSlider,\n",
       " 'IntSlider': ipywidgets.widgets.widget_int.IntSlider,\n",
       " 'IntText': ipywidgets.widgets.widget_int.IntText,\n",
       " 'K': 10,\n",
       " 'Label': ipywidgets.widgets.widget_string.Label,\n",
       " 'Layout': ipywidgets.widgets.widget_layout.Layout,\n",
       " 'LoadBatch': <function utils_assignment1.LoadBatch>,\n",
       " 'N': 100,\n",
       " 'NumberFormat': ipywidgets.widgets.trait_types.NumberFormat,\n",
       " 'Out': {7: Text(0.5,1,'Differences'),\n",
       "  15: 0.9999999999449815,\n",
       "  23: 0.9999999998899629,\n",
       "  24: <function globals>},\n",
       " 'Output': ipywidgets.widgets.widget_output.Output,\n",
       " 'P': array([[0.07209067, 0.0979767 , 0.15393732, 0.07842604, 0.10976513,\n",
       "         0.10585778, 0.08376613, 0.08237298, 0.08297619, 0.12006231,\n",
       "         0.08431677, 0.11256418, 0.11176894, 0.08444731, 0.10216372,\n",
       "         0.09812723, 0.1194847 , 0.07770115, 0.11322402, 0.08856322,\n",
       "         0.07029624, 0.07425575, 0.08005054, 0.09177115, 0.09962455,\n",
       "         0.10653722, 0.10869699, 0.09630745, 0.08926399, 0.11919663,\n",
       "         0.082439  , 0.11670764, 0.08577962, 0.06752323, 0.09824159,\n",
       "         0.09427278, 0.08043806, 0.09804789, 0.09180957, 0.08375431,\n",
       "         0.10158721, 0.07059231, 0.08309803, 0.07858455, 0.09539103,\n",
       "         0.08560433, 0.06647027, 0.08523642, 0.0703013 , 0.09538036,\n",
       "         0.0926386 , 0.10327795, 0.07379186, 0.08418377, 0.09264058,\n",
       "         0.10818986, 0.07716247, 0.08896602, 0.08618895, 0.11612719,\n",
       "         0.08862362, 0.09772362, 0.0785983 , 0.09430801, 0.07602262,\n",
       "         0.07409631, 0.08252751, 0.11594825, 0.11419598, 0.10108034,\n",
       "         0.09265031, 0.09303577, 0.11476181, 0.09537457, 0.0831714 ,\n",
       "         0.07485603, 0.10479661, 0.07125181, 0.07558112, 0.08115786,\n",
       "         0.08200582, 0.08591236, 0.07781023, 0.089772  , 0.08565577,\n",
       "         0.08555062, 0.07301309, 0.06581308, 0.08980083, 0.1207279 ,\n",
       "         0.08477662, 0.09014948, 0.06765082, 0.10723112, 0.12410015,\n",
       "         0.07984631, 0.09570685, 0.09591744, 0.0782247 , 0.07175234],\n",
       "        [0.07192099, 0.0613794 , 0.05488794, 0.07669821, 0.06989166,\n",
       "         0.07327056, 0.07125493, 0.06631238, 0.07458102, 0.07262292,\n",
       "         0.08368597, 0.07326414, 0.06401555, 0.08571028, 0.07481744,\n",
       "         0.06636074, 0.06388168, 0.07485236, 0.07355211, 0.08244937,\n",
       "         0.06621635, 0.07701118, 0.07683607, 0.07250779, 0.07615388,\n",
       "         0.05917747, 0.07079455, 0.0901032 , 0.06754085, 0.06118927,\n",
       "         0.06883841, 0.06166125, 0.08482804, 0.07852623, 0.07030247,\n",
       "         0.06172109, 0.0640426 , 0.07683382, 0.07525307, 0.074377  ,\n",
       "         0.07002712, 0.06118096, 0.06772734, 0.06200954, 0.06765117,\n",
       "         0.08844022, 0.07295146, 0.08484764, 0.06324236, 0.05035321,\n",
       "         0.09152591, 0.06317206, 0.08565418, 0.07293777, 0.08234966,\n",
       "         0.06376532, 0.07906889, 0.08650053, 0.07221053, 0.06919583,\n",
       "         0.06402684, 0.06935439, 0.06553527, 0.07634827, 0.07007899,\n",
       "         0.06609224, 0.07119139, 0.06035049, 0.08425723, 0.06756818,\n",
       "         0.07247003, 0.06270824, 0.06663826, 0.08317726, 0.07670241,\n",
       "         0.08243413, 0.06927627, 0.07517992, 0.07666657, 0.06810883,\n",
       "         0.0768079 , 0.08253193, 0.07757283, 0.07864721, 0.08528596,\n",
       "         0.09572834, 0.08243602, 0.07607146, 0.07819019, 0.05880992,\n",
       "         0.07401721, 0.0788128 , 0.06251874, 0.06531119, 0.06171673,\n",
       "         0.04931354, 0.06791611, 0.08748147, 0.06827427, 0.0779765 ],\n",
       "        [0.10694726, 0.14718901, 0.08926037, 0.10292546, 0.1168886 ,\n",
       "         0.11889924, 0.10268327, 0.1222581 , 0.12794453, 0.1040956 ,\n",
       "         0.10714539, 0.10429603, 0.09482269, 0.07475592, 0.10798491,\n",
       "         0.14827173, 0.11663461, 0.10956411, 0.11636341, 0.09508227,\n",
       "         0.11858261, 0.0879855 , 0.10629956, 0.11972511, 0.09559459,\n",
       "         0.11138909, 0.08911721, 0.12355905, 0.10350352, 0.11721051,\n",
       "         0.09886464, 0.12175015, 0.10079774, 0.11570536, 0.08697853,\n",
       "         0.10597046, 0.08610955, 0.11273911, 0.11090855, 0.10573149,\n",
       "         0.0877823 , 0.12267117, 0.10759811, 0.11816856, 0.09511965,\n",
       "         0.08647619, 0.12237653, 0.11582414, 0.12062432, 0.10784191,\n",
       "         0.11776387, 0.10233773, 0.1088641 , 0.09672013, 0.09794109,\n",
       "         0.14630058, 0.08910728, 0.09731427, 0.10040751, 0.12237394,\n",
       "         0.09437362, 0.11320733, 0.13577986, 0.09416696, 0.10565204,\n",
       "         0.13455688, 0.11079207, 0.10191148, 0.10454686, 0.11061158,\n",
       "         0.10829293, 0.09674187, 0.10626672, 0.08654673, 0.13009728,\n",
       "         0.10348024, 0.12626252, 0.11109578, 0.10461575, 0.10443213,\n",
       "         0.12363121, 0.09743153, 0.11403705, 0.11544736, 0.08895804,\n",
       "         0.10772301, 0.09908577, 0.09698949, 0.11962476, 0.09775829,\n",
       "         0.12425357, 0.12782586, 0.13345562, 0.11384856, 0.10059755,\n",
       "         0.08145096, 0.09528498, 0.0967143 , 0.11089418, 0.13065907],\n",
       "        [0.08267341, 0.06165728, 0.07678477, 0.0924543 , 0.0831973 ,\n",
       "         0.07127059, 0.08598445, 0.08411027, 0.0932157 , 0.07575741,\n",
       "         0.08728028, 0.06772136, 0.07821922, 0.09906289, 0.07898024,\n",
       "         0.07724193, 0.06379957, 0.08819815, 0.07718657, 0.09320875,\n",
       "         0.08979454, 0.06637923, 0.08921825, 0.07415383, 0.08247486,\n",
       "         0.0680385 , 0.07984087, 0.09515719, 0.07899999, 0.05941812,\n",
       "         0.08922195, 0.06577125, 0.08624013, 0.09526058, 0.07256227,\n",
       "         0.07203906, 0.09893465, 0.10373476, 0.09150298, 0.08727514,\n",
       "         0.08531126, 0.06925178, 0.08421485, 0.08160157, 0.08134631,\n",
       "         0.07162685, 0.0881608 , 0.09285701, 0.08068554, 0.06781964,\n",
       "         0.10370922, 0.07856941, 0.10484717, 0.07976495, 0.09130698,\n",
       "         0.06376582, 0.0818292 , 0.08900694, 0.07836336, 0.06735179,\n",
       "         0.09395095, 0.07155706, 0.08282965, 0.08766271, 0.06672794,\n",
       "         0.0700275 , 0.08060113, 0.06546423, 0.09746529, 0.08029639,\n",
       "         0.07924897, 0.11545182, 0.07593351, 0.10017236, 0.08139681,\n",
       "         0.09347061, 0.07418505, 0.09139516, 0.08890803, 0.09377817,\n",
       "         0.08282877, 0.09587234, 0.08547443, 0.09197316, 0.09640017,\n",
       "         0.09132423, 0.08765032, 0.10413215, 0.08434026, 0.07495474,\n",
       "         0.07227251, 0.08250909, 0.08049991, 0.08020802, 0.07548677,\n",
       "         0.07142337, 0.10514977, 0.0856526 , 0.08284417, 0.07696629],\n",
       "        [0.09371997, 0.11629281, 0.08198124, 0.09546283, 0.09006071,\n",
       "         0.08205312, 0.07376552, 0.0727293 , 0.08431454, 0.08623778,\n",
       "         0.09457281, 0.07143493, 0.07924331, 0.09120556, 0.08406702,\n",
       "         0.08410395, 0.07868095, 0.11224431, 0.07120303, 0.08074726,\n",
       "         0.07692215, 0.07305632, 0.10019953, 0.08762004, 0.08286382,\n",
       "         0.08744289, 0.09000358, 0.07497849, 0.09302457, 0.06648175,\n",
       "         0.09852023, 0.09263799, 0.07766201, 0.08813064, 0.08191876,\n",
       "         0.08120586, 0.09346132, 0.07576341, 0.08171126, 0.10839895,\n",
       "         0.07526421, 0.0733565 , 0.08680273, 0.06829515, 0.09231412,\n",
       "         0.08718067, 0.11390784, 0.09375429, 0.07530097, 0.07639899,\n",
       "         0.08008616, 0.10119197, 0.08399629, 0.08228131, 0.0959518 ,\n",
       "         0.06570985, 0.06929929, 0.09562742, 0.09134623, 0.08116807,\n",
       "         0.08283016, 0.0859285 , 0.09745006, 0.11712131, 0.09302748,\n",
       "         0.12450241, 0.08578963, 0.08303348, 0.0915593 , 0.08626336,\n",
       "         0.09171454, 0.06178316, 0.05566509, 0.08815902, 0.08707849,\n",
       "         0.08317284, 0.08966264, 0.09627571, 0.07913196, 0.07017946,\n",
       "         0.09036405, 0.0845916 , 0.09230964, 0.08124928, 0.08825308,\n",
       "         0.10171137, 0.07899674, 0.08071788, 0.09214667, 0.09513249,\n",
       "         0.09646468, 0.10416743, 0.0783703 , 0.10158886, 0.06870879,\n",
       "         0.08340768, 0.08112686, 0.07771106, 0.10399335, 0.08421835],\n",
       "        [0.09098207, 0.08086225, 0.06033815, 0.09269762, 0.0832606 ,\n",
       "         0.07861668, 0.08272885, 0.07500796, 0.07804159, 0.08639129,\n",
       "         0.08946655, 0.06446995, 0.06245822, 0.10023259, 0.08334499,\n",
       "         0.08395423, 0.05891102, 0.09108167, 0.0738081 , 0.0789897 ,\n",
       "         0.07080166, 0.08455787, 0.09765773, 0.08192455, 0.07662296,\n",
       "         0.07104926, 0.07452537, 0.08822768, 0.08221829, 0.06327854,\n",
       "         0.07537764, 0.06943929, 0.11315205, 0.09108793, 0.07136293,\n",
       "         0.07564244, 0.07123977, 0.07528511, 0.06648912, 0.08764217,\n",
       "         0.08712708, 0.06950234, 0.06832251, 0.07670134, 0.09127348,\n",
       "         0.07772999, 0.09487773, 0.09581031, 0.07269774, 0.05047007,\n",
       "         0.07984592, 0.06502927, 0.08336019, 0.06949012, 0.07727191,\n",
       "         0.07052051, 0.06303959, 0.09308432, 0.07550617, 0.07050258,\n",
       "         0.08660439, 0.07557305, 0.0832449 , 0.09800211, 0.09402075,\n",
       "         0.09594347, 0.08365102, 0.06092447, 0.07695915, 0.07671774,\n",
       "         0.07378625, 0.09241703, 0.06238288, 0.08127878, 0.06842941,\n",
       "         0.07427989, 0.07017569, 0.09372095, 0.0800331 , 0.05820438,\n",
       "         0.07162783, 0.07576718, 0.08360206, 0.06725345, 0.09168125,\n",
       "         0.0949283 , 0.07897016, 0.08903446, 0.08564293, 0.06527237,\n",
       "         0.07833389, 0.08137768, 0.07475721, 0.07849996, 0.07939219,\n",
       "         0.05710912, 0.08446209, 0.08103569, 0.08896371, 0.08978167],\n",
       "        [0.14957285, 0.14489908, 0.13668371, 0.14139878, 0.14576708,\n",
       "         0.11764481, 0.14442042, 0.16223943, 0.13833365, 0.13601849,\n",
       "         0.13013759, 0.15063149, 0.16434698, 0.12887175, 0.1499983 ,\n",
       "         0.13896064, 0.15503861, 0.14205242, 0.14074355, 0.1553334 ,\n",
       "         0.16382758, 0.14103024, 0.13499097, 0.12260793, 0.15573955,\n",
       "         0.15004576, 0.13918176, 0.11482198, 0.14842229, 0.1494857 ,\n",
       "         0.1439416 , 0.13842922, 0.15239074, 0.13803984, 0.17622087,\n",
       "         0.16883616, 0.187757  , 0.13147209, 0.15109156, 0.14930673,\n",
       "         0.16121419, 0.21427378, 0.15064338, 0.14902849, 0.14513512,\n",
       "         0.16592963, 0.16018038, 0.13159731, 0.19040053, 0.19304377,\n",
       "         0.130628  , 0.13770389, 0.12786075, 0.17709154, 0.14112491,\n",
       "         0.1562656 , 0.19195871, 0.13518359, 0.15733254, 0.11708082,\n",
       "         0.14601416, 0.15019289, 0.16926251, 0.14517456, 0.15588241,\n",
       "         0.13011085, 0.16917518, 0.15743827, 0.11593467, 0.15503415,\n",
       "         0.14565119, 0.13511783, 0.14846148, 0.14304996, 0.15995632,\n",
       "         0.13772464, 0.13716798, 0.15069798, 0.18152769, 0.16655682,\n",
       "         0.15198902, 0.13931575, 0.15780662, 0.13308332, 0.14557801,\n",
       "         0.11774713, 0.15549835, 0.1475611 , 0.11870319, 0.16177386,\n",
       "         0.12276078, 0.1221248 , 0.17370763, 0.13708037, 0.15213571,\n",
       "         0.20531915, 0.14431376, 0.15821931, 0.14696631, 0.14220455],\n",
       "        [0.11436872, 0.1082068 , 0.10924449, 0.11380776, 0.10730878,\n",
       "         0.12053319, 0.13707823, 0.11601199, 0.08784823, 0.08705532,\n",
       "         0.10582975, 0.11411382, 0.11874174, 0.12208565, 0.10911936,\n",
       "         0.11424428, 0.10278802, 0.09152115, 0.1163749 , 0.12684833,\n",
       "         0.13553724, 0.12703174, 0.10268571, 0.11199934, 0.11166366,\n",
       "         0.11120252, 0.10328248, 0.10050047, 0.11265416, 0.13282347,\n",
       "         0.12287208, 0.10921207, 0.12741232, 0.10221126, 0.12121099,\n",
       "         0.11476944, 0.10911425, 0.10270913, 0.10762558, 0.10828083,\n",
       "         0.11595401, 0.09896942, 0.11540496, 0.10283523, 0.12042159,\n",
       "         0.09533397, 0.10346485, 0.10325427, 0.12044014, 0.10308714,\n",
       "         0.11111838, 0.10404631, 0.11498715, 0.1081776 , 0.10749724,\n",
       "         0.12614506, 0.10395622, 0.10399296, 0.11168201, 0.10178203,\n",
       "         0.15974674, 0.12702179, 0.09918731, 0.09859509, 0.12401258,\n",
       "         0.10501698, 0.10514776, 0.10020645, 0.11281691, 0.10579708,\n",
       "         0.11355067, 0.12574436, 0.14730298, 0.12591628, 0.1068809 ,\n",
       "         0.1340082 , 0.10924581, 0.12570357, 0.1095633 , 0.14366947,\n",
       "         0.09251337, 0.11928248, 0.1112465 , 0.11321446, 0.10722751,\n",
       "         0.11217881, 0.119648  , 0.12319049, 0.11807792, 0.09125068,\n",
       "         0.12139437, 0.10359553, 0.11119187, 0.1136617 , 0.1238994 ,\n",
       "         0.11392198, 0.13458735, 0.09268009, 0.09927883, 0.09695648],\n",
       "        [0.13174589, 0.09287194, 0.13976698, 0.11622297, 0.10797085,\n",
       "         0.12247764, 0.10162217, 0.09482629, 0.11553722, 0.11572034,\n",
       "         0.10699236, 0.11389135, 0.1107601 , 0.09898524, 0.09622632,\n",
       "         0.09652219, 0.13635659, 0.10771673, 0.12518426, 0.10036786,\n",
       "         0.10507089, 0.14350055, 0.11171225, 0.12755302, 0.10860486,\n",
       "         0.13636989, 0.15674723, 0.13015924, 0.11511263, 0.11957498,\n",
       "         0.12711964, 0.12231437, 0.06765451, 0.12058963, 0.11277541,\n",
       "         0.12048169, 0.12602609, 0.12010768, 0.10489615, 0.10328702,\n",
       "         0.12171313, 0.12554921, 0.12619446, 0.13505787, 0.11189856,\n",
       "         0.11593245, 0.10449712, 0.09932061, 0.10771829, 0.13910127,\n",
       "         0.10201341, 0.13478832, 0.11338832, 0.1254172 , 0.12107728,\n",
       "         0.10902646, 0.14140913, 0.10910023, 0.1180501 , 0.13376886,\n",
       "         0.0974939 , 0.11501983, 0.10730285, 0.10429362, 0.12688094,\n",
       "         0.11745662, 0.10143474, 0.13146393, 0.10392275, 0.11287422,\n",
       "         0.1121776 , 0.09396903, 0.10421226, 0.09290928, 0.10534036,\n",
       "         0.11065084, 0.12793683, 0.10003018, 0.10858253, 0.11832302,\n",
       "         0.12456059, 0.1221055 , 0.10386026, 0.12437621, 0.10714894,\n",
       "         0.10230903, 0.11257   , 0.106211  , 0.11179741, 0.12385294,\n",
       "         0.12172744, 0.10825733, 0.12974341, 0.10809794, 0.10412303,\n",
       "         0.13857812, 0.10503256, 0.13182924, 0.12126249, 0.13123849],\n",
       "        [0.08597817, 0.08866473, 0.09711503, 0.08990604, 0.0858893 ,\n",
       "         0.10937639, 0.11669604, 0.12413131, 0.11720733, 0.11603855,\n",
       "         0.11057253, 0.12761275, 0.11562327, 0.1146428 , 0.11329771,\n",
       "         0.09221308, 0.10442426, 0.10506795, 0.09236006, 0.09840983,\n",
       "         0.10295073, 0.12519162, 0.1003494 , 0.11013724, 0.11065727,\n",
       "         0.0987474 , 0.08780997, 0.08618526, 0.10925971, 0.11134101,\n",
       "         0.09280479, 0.10207676, 0.10408284, 0.10292529, 0.10842618,\n",
       "         0.10506101, 0.08287671, 0.103307  , 0.11871215, 0.09194636,\n",
       "         0.09401949, 0.09465252, 0.10999363, 0.1277177 , 0.09944898,\n",
       "         0.1257457 , 0.07311302, 0.097498  , 0.09858881, 0.11650365,\n",
       "         0.09067054, 0.10988309, 0.10325   , 0.10393559, 0.09283855,\n",
       "         0.09031093, 0.10316922, 0.10122371, 0.10891261, 0.1206489 ,\n",
       "         0.08633561, 0.09442154, 0.08080929, 0.08432736, 0.08769427,\n",
       "         0.08219674, 0.10968958, 0.12325894, 0.09834187, 0.10375697,\n",
       "         0.11045751, 0.12303089, 0.11837501, 0.10341575, 0.10094661,\n",
       "         0.10592258, 0.09129061, 0.08464895, 0.09538997, 0.09558987,\n",
       "         0.10367145, 0.09718932, 0.09628037, 0.10498356, 0.10381126,\n",
       "         0.09079916, 0.11213154, 0.11027887, 0.10167584, 0.11046681,\n",
       "         0.10399893, 0.10118   , 0.0881045 , 0.09447228, 0.10983969,\n",
       "         0.11962979, 0.08641966, 0.09275881, 0.099298  , 0.09824627]]),\n",
       " 'Password': ipywidgets.widgets.widget_string.Password,\n",
       " 'Play': ipywidgets.widgets.widget_int.Play,\n",
       " 'RadioButtons': ipywidgets.widgets.widget_selection.RadioButtons,\n",
       " 'Select': ipywidgets.widgets.widget_selection.Select,\n",
       " 'SelectMultiple': ipywidgets.widgets.widget_selection.SelectMultiple,\n",
       " 'SelectionRangeSlider': ipywidgets.widgets.widget_selection.SelectionRangeSlider,\n",
       " 'SelectionSlider': ipywidgets.widgets.widget_selection.SelectionSlider,\n",
       " 'SliderStyle': ipywidgets.widgets.widget_int.SliderStyle,\n",
       " 'Style': ipywidgets.widgets.widget_style.Style,\n",
       " 'Tab': ipywidgets.widgets.widget_selectioncontainer.Tab,\n",
       " 'Text': ipywidgets.widgets.widget_string.Text,\n",
       " 'Textarea': ipywidgets.widgets.widget_string.Textarea,\n",
       " 'ToggleButton': ipywidgets.widgets.widget_bool.ToggleButton,\n",
       " 'ToggleButtons': ipywidgets.widgets.widget_selection.ToggleButtons,\n",
       " 'ToggleButtonsStyle': ipywidgets.widgets.widget_selection.ToggleButtonsStyle,\n",
       " 'VBox': ipywidgets.widgets.widget_box.VBox,\n",
       " 'Valid': ipywidgets.widgets.widget_bool.Valid,\n",
       " 'ValueWidget': ipywidgets.widgets.valuewidget.ValueWidget,\n",
       " 'Video': ipywidgets.widgets.widget_media.Video,\n",
       " 'W': array([[-0.01450748,  0.01911153,  0.00712079, ...,  0.00502905,\n",
       "         -0.00547657, -0.00850955],\n",
       "        [-0.00750748,  0.00874846, -0.00349592, ..., -0.00911549,\n",
       "         -0.01534913, -0.00281617],\n",
       "        [ 0.00215474,  0.0048328 ,  0.00875116, ..., -0.00888122,\n",
       "          0.01629524,  0.00631037],\n",
       "        ...,\n",
       "        [ 0.00479418, -0.00378068,  0.00994624, ...,  0.02047632,\n",
       "         -0.00803045, -0.004562  ],\n",
       "        [ 0.00108902, -0.00048424,  0.00441777, ...,  0.01417221,\n",
       "         -0.00888738, -0.00369944],\n",
       "        [ 0.00809637,  0.00129246,  0.00346472, ..., -0.0115622 ,\n",
       "         -0.00358414, -0.00125935]]),\n",
       " 'Widget': ipywidgets.widgets.widget.Widget,\n",
       " 'X': array([[0.23137255, 0.60392157, 1.        , ..., 0.12941176, 0.35294118,\n",
       "         0.24313725],\n",
       "        [0.16862745, 0.49411765, 0.99215686, ..., 0.11372549, 0.36862745,\n",
       "         0.19607843],\n",
       "        [0.19607843, 0.41176471, 0.99215686, ..., 0.1254902 , 0.34117647,\n",
       "         0.18039216],\n",
       "        ...,\n",
       "        [0.54901961, 0.54509804, 0.3254902 , ..., 0.24705882, 0.33333333,\n",
       "         0.18431373],\n",
       "        [0.32941176, 0.55686275, 0.3254902 , ..., 0.18431373, 0.31764706,\n",
       "         0.43529412],\n",
       "        [0.28235294, 0.56470588, 0.32941176, ..., 0.15686275, 0.31372549,\n",
       "         0.62745098]]),\n",
       " 'Y': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         1., 1., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]]),\n",
       " '_': <function globals>,\n",
       " '_15': 0.9999999999449815,\n",
       " '_23': 0.9999999998899629,\n",
       " '_24': <function globals>,\n",
       " '_7': Text(0.5,1,'Differences'),\n",
       " '__': 0.9999999998899629,\n",
       " '___': 0.9999999999449815,\n",
       " '__builtin__': <module '__builtin__' (built-in)>,\n",
       " '__builtins__': <module '__builtin__' (built-in)>,\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__name__': '__main__',\n",
       " '__package__': None,\n",
       " '_dh': [u'/media/datta/Sri Datta/_KTH_ACADEMIA/2424'],\n",
       " '_i': u'globals',\n",
       " '_i1': u\"import matplotlib.pyplot as plt\\nimport numpy as np\\nfrom PIL import Image\\nfrom utils_assignment1 import LoadBatch, show_image, update_params\\nimport pickle\\nimport pprint\\nimport seaborn as sns\\nfrom IPython.html.widgets import *\\n\\n%matplotlib inline\\n\\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\\nplt.rcParams['image.interpolation'] = 'nearest'\\nplt.rcParams['image.cmap'] = 'gray'\\n\\n%load_ext autoreload\\n%autoreload 2\\n\\nnp.random.seed(200)\",\n",
       " '_i10': u'def ComputeAccuracy(P,Y):\\n    #instead of using function acc = ComputeAccuracy(X, y, W, b)\\n    # and calculating P again lets use it from the from the prev. output\\n    predictions=np.argmax(P,axis=0)\\n    groundtruth=np.argmax(Y,axis=0)\\n    matches=np.sum(predictions==groundtruth)\\n    total=len(predictions)\\n    accuracy=(matches/float(total))\\n    return accuracy',\n",
       " '_i11': u'accuracy=ComputeAccuracy(P,Y)\\nprint (\"Accuracy :\"+ str(accuracy))',\n",
       " '_i12': u'def ComputeGradients(X, Y, P, W, lambd):\\n    \"\"\" \\n    G -- dJ/dZ - g for Batch\\n    Z -- WX\\n    grad_W1 -- dJ/dW\\n    grad_b -- dJ/db\\n    grad_L -- dJ/dL Regularization term\\n    grad_W -- dJ/dW + Regularization term\\n\\n    lambd - lambda, to differ from the keyword itself\\n    \"\"\"\\n    N=float(X.shape[1])\\n    G= -1*(Y-P)\\n    grad_W1= (1/N)*np.dot(G,X.T)\\n    grad_L = 2*lambd*W\\n    grad_W = grad_W1+grad_L\\n    grad_b = (1/N)*np.dot(G,np.ones((int(N),1)))\\n    grad_b = np.sum(G, axis=1, keepdims=True)/ N\\n    \\n    print grad_b.shape\\n    assert(grad_W.shape == (K,d))\\n    assert(grad_b.shape == (K,1))\\n    \\n    return grad_W, grad_b',\n",
       " '_i13': u'grad_W,grad_b=ComputeGradients(X,Y,P,W,lambd)',\n",
       " '_i14': u'def ComputeGradsNum(X, Y, W, b):\\n#function [grad_b, grad_W] = ComputeGradsNum(X, Y, W, b, lambda, h) \\n    h=float(1e-6)\\n    grad_W = np.zeros_like(W)\\n    grad_b = np.zeros_like(b)\\n    \\n    P=EvaluateClassifier(X,W,b)\\n    c = ComputeCost_CrossEntropy(P,Y,W)\\n    for i in range(len(b)):\\n        b_try = b\\n        b_try[i] = b_try[i] + h\\n        P=EvaluateClassifier(X,W,b_try)\\n        c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        grad_b[i] = (c2-c) / h\\n        \\n    for i,j in np.ndindex(W.shape):\\n        W_try = W\\n        W_try[i,j] = W_try[i,j] + h\\n        P=EvaluateClassifier(X,W_try,b)\\n        c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        grad_W[i,j] = (c2-c) / h\\n        \\n    return grad_W,grad_b\\n\\n\\ndef gradient_check(X,Y,W,b, epsilon=1e-6):\\n\\n    Wplus = W + epsilon                               \\n    Wminus = W - epsilon  \\n    P=EvaluateClassifier(X,Wplus,b)\\n    J_plus = ComputeCost_CrossEntropy(P,Y,Wplus)\\n    P=EvaluateClassifier(X,Wminus,b)\\n    J_minus = ComputeCost_CrossEntropy(P,Y,Wminus)\\n    \\n    gradapprox = (J_plus - J_minus) / (2 * epsilon)           \\n    gradWnum,gradbnum=ComputeGradsNum(X,Y,W,b)\\n    #gradapprox=grad_w_num\\n    grad,grad_b =ComputeGradients(X,Y,P,W,lambd)\\n    \\n    numerator = np.linalg.norm(grad - gradapprox)                      \\n    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)    \\n    difference = numerator / denominator                               \\n    \\n    if difference < 1e-6:\\n        print(\"The gradient is correct!\")\\n    else:\\n        print(\"The gradient is wrong!\")\\n        print(\"Variation of :\"+str(difference))\\n    \\n    return difference',\n",
       " '_i15': u'gradient_check(X,Y,W,b)',\n",
       " '_i16': u'def softmax1(x):\\n    try:\\n        e = np.exp(x)\\n        return e / np.sum(e, axis=0)\\n    except FloatingPointError:\\n        # What happens here is that we get some of the values in x that are too big\\n        # so the exp overflows. In that case we check column by column where the\\n        # problem is\\n        return softmax_column_by_column(x)\\n\\ndef softmax_column_by_column( x):\\n    cols = x.shape[1]\\n    res = np.finfo(float).eps * np.ones_like(x)\\n    for c in range(cols):\\n        try:\\n            e = np.exp(x[:, c])\\n            res[:, c] = e / np.sum(e, axis=0)\\n        except FloatingPointError:\\n            res[np.argmax(x[:, c]), :] = 1 - np.finfo(float).eps\\n    return res\\n\\ndef evaluate(input_data):\\n    \"\"\"\\n    input_data must be a matrix containing one input per column (dimensions: input_size * N)\\n\\n    output will be:\\n    - a matrix containing the probabilities of each class, one image per column \\n      (dimensions: output_size * N)\\n    - a row vector containing the numerical index of the class with the highest probability,\\n      one image per column (dimensions: 1 * N)\\n    \"\"\"\\n    X = np.reshape(input_data, (d, -1))\\n    probabilities = softmax1(np.dot(W, X) + b)\\n    predictions = np.argmax(probabilities, axis=0)\\n    return probabilities, predictions\\n\\ndef cost_1( probabilities, ground_truth):\\n    \"\"\"\\n    probabilities should be the same as the first output of the evaluate function\\n    (dimensions: output_size * N)\\n\\n    ground_truth is matrix containing the one-hot encoded true labels, one image per column\\n    (dimensions: output_size * N)\\n\\n    l is a scalar used as a weight in the regularization term\\n    \"\"\"\\n    N = probabilities.shape[1]\\n    probabilities = np.reshape(probabilities, (K, -1))\\n    ground_truth = np.reshape(ground_truth, (K, -1))\\n\\n    log_arg = np.multiply(ground_truth, probabilities).sum(axis=0)\\n    log_arg[log_arg == 0] = np.finfo(float).eps\\n\\n    return - np.log(log_arg).sum() / N + lambd * np.power(W, 2).sum()\\n\\ndef accuracy(predictions, ground_truth):\\n    \"\"\"\\n    predictions should be the same as the second output of the evaluate function\\n    (dimensions: N * 1)\\n\\n    ground_truth is matrix containing the one-hot encoded true labels, one image per column\\n    (dimensions: output_size * N)\\n    \"\"\"\\n    N = predictions.shape[0]\\n    ground_truth = np.argmax(ground_truth, axis=0)\\n    return np.sum(predictions == ground_truth) / N\\n\\ndef gradients_slides( input_data, probabilities, ground_truth):\\n    N = probabilities.shape[1]\\n\\n    grad_w = np.zeros_like(W)\\n    grad_b = np.zeros_like(b)\\n\\n    for i in range(N):\\n        x = input_data[:, i]\\n        y = ground_truth[:, i]\\n        p = probabilities[:, i]\\n\\n        # The actual formula, which I broke down for ease of debuggin\\n        # g = - np.dot(y, (np.diag(p) - np.outer(p, p))) / np.dot(y, p)\\n\\n        t1 = np.outer(p, p)\\n        t2 = np.dot(y, (np.diag(p) - t1))\\n        t3 = np.dot(y, p)\\n        if t3 == 0:\\n            t3 = np.finfo(float).eps\\n        g = - t2 / t3\\n\\n        grad_w += np.outer(g, x)\\n        grad_b += np.reshape(g, grad_b.shape)\\n\\n    grad_w = grad_w / N + 2 * lambd * W\\n    grad_b /= N\\n\\n    return grad_w, grad_b\\n\\ndef gradients_simple( input_data, probabilities, ground_truth):\\n    N = probabilities.shape[1]\\n\\n    grad_w = np.zeros_like(W)\\n    grad_b = np.zeros_like(b)\\n\\n    for i in range(N):\\n        x = input_data[:, i]\\n        y = ground_truth[:, i]\\n        p = probabilities[:, i]\\n\\n        g = p - y\\n\\n        grad_w += np.outer(g, x)\\n        grad_b += np.reshape(g, grad_b.shape)\\n\\n    grad_w = grad_w / N + 2 * lambd * W\\n    grad_b /= N\\n\\n    return grad_w, grad_b',\n",
       " '_i17': u'cost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint(\"cost :\"+str(cost_CE))\\n\\ncost_Check=cost_1(P,Y)\\ncost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint cost_Check\\nprint cost_CE\\nprint cost_CE-cost_Check',\n",
       " '_i18': u'#probs check\\n\\nprobabilities, predictions=evaluate(X)\\nprint np.sum(np.abs(probabilities-P))',\n",
       " '_i19': u'cost_Check=cost_1(P,Y)\\ncost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint cost_Check\\nprint cost_CE\\nprint cost_CE-cost_Check',\n",
       " '_i2': u'#get the data\\ntrain_set_x, train_set_y, valid_set_x, valid_set_y, test_set_x, test_set_y, classes = LoadBatch(\\'cifar-10\\')\\n\\n#reshaping labels\\ntrain_x = train_set_x.transpose()\\nvalid_x = valid_set_x.transpose()\\ntest_x  = test_set_x.transpose()\\ntrain_y = train_set_y.transpose()\\nvalid_y = valid_set_y.transpose()\\ntest_y  = test_set_y.transpose()\\nprint(\"Reshaping \" +u\\'\\\\u2713\\' )',\n",
       " '_i20': u'#probs check\\n\\nprobabilities, predictions=evaluate(X)\\nprint np.sum(np.abs(probabilities-P))',\n",
       " '_i21': u'#probs check\\n\\nprobabilities, predictions=evaluate(X)\\nprint np.sum(np.abs(probabilities-P))',\n",
       " '_i22': u'def ComputeGradsNum(X, Y, W, b):\\n#function [grad_b, grad_W] = ComputeGradsNum(X, Y, W, b, lambda, h) \\n    h=float(1e-6)\\n    grad_W = np.zeros_like(W)\\n    grad_b = np.zeros_like(b)\\n    \\n    P=EvaluateClassifier(X,W,b)\\n    #c = ComputeCost_CrossEntropy(P,Y,W)\\n    c = cost_1(P,Y)\\n    \\n    for i in range(len(b)):\\n        b_try = b\\n        b_try[i] = b_try[i] + h\\n        P=EvaluateClassifier(X,W,b_try)\\n        #c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        c2 = cost_1(P,Y)\\n        grad_b[i] = (c2-c) / h\\n        \\n    for i,j in np.ndindex(W.shape):\\n        W_try = W\\n        W_try[i,j] = W_try[i,j] + h\\n        P=EvaluateClassifier(X,W_try,b)\\n        #change params for W and b\\n        #c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        c2 = cost_1(P,Y)\\n        grad_W[i,j] = (c2-c) / h\\n        \\n    return grad_W,grad_b\\n\\n\\ndef gradient_check(X,Y,W,b, epsilon=1e-6):\\n\\n    Wplus = W + epsilon                               \\n    Wminus = W - epsilon  \\n    P=EvaluateClassifier(X,Wplus,b)\\n    J_plus = ComputeCost_CrossEntropy(P,Y,Wplus)\\n    P=EvaluateClassifier(X,Wminus,b)\\n    J_minus = ComputeCost_CrossEntropy(P,Y,Wminus)\\n    \\n    gradapprox = (J_plus - J_minus) / (2 * epsilon)           \\n    gradWnum,gradbnum=ComputeGradsNum(X,Y,W,b)\\n    #gradapprox=grad_w_num\\n    grad,grad_b =ComputeGradients(X,Y,P,W,lambd)\\n    \\n    numerator = np.linalg.norm(grad - gradapprox)                      \\n    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)    \\n    difference = numerator / denominator                               \\n    \\n    if difference < 1e-6:\\n        print(\"The gradient is correct!\")\\n    else:\\n        print(\"The gradient is wrong!\")\\n        print(\"Variation of :\"+str(difference))\\n    \\n    return difference',\n",
       " '_i23': u'gradient_check(X,Y,W,b)',\n",
       " '_i24': u'globals',\n",
       " '_i25': u'globals()',\n",
       " '_i3': u'num_train = train_x.shape[1]\\nnum_valid = valid_x.shape[1]\\nnum_test = test_x.shape[1]\\nnum_px = train_x.shape[0]\\n\\n\\nprint (\"Number of training examples: \" + str(num_train))\\nprint (\"Number of valid examples: \" + str(num_valid))\\nprint (\"Number of testing examples: \" + str(num_test))\\nprint (\"Each image is of size: \"+str(num_px))\\nprint (\"train_x shape: \" + str(train_x.shape))\\nprint (\"train_y shape: \" + str(train_y.shape))\\nprint (\"valid_x shape: \" + str(valid_x.shape))\\nprint (\"valid_y shape: \" + str(valid_y.shape))\\nprint (\"test_x shape: \" + str(test_x.shape))\\nprint (\"test_y shape: \" + str(test_y.shape))\\nprint (\"classes shape :\"+str(classes.shape))',\n",
       " '_i4': u'index=1\\nshow_image(train_set_x,index)\\n#using train_set_x keeping the show_image dimensions in mind\\nlabel=np.where(train_y[:,index]==1)\\nprint (\"It is a \"+str(classes[label][0]))',\n",
       " '_i5': u\"#Number for images to train\\nN=100\\n#Number of possible predictions\\nK=len(classes)\\n#Dimensions of the input image\\nd=num_px\\n#Subset of data to be trained on\\nX=train_x[:,:N]\\nY=train_y[:,:N]\\n#Gausssian parameters \\nmu=0\\nsigma=0.01\\nW = np.random.normal(loc=mu,scale=sigma,size=(K,d)) \\nb = np.random.normal(loc=mu,scale=sigma,size=(K,1))\\n\\nlambd=0\\nGDparams={}\\n#n_batch=batch_size NOT the number\\nGDparams['n_batch']=100\\nGDparams['eta']=0.001\\nGDparams['n_epochs']=200\\n\\n#Get the shapes right\\nassert(X.shape == (d, N))\\nassert(Y.shape == (K, N))\\nassert(W.shape == (K, d))\\nassert(b.shape == (K, 1))\\nassert(classes.size==10)\",\n",
       " '_i6': u'def softmax(x):\\n    #Instead of using np.exp(x)/np.sum(np.exp(x))\\n    #I decrease the value of x with the max, \\n    #it could be any number as it cancels out when equation is expanded\\n    #This is to avoid overflow due to exponential increase\\n    e_x = np.exp(x - np.max(x))\\n    soft= e_x / np.sum(e_x,axis=0)\\n    return soft\\n\\ndef EvaluateClassifier(X, W, b):\\n    \"\"\"\\n    s = W x + b\\n    p = SOFTMAX (s) \\n    \"\"\"\\n    s = np.dot(W,X) + b\\n    P = softmax(s)\\n    #Dimensional check\\n    assert(X.shape == (d,N))\\n    assert(P.shape == (K,N))\\n    return P',\n",
       " '_i7': u'# N set to 100\\nP=EvaluateClassifier(X,W,b)\\nplt.rcParams[\\'figure.figsize\\'] = [15, 10]\\n# plt.subplot(221).set_title(\"Probabilities\")\\n# ax = sns.heatmap(P)\\nplt.subplot(222).set_title(\"Prediction\")\\npredictions=(P == P.max(axis=0, keepdims=1)).astype(float)\\nax = sns.heatmap(predictions)\\nplt.subplot(221).set_title(\"Ground Truth\")\\nax=sns.heatmap(Y)\\nplt.subplot(224).set_title(\"Correct Matches\")\\nmatches=(np.multiply(Y,predictions)).astype(float)\\nax=sns.heatmap(matches)\\nplt.subplot(223).set_title(\"Differences\")\\nmatches= Y+predictions\\nax=sns.heatmap(matches)\\nplt.show(ax)',\n",
       " '_i8': u'def ComputeCost_CrossEntropy(P,Y,W):\\n    \\n    #instead of using function J = ComputeCost(X, Y, W, b, lambda)\\n    # and calculating P again lets use it from the from the prev. output\\n    \"\"\"\\n    P - Softmax predictions\\n    Y - Ground Truth\\n    \"\"\"\\n    #cross entropy loss - Handling -log0 tending to infinity\\n    #crossEntropyLoss =  -1*np.log(np.matmul(Y,P))\\n    D = Y.shape[1]\\n\\n    crossEntropyLoss = -1*(np.multiply(Y, np.log(P)) + np.multiply(1 - Y, np.log(1 - P)))\\n    #L2 regularization term\\n    L2_regularization_cost = lambd * np.sum(np.square(W)) \\n    Jacobian = np.sum(crossEntropyLoss)/float(D) + L2_regularization_cost\\n    cost = np.squeeze(Jacobian) # just to make sure the output is a value and not an array\\n    assert(Y.shape == (K,N))\\n    return cost',\n",
       " '_i9': u'cost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint(\"cost :\"+str(cost_CE))\\n\\ncost_Check=cost_1(P,Y)\\ncost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint cost_Check\\nprint cost_CE\\nprint cost_CE-cost_Check',\n",
       " '_ih': ['',\n",
       "  u\"import matplotlib.pyplot as plt\\nimport numpy as np\\nfrom PIL import Image\\nfrom utils_assignment1 import LoadBatch, show_image, update_params\\nimport pickle\\nimport pprint\\nimport seaborn as sns\\nfrom IPython.html.widgets import *\\n\\nget_ipython().magic(u'matplotlib inline')\\n\\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\\nplt.rcParams['image.interpolation'] = 'nearest'\\nplt.rcParams['image.cmap'] = 'gray'\\n\\nget_ipython().magic(u'load_ext autoreload')\\nget_ipython().magic(u'autoreload 2')\\n\\nnp.random.seed(200)\",\n",
       "  u'#get the data\\ntrain_set_x, train_set_y, valid_set_x, valid_set_y, test_set_x, test_set_y, classes = LoadBatch(\\'cifar-10\\')\\n\\n#reshaping labels\\ntrain_x = train_set_x.transpose()\\nvalid_x = valid_set_x.transpose()\\ntest_x  = test_set_x.transpose()\\ntrain_y = train_set_y.transpose()\\nvalid_y = valid_set_y.transpose()\\ntest_y  = test_set_y.transpose()\\nprint(\"Reshaping \" +u\\'\\\\u2713\\' )',\n",
       "  u'num_train = train_x.shape[1]\\nnum_valid = valid_x.shape[1]\\nnum_test = test_x.shape[1]\\nnum_px = train_x.shape[0]\\n\\n\\nprint (\"Number of training examples: \" + str(num_train))\\nprint (\"Number of valid examples: \" + str(num_valid))\\nprint (\"Number of testing examples: \" + str(num_test))\\nprint (\"Each image is of size: \"+str(num_px))\\nprint (\"train_x shape: \" + str(train_x.shape))\\nprint (\"train_y shape: \" + str(train_y.shape))\\nprint (\"valid_x shape: \" + str(valid_x.shape))\\nprint (\"valid_y shape: \" + str(valid_y.shape))\\nprint (\"test_x shape: \" + str(test_x.shape))\\nprint (\"test_y shape: \" + str(test_y.shape))\\nprint (\"classes shape :\"+str(classes.shape))',\n",
       "  u'index=1\\nshow_image(train_set_x,index)\\n#using train_set_x keeping the show_image dimensions in mind\\nlabel=np.where(train_y[:,index]==1)\\nprint (\"It is a \"+str(classes[label][0]))',\n",
       "  u\"#Number for images to train\\nN=100\\n#Number of possible predictions\\nK=len(classes)\\n#Dimensions of the input image\\nd=num_px\\n#Subset of data to be trained on\\nX=train_x[:,:N]\\nY=train_y[:,:N]\\n#Gausssian parameters \\nmu=0\\nsigma=0.01\\nW = np.random.normal(loc=mu,scale=sigma,size=(K,d)) \\nb = np.random.normal(loc=mu,scale=sigma,size=(K,1))\\n\\nlambd=0\\nGDparams={}\\n#n_batch=batch_size NOT the number\\nGDparams['n_batch']=100\\nGDparams['eta']=0.001\\nGDparams['n_epochs']=200\\n\\n#Get the shapes right\\nassert(X.shape == (d, N))\\nassert(Y.shape == (K, N))\\nassert(W.shape == (K, d))\\nassert(b.shape == (K, 1))\\nassert(classes.size==10)\",\n",
       "  u'def softmax(x):\\n    #Instead of using np.exp(x)/np.sum(np.exp(x))\\n    #I decrease the value of x with the max, \\n    #it could be any number as it cancels out when equation is expanded\\n    #This is to avoid overflow due to exponential increase\\n    e_x = np.exp(x - np.max(x))\\n    soft= e_x / np.sum(e_x,axis=0)\\n    return soft\\n\\ndef EvaluateClassifier(X, W, b):\\n    \"\"\"\\n    s = W x + b\\n    p = SOFTMAX (s) \\n    \"\"\"\\n    s = np.dot(W,X) + b\\n    P = softmax(s)\\n    #Dimensional check\\n    assert(X.shape == (d,N))\\n    assert(P.shape == (K,N))\\n    return P',\n",
       "  u'# N set to 100\\nP=EvaluateClassifier(X,W,b)\\nplt.rcParams[\\'figure.figsize\\'] = [15, 10]\\n# plt.subplot(221).set_title(\"Probabilities\")\\n# ax = sns.heatmap(P)\\nplt.subplot(222).set_title(\"Prediction\")\\npredictions=(P == P.max(axis=0, keepdims=1)).astype(float)\\nax = sns.heatmap(predictions)\\nplt.subplot(221).set_title(\"Ground Truth\")\\nax=sns.heatmap(Y)\\nplt.subplot(224).set_title(\"Correct Matches\")\\nmatches=(np.multiply(Y,predictions)).astype(float)\\nax=sns.heatmap(matches)\\nplt.subplot(223).set_title(\"Differences\")\\nmatches= Y+predictions\\nax=sns.heatmap(matches)\\nplt.show(ax)',\n",
       "  u'def ComputeCost_CrossEntropy(P,Y,W):\\n    \\n    #instead of using function J = ComputeCost(X, Y, W, b, lambda)\\n    # and calculating P again lets use it from the from the prev. output\\n    \"\"\"\\n    P - Softmax predictions\\n    Y - Ground Truth\\n    \"\"\"\\n    #cross entropy loss - Handling -log0 tending to infinity\\n    #crossEntropyLoss =  -1*np.log(np.matmul(Y,P))\\n    D = Y.shape[1]\\n\\n    crossEntropyLoss = -1*(np.multiply(Y, np.log(P)) + np.multiply(1 - Y, np.log(1 - P)))\\n    #L2 regularization term\\n    L2_regularization_cost = lambd * np.sum(np.square(W)) \\n    Jacobian = np.sum(crossEntropyLoss)/float(D) + L2_regularization_cost\\n    cost = np.squeeze(Jacobian) # just to make sure the output is a value and not an array\\n    assert(Y.shape == (K,N))\\n    return cost',\n",
       "  u'cost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint(\"cost :\"+str(cost_CE))\\n\\ncost_Check=cost_1(P,Y)\\ncost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint cost_Check\\nprint cost_CE\\nprint cost_CE-cost_Check',\n",
       "  u'def ComputeAccuracy(P,Y):\\n    #instead of using function acc = ComputeAccuracy(X, y, W, b)\\n    # and calculating P again lets use it from the from the prev. output\\n    predictions=np.argmax(P,axis=0)\\n    groundtruth=np.argmax(Y,axis=0)\\n    matches=np.sum(predictions==groundtruth)\\n    total=len(predictions)\\n    accuracy=(matches/float(total))\\n    return accuracy',\n",
       "  u'accuracy=ComputeAccuracy(P,Y)\\nprint (\"Accuracy :\"+ str(accuracy))',\n",
       "  u'def ComputeGradients(X, Y, P, W, lambd):\\n    \"\"\" \\n    G -- dJ/dZ - g for Batch\\n    Z -- WX\\n    grad_W1 -- dJ/dW\\n    grad_b -- dJ/db\\n    grad_L -- dJ/dL Regularization term\\n    grad_W -- dJ/dW + Regularization term\\n\\n    lambd - lambda, to differ from the keyword itself\\n    \"\"\"\\n    N=float(X.shape[1])\\n    G= -1*(Y-P)\\n    grad_W1= (1/N)*np.dot(G,X.T)\\n    grad_L = 2*lambd*W\\n    grad_W = grad_W1+grad_L\\n    grad_b = (1/N)*np.dot(G,np.ones((int(N),1)))\\n    grad_b = np.sum(G, axis=1, keepdims=True)/ N\\n    \\n    print grad_b.shape\\n    assert(grad_W.shape == (K,d))\\n    assert(grad_b.shape == (K,1))\\n    \\n    return grad_W, grad_b',\n",
       "  u'grad_W,grad_b=ComputeGradients(X,Y,P,W,lambd)',\n",
       "  u'def ComputeGradsNum(X, Y, W, b):\\n#function [grad_b, grad_W] = ComputeGradsNum(X, Y, W, b, lambda, h) \\n    h=float(1e-6)\\n    grad_W = np.zeros_like(W)\\n    grad_b = np.zeros_like(b)\\n    \\n    P=EvaluateClassifier(X,W,b)\\n    c = ComputeCost_CrossEntropy(P,Y,W)\\n    for i in range(len(b)):\\n        b_try = b\\n        b_try[i] = b_try[i] + h\\n        P=EvaluateClassifier(X,W,b_try)\\n        c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        grad_b[i] = (c2-c) / h\\n        \\n    for i,j in np.ndindex(W.shape):\\n        W_try = W\\n        W_try[i,j] = W_try[i,j] + h\\n        P=EvaluateClassifier(X,W_try,b)\\n        c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        grad_W[i,j] = (c2-c) / h\\n        \\n    return grad_W,grad_b\\n\\n\\ndef gradient_check(X,Y,W,b, epsilon=1e-6):\\n\\n    Wplus = W + epsilon                               \\n    Wminus = W - epsilon  \\n    P=EvaluateClassifier(X,Wplus,b)\\n    J_plus = ComputeCost_CrossEntropy(P,Y,Wplus)\\n    P=EvaluateClassifier(X,Wminus,b)\\n    J_minus = ComputeCost_CrossEntropy(P,Y,Wminus)\\n    \\n    gradapprox = (J_plus - J_minus) / (2 * epsilon)           \\n    gradWnum,gradbnum=ComputeGradsNum(X,Y,W,b)\\n    #gradapprox=grad_w_num\\n    grad,grad_b =ComputeGradients(X,Y,P,W,lambd)\\n    \\n    numerator = np.linalg.norm(grad - gradapprox)                      \\n    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)    \\n    difference = numerator / denominator                               \\n    \\n    if difference < 1e-6:\\n        print(\"The gradient is correct!\")\\n    else:\\n        print(\"The gradient is wrong!\")\\n        print(\"Variation of :\"+str(difference))\\n    \\n    return difference',\n",
       "  u'gradient_check(X,Y,W,b)',\n",
       "  u'def softmax1(x):\\n    try:\\n        e = np.exp(x)\\n        return e / np.sum(e, axis=0)\\n    except FloatingPointError:\\n        # What happens here is that we get some of the values in x that are too big\\n        # so the exp overflows. In that case we check column by column where the\\n        # problem is\\n        return softmax_column_by_column(x)\\n\\ndef softmax_column_by_column( x):\\n    cols = x.shape[1]\\n    res = np.finfo(float).eps * np.ones_like(x)\\n    for c in range(cols):\\n        try:\\n            e = np.exp(x[:, c])\\n            res[:, c] = e / np.sum(e, axis=0)\\n        except FloatingPointError:\\n            res[np.argmax(x[:, c]), :] = 1 - np.finfo(float).eps\\n    return res\\n\\ndef evaluate(input_data):\\n    \"\"\"\\n    input_data must be a matrix containing one input per column (dimensions: input_size * N)\\n\\n    output will be:\\n    - a matrix containing the probabilities of each class, one image per column \\n      (dimensions: output_size * N)\\n    - a row vector containing the numerical index of the class with the highest probability,\\n      one image per column (dimensions: 1 * N)\\n    \"\"\"\\n    X = np.reshape(input_data, (d, -1))\\n    probabilities = softmax1(np.dot(W, X) + b)\\n    predictions = np.argmax(probabilities, axis=0)\\n    return probabilities, predictions\\n\\ndef cost_1( probabilities, ground_truth):\\n    \"\"\"\\n    probabilities should be the same as the first output of the evaluate function\\n    (dimensions: output_size * N)\\n\\n    ground_truth is matrix containing the one-hot encoded true labels, one image per column\\n    (dimensions: output_size * N)\\n\\n    l is a scalar used as a weight in the regularization term\\n    \"\"\"\\n    N = probabilities.shape[1]\\n    probabilities = np.reshape(probabilities, (K, -1))\\n    ground_truth = np.reshape(ground_truth, (K, -1))\\n\\n    log_arg = np.multiply(ground_truth, probabilities).sum(axis=0)\\n    log_arg[log_arg == 0] = np.finfo(float).eps\\n\\n    return - np.log(log_arg).sum() / N + lambd * np.power(W, 2).sum()\\n\\ndef accuracy(predictions, ground_truth):\\n    \"\"\"\\n    predictions should be the same as the second output of the evaluate function\\n    (dimensions: N * 1)\\n\\n    ground_truth is matrix containing the one-hot encoded true labels, one image per column\\n    (dimensions: output_size * N)\\n    \"\"\"\\n    N = predictions.shape[0]\\n    ground_truth = np.argmax(ground_truth, axis=0)\\n    return np.sum(predictions == ground_truth) / N\\n\\ndef gradients_slides( input_data, probabilities, ground_truth):\\n    N = probabilities.shape[1]\\n\\n    grad_w = np.zeros_like(W)\\n    grad_b = np.zeros_like(b)\\n\\n    for i in range(N):\\n        x = input_data[:, i]\\n        y = ground_truth[:, i]\\n        p = probabilities[:, i]\\n\\n        # The actual formula, which I broke down for ease of debuggin\\n        # g = - np.dot(y, (np.diag(p) - np.outer(p, p))) / np.dot(y, p)\\n\\n        t1 = np.outer(p, p)\\n        t2 = np.dot(y, (np.diag(p) - t1))\\n        t3 = np.dot(y, p)\\n        if t3 == 0:\\n            t3 = np.finfo(float).eps\\n        g = - t2 / t3\\n\\n        grad_w += np.outer(g, x)\\n        grad_b += np.reshape(g, grad_b.shape)\\n\\n    grad_w = grad_w / N + 2 * lambd * W\\n    grad_b /= N\\n\\n    return grad_w, grad_b\\n\\ndef gradients_simple( input_data, probabilities, ground_truth):\\n    N = probabilities.shape[1]\\n\\n    grad_w = np.zeros_like(W)\\n    grad_b = np.zeros_like(b)\\n\\n    for i in range(N):\\n        x = input_data[:, i]\\n        y = ground_truth[:, i]\\n        p = probabilities[:, i]\\n\\n        g = p - y\\n\\n        grad_w += np.outer(g, x)\\n        grad_b += np.reshape(g, grad_b.shape)\\n\\n    grad_w = grad_w / N + 2 * lambd * W\\n    grad_b /= N\\n\\n    return grad_w, grad_b',\n",
       "  u'cost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint(\"cost :\"+str(cost_CE))\\n\\ncost_Check=cost_1(P,Y)\\ncost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint cost_Check\\nprint cost_CE\\nprint cost_CE-cost_Check',\n",
       "  u'#probs check\\n\\nprobabilities, predictions=evaluate(X)\\nprint np.sum(np.abs(probabilities-P))',\n",
       "  u'cost_Check=cost_1(P,Y)\\ncost_CE=ComputeCost_CrossEntropy(P,Y,W)\\nprint cost_Check\\nprint cost_CE\\nprint cost_CE-cost_Check',\n",
       "  u'#probs check\\n\\nprobabilities, predictions=evaluate(X)\\nprint np.sum(np.abs(probabilities-P))',\n",
       "  u'#probs check\\n\\nprobabilities, predictions=evaluate(X)\\nprint np.sum(np.abs(probabilities-P))',\n",
       "  u'def ComputeGradsNum(X, Y, W, b):\\n#function [grad_b, grad_W] = ComputeGradsNum(X, Y, W, b, lambda, h) \\n    h=float(1e-6)\\n    grad_W = np.zeros_like(W)\\n    grad_b = np.zeros_like(b)\\n    \\n    P=EvaluateClassifier(X,W,b)\\n    #c = ComputeCost_CrossEntropy(P,Y,W)\\n    c = cost_1(P,Y)\\n    \\n    for i in range(len(b)):\\n        b_try = b\\n        b_try[i] = b_try[i] + h\\n        P=EvaluateClassifier(X,W,b_try)\\n        #c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        c2 = cost_1(P,Y)\\n        grad_b[i] = (c2-c) / h\\n        \\n    for i,j in np.ndindex(W.shape):\\n        W_try = W\\n        W_try[i,j] = W_try[i,j] + h\\n        P=EvaluateClassifier(X,W_try,b)\\n        #change params for W and b\\n        #c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        c2 = cost_1(P,Y)\\n        grad_W[i,j] = (c2-c) / h\\n        \\n    return grad_W,grad_b\\n\\n\\ndef gradient_check(X,Y,W,b, epsilon=1e-6):\\n\\n    Wplus = W + epsilon                               \\n    Wminus = W - epsilon  \\n    P=EvaluateClassifier(X,Wplus,b)\\n    J_plus = ComputeCost_CrossEntropy(P,Y,Wplus)\\n    P=EvaluateClassifier(X,Wminus,b)\\n    J_minus = ComputeCost_CrossEntropy(P,Y,Wminus)\\n    \\n    gradapprox = (J_plus - J_minus) / (2 * epsilon)           \\n    gradWnum,gradbnum=ComputeGradsNum(X,Y,W,b)\\n    #gradapprox=grad_w_num\\n    grad,grad_b =ComputeGradients(X,Y,P,W,lambd)\\n    \\n    numerator = np.linalg.norm(grad - gradapprox)                      \\n    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)    \\n    difference = numerator / denominator                               \\n    \\n    if difference < 1e-6:\\n        print(\"The gradient is correct!\")\\n    else:\\n        print(\"The gradient is wrong!\")\\n        print(\"Variation of :\"+str(difference))\\n    \\n    return difference',\n",
       "  u'gradient_check(X,Y,W,b)',\n",
       "  u'globals',\n",
       "  u'globals()'],\n",
       " '_ii': u'gradient_check(X,Y,W,b)',\n",
       " '_iii': u'def ComputeGradsNum(X, Y, W, b):\\n#function [grad_b, grad_W] = ComputeGradsNum(X, Y, W, b, lambda, h) \\n    h=float(1e-6)\\n    grad_W = np.zeros_like(W)\\n    grad_b = np.zeros_like(b)\\n    \\n    P=EvaluateClassifier(X,W,b)\\n    #c = ComputeCost_CrossEntropy(P,Y,W)\\n    c = cost_1(P,Y)\\n    \\n    for i in range(len(b)):\\n        b_try = b\\n        b_try[i] = b_try[i] + h\\n        P=EvaluateClassifier(X,W,b_try)\\n        #c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        c2 = cost_1(P,Y)\\n        grad_b[i] = (c2-c) / h\\n        \\n    for i,j in np.ndindex(W.shape):\\n        W_try = W\\n        W_try[i,j] = W_try[i,j] + h\\n        P=EvaluateClassifier(X,W_try,b)\\n        #change params for W and b\\n        #c2 = ComputeCost_CrossEntropy(P,Y,W)\\n        c2 = cost_1(P,Y)\\n        grad_W[i,j] = (c2-c) / h\\n        \\n    return grad_W,grad_b\\n\\n\\ndef gradient_check(X,Y,W,b, epsilon=1e-6):\\n\\n    Wplus = W + epsilon                               \\n    Wminus = W - epsilon  \\n    P=EvaluateClassifier(X,Wplus,b)\\n    J_plus = ComputeCost_CrossEntropy(P,Y,Wplus)\\n    P=EvaluateClassifier(X,Wminus,b)\\n    J_minus = ComputeCost_CrossEntropy(P,Y,Wminus)\\n    \\n    gradapprox = (J_plus - J_minus) / (2 * epsilon)           \\n    gradWnum,gradbnum=ComputeGradsNum(X,Y,W,b)\\n    #gradapprox=grad_w_num\\n    grad,grad_b =ComputeGradients(X,Y,P,W,lambd)\\n    \\n    numerator = np.linalg.norm(grad - gradapprox)                      \\n    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)    \\n    difference = numerator / denominator                               \\n    \\n    if difference < 1e-6:\\n        print(\"The gradient is correct!\")\\n    else:\\n        print(\"The gradient is wrong!\")\\n        print(\"Variation of :\"+str(difference))\\n    \\n    return difference',\n",
       " '_oh': {7: Text(0.5,1,'Differences'),\n",
       "  15: 0.9999999999449815,\n",
       "  23: 0.9999999998899629,\n",
       "  24: <function globals>},\n",
       " '_sh': <module 'IPython.core.shadowns' from '/usr/local/lib/python2.7/dist-packages/IPython/core/shadowns.pyc'>,\n",
       " 'accuracy': <function __main__.accuracy>,\n",
       " 'ax': <matplotlib.axes._subplots.AxesSubplot at 0x7fdbb07dd690>,\n",
       " 'b': array([[ 0.00326642],\n",
       "        [ 0.01168153],\n",
       "        [ 0.00228038],\n",
       "        [-0.00795789],\n",
       "        [ 0.0011058 ],\n",
       "        [ 0.01967552],\n",
       "        [ 0.00884968],\n",
       "        [-0.01046011],\n",
       "        [-0.01412821],\n",
       "        [ 0.01225941]]),\n",
       " 'classes': array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog',\n",
       "        'horse', 'ship', 'truck'], dtype='|S10'),\n",
       " 'cost_1': <function __main__.cost_1>,\n",
       " 'cost_CE': 3.308936409770906,\n",
       " 'cost_Check': 2.3544753109271412,\n",
       " 'd': 3072,\n",
       " 'dlink': traitlets.traitlets.directional_link,\n",
       " 'docutils': <module 'ipywidgets.widgets.docutils' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/docutils.pyc'>,\n",
       " 'domwidget': <module 'ipywidgets.widgets.domwidget' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/domwidget.pyc'>,\n",
       " 'evaluate': <function __main__.evaluate>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7fdbdc790750>,\n",
       " 'fixed': ipywidgets.widgets.interaction.fixed,\n",
       " 'get_ipython': <function IPython.core.getipython.get_ipython>,\n",
       " 'grad_W': array([[ 0.00681905,  0.00595145,  0.00586157, ...,  0.00744352,\n",
       "          0.00794412,  0.00942591],\n",
       "        [-0.04190538, -0.03808042, -0.04272708, ..., -0.02631769,\n",
       "         -0.02824674, -0.02858021],\n",
       "        [-0.00696871, -0.0069026 , -0.01063849, ...,  0.00026863,\n",
       "          0.00142234,  0.00301506],\n",
       "        ...,\n",
       "        [ 0.02037964,  0.01805024,  0.01916518, ...,  0.00473338,\n",
       "          0.00372027,  0.00524338],\n",
       "        [ 0.03960648,  0.03848919,  0.03914022, ...,  0.02603839,\n",
       "          0.02615773,  0.02634293],\n",
       "        [-0.02176277, -0.0186952 , -0.0175728 , ..., -0.0151007 ,\n",
       "         -0.01591919, -0.01829469]]),\n",
       " 'grad_b': array([[ 0.03131199],\n",
       "        [-0.08762279],\n",
       "        [-0.0214475 ],\n",
       "        [-0.04696101],\n",
       "        [-0.02359083],\n",
       "        [ 0.00899232],\n",
       "        [ 0.0789843 ],\n",
       "        [ 0.00268612],\n",
       "        [ 0.07537403],\n",
       "        [-0.01772664]]),\n",
       " 'gradient_check': <function __main__.gradient_check>,\n",
       " 'gradients_simple': <function __main__.gradients_simple>,\n",
       " 'gradients_slides': <function __main__.gradients_slides>,\n",
       " 'handle_kernel': <function ipywidgets.register_comm_target>,\n",
       " 'index': 1,\n",
       " 'interact': <ipywidgets.widgets.interaction._InteractFactory at 0x7fdbdc6eda50>,\n",
       " 'interact_manual': <ipywidgets.widgets.interaction._InteractFactory at 0x7fdbdc6edad0>,\n",
       " 'interaction': <module 'ipywidgets.widgets.interaction' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/interaction.pyc'>,\n",
       " 'interactive': ipywidgets.widgets.interaction.interactive,\n",
       " 'interactive_output': <function ipywidgets.widgets.interaction.interactive_output>,\n",
       " 'jsdlink': <function ipywidgets.widgets.widget_link.jsdlink>,\n",
       " 'jslink': <function ipywidgets.widgets.widget_link.jslink>,\n",
       " 'label': (array([9]),),\n",
       " 'lambd': 0,\n",
       " 'link': traitlets.traitlets.link,\n",
       " 'load_ipython_extension': <function ipywidgets.load_ipython_extension>,\n",
       " 'matches': array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         1., 1., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 2., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [2., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 2., 1., 0., 2., 1., 1., 2., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 2.,\n",
       "         1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]]),\n",
       " 'mu': 0,\n",
       " 'np': <module 'numpy' from '/usr/local/lib/python2.7/dist-packages/numpy/__init__.pyc'>,\n",
       " 'num_px': 3072,\n",
       " 'num_test': 10000,\n",
       " 'num_train': 10000,\n",
       " 'num_valid': 10000,\n",
       " 'os': <module 'os' from '/usr/lib/python2.7/os.pyc'>,\n",
       " 'pickle': <module 'pickle' from '/usr/lib/python2.7/pickle.pyc'>,\n",
       " 'plt': <module 'matplotlib.pyplot' from '/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.pyc'>,\n",
       " 'pprint': <module 'pprint' from '/usr/lib/python2.7/pprint.pyc'>,\n",
       " 'predictions': array([6, 2, 0, 6, 6, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 8,\n",
       "        6, 8, 6, 6, 8, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 7, 6, 6, 6, 6, 2,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        2, 6, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6]),\n",
       " 'probabilities': array([[0.07209067, 0.0979767 , 0.15393732, 0.07842604, 0.10976513,\n",
       "         0.10585778, 0.08376613, 0.08237298, 0.08297619, 0.12006231,\n",
       "         0.08431677, 0.11256418, 0.11176894, 0.08444731, 0.10216372,\n",
       "         0.09812723, 0.1194847 , 0.07770115, 0.11322402, 0.08856322,\n",
       "         0.07029624, 0.07425575, 0.08005054, 0.09177115, 0.09962455,\n",
       "         0.10653722, 0.10869699, 0.09630745, 0.08926399, 0.11919663,\n",
       "         0.082439  , 0.11670764, 0.08577962, 0.06752323, 0.09824159,\n",
       "         0.09427278, 0.08043806, 0.09804789, 0.09180957, 0.08375431,\n",
       "         0.10158721, 0.07059231, 0.08309803, 0.07858455, 0.09539103,\n",
       "         0.08560433, 0.06647027, 0.08523642, 0.0703013 , 0.09538036,\n",
       "         0.0926386 , 0.10327795, 0.07379186, 0.08418377, 0.09264058,\n",
       "         0.10818986, 0.07716247, 0.08896602, 0.08618895, 0.11612719,\n",
       "         0.08862362, 0.09772362, 0.0785983 , 0.09430801, 0.07602262,\n",
       "         0.07409631, 0.08252751, 0.11594825, 0.11419598, 0.10108034,\n",
       "         0.09265031, 0.09303577, 0.11476181, 0.09537457, 0.0831714 ,\n",
       "         0.07485603, 0.10479661, 0.07125181, 0.07558112, 0.08115786,\n",
       "         0.08200582, 0.08591236, 0.07781023, 0.089772  , 0.08565577,\n",
       "         0.08555062, 0.07301309, 0.06581308, 0.08980083, 0.1207279 ,\n",
       "         0.08477662, 0.09014948, 0.06765082, 0.10723112, 0.12410015,\n",
       "         0.07984631, 0.09570685, 0.09591744, 0.0782247 , 0.07175234],\n",
       "        [0.07192099, 0.0613794 , 0.05488794, 0.07669821, 0.06989166,\n",
       "         0.07327056, 0.07125493, 0.06631238, 0.07458102, 0.07262292,\n",
       "         0.08368597, 0.07326414, 0.06401555, 0.08571028, 0.07481744,\n",
       "         0.06636074, 0.06388168, 0.07485236, 0.07355211, 0.08244937,\n",
       "         0.06621635, 0.07701118, 0.07683607, 0.07250779, 0.07615388,\n",
       "         0.05917747, 0.07079455, 0.0901032 , 0.06754085, 0.06118927,\n",
       "         0.06883841, 0.06166125, 0.08482804, 0.07852623, 0.07030247,\n",
       "         0.06172109, 0.0640426 , 0.07683382, 0.07525307, 0.074377  ,\n",
       "         0.07002712, 0.06118096, 0.06772734, 0.06200954, 0.06765117,\n",
       "         0.08844022, 0.07295146, 0.08484764, 0.06324236, 0.05035321,\n",
       "         0.09152591, 0.06317206, 0.08565418, 0.07293777, 0.08234966,\n",
       "         0.06376532, 0.07906889, 0.08650053, 0.07221053, 0.06919583,\n",
       "         0.06402684, 0.06935439, 0.06553527, 0.07634827, 0.07007899,\n",
       "         0.06609224, 0.07119139, 0.06035049, 0.08425723, 0.06756818,\n",
       "         0.07247003, 0.06270824, 0.06663826, 0.08317726, 0.07670241,\n",
       "         0.08243413, 0.06927627, 0.07517992, 0.07666657, 0.06810883,\n",
       "         0.0768079 , 0.08253193, 0.07757283, 0.07864721, 0.08528596,\n",
       "         0.09572834, 0.08243602, 0.07607146, 0.07819019, 0.05880992,\n",
       "         0.07401721, 0.0788128 , 0.06251874, 0.06531119, 0.06171673,\n",
       "         0.04931354, 0.06791611, 0.08748147, 0.06827427, 0.0779765 ],\n",
       "        [0.10694726, 0.14718901, 0.08926037, 0.10292546, 0.1168886 ,\n",
       "         0.11889924, 0.10268327, 0.1222581 , 0.12794453, 0.1040956 ,\n",
       "         0.10714539, 0.10429603, 0.09482269, 0.07475592, 0.10798491,\n",
       "         0.14827173, 0.11663461, 0.10956411, 0.11636341, 0.09508227,\n",
       "         0.11858261, 0.0879855 , 0.10629956, 0.11972511, 0.09559459,\n",
       "         0.11138909, 0.08911721, 0.12355905, 0.10350352, 0.11721051,\n",
       "         0.09886464, 0.12175015, 0.10079774, 0.11570536, 0.08697853,\n",
       "         0.10597046, 0.08610955, 0.11273911, 0.11090855, 0.10573149,\n",
       "         0.0877823 , 0.12267117, 0.10759811, 0.11816856, 0.09511965,\n",
       "         0.08647619, 0.12237653, 0.11582414, 0.12062432, 0.10784191,\n",
       "         0.11776387, 0.10233773, 0.1088641 , 0.09672013, 0.09794109,\n",
       "         0.14630058, 0.08910728, 0.09731427, 0.10040751, 0.12237394,\n",
       "         0.09437362, 0.11320733, 0.13577986, 0.09416696, 0.10565204,\n",
       "         0.13455688, 0.11079207, 0.10191148, 0.10454686, 0.11061158,\n",
       "         0.10829293, 0.09674187, 0.10626672, 0.08654673, 0.13009728,\n",
       "         0.10348024, 0.12626252, 0.11109578, 0.10461575, 0.10443213,\n",
       "         0.12363121, 0.09743153, 0.11403705, 0.11544736, 0.08895804,\n",
       "         0.10772301, 0.09908577, 0.09698949, 0.11962476, 0.09775829,\n",
       "         0.12425357, 0.12782586, 0.13345562, 0.11384856, 0.10059755,\n",
       "         0.08145096, 0.09528498, 0.0967143 , 0.11089418, 0.13065907],\n",
       "        [0.08267341, 0.06165728, 0.07678477, 0.0924543 , 0.0831973 ,\n",
       "         0.07127059, 0.08598445, 0.08411027, 0.0932157 , 0.07575741,\n",
       "         0.08728028, 0.06772136, 0.07821922, 0.09906289, 0.07898024,\n",
       "         0.07724193, 0.06379957, 0.08819815, 0.07718657, 0.09320875,\n",
       "         0.08979454, 0.06637923, 0.08921825, 0.07415383, 0.08247486,\n",
       "         0.0680385 , 0.07984087, 0.09515719, 0.07899999, 0.05941812,\n",
       "         0.08922195, 0.06577125, 0.08624013, 0.09526058, 0.07256227,\n",
       "         0.07203906, 0.09893465, 0.10373476, 0.09150298, 0.08727514,\n",
       "         0.08531126, 0.06925178, 0.08421485, 0.08160157, 0.08134631,\n",
       "         0.07162685, 0.0881608 , 0.09285701, 0.08068554, 0.06781964,\n",
       "         0.10370922, 0.07856941, 0.10484717, 0.07976495, 0.09130698,\n",
       "         0.06376582, 0.0818292 , 0.08900694, 0.07836336, 0.06735179,\n",
       "         0.09395095, 0.07155706, 0.08282965, 0.08766271, 0.06672794,\n",
       "         0.0700275 , 0.08060113, 0.06546423, 0.09746529, 0.08029639,\n",
       "         0.07924897, 0.11545182, 0.07593351, 0.10017236, 0.08139681,\n",
       "         0.09347061, 0.07418505, 0.09139516, 0.08890803, 0.09377817,\n",
       "         0.08282877, 0.09587234, 0.08547443, 0.09197316, 0.09640017,\n",
       "         0.09132423, 0.08765032, 0.10413215, 0.08434026, 0.07495474,\n",
       "         0.07227251, 0.08250909, 0.08049991, 0.08020802, 0.07548677,\n",
       "         0.07142337, 0.10514977, 0.0856526 , 0.08284417, 0.07696629],\n",
       "        [0.09371997, 0.11629281, 0.08198124, 0.09546283, 0.09006071,\n",
       "         0.08205312, 0.07376552, 0.0727293 , 0.08431454, 0.08623778,\n",
       "         0.09457281, 0.07143493, 0.07924331, 0.09120556, 0.08406702,\n",
       "         0.08410395, 0.07868095, 0.11224431, 0.07120303, 0.08074726,\n",
       "         0.07692215, 0.07305632, 0.10019953, 0.08762004, 0.08286382,\n",
       "         0.08744289, 0.09000358, 0.07497849, 0.09302457, 0.06648175,\n",
       "         0.09852023, 0.09263799, 0.07766201, 0.08813064, 0.08191876,\n",
       "         0.08120586, 0.09346132, 0.07576341, 0.08171126, 0.10839895,\n",
       "         0.07526421, 0.0733565 , 0.08680273, 0.06829515, 0.09231412,\n",
       "         0.08718067, 0.11390784, 0.09375429, 0.07530097, 0.07639899,\n",
       "         0.08008616, 0.10119197, 0.08399629, 0.08228131, 0.0959518 ,\n",
       "         0.06570985, 0.06929929, 0.09562742, 0.09134623, 0.08116807,\n",
       "         0.08283016, 0.0859285 , 0.09745006, 0.11712131, 0.09302748,\n",
       "         0.12450241, 0.08578963, 0.08303348, 0.0915593 , 0.08626336,\n",
       "         0.09171454, 0.06178316, 0.05566509, 0.08815902, 0.08707849,\n",
       "         0.08317284, 0.08966264, 0.09627571, 0.07913196, 0.07017946,\n",
       "         0.09036405, 0.0845916 , 0.09230964, 0.08124928, 0.08825308,\n",
       "         0.10171137, 0.07899674, 0.08071788, 0.09214667, 0.09513249,\n",
       "         0.09646468, 0.10416743, 0.0783703 , 0.10158886, 0.06870879,\n",
       "         0.08340768, 0.08112686, 0.07771106, 0.10399335, 0.08421835],\n",
       "        [0.09098207, 0.08086225, 0.06033815, 0.09269762, 0.0832606 ,\n",
       "         0.07861668, 0.08272885, 0.07500796, 0.07804159, 0.08639129,\n",
       "         0.08946655, 0.06446995, 0.06245822, 0.10023259, 0.08334499,\n",
       "         0.08395423, 0.05891102, 0.09108167, 0.0738081 , 0.0789897 ,\n",
       "         0.07080166, 0.08455787, 0.09765773, 0.08192455, 0.07662296,\n",
       "         0.07104926, 0.07452537, 0.08822768, 0.08221829, 0.06327854,\n",
       "         0.07537764, 0.06943929, 0.11315205, 0.09108793, 0.07136293,\n",
       "         0.07564244, 0.07123977, 0.07528511, 0.06648912, 0.08764217,\n",
       "         0.08712708, 0.06950234, 0.06832251, 0.07670134, 0.09127348,\n",
       "         0.07772999, 0.09487773, 0.09581031, 0.07269774, 0.05047007,\n",
       "         0.07984592, 0.06502927, 0.08336019, 0.06949012, 0.07727191,\n",
       "         0.07052051, 0.06303959, 0.09308432, 0.07550617, 0.07050258,\n",
       "         0.08660439, 0.07557305, 0.0832449 , 0.09800211, 0.09402075,\n",
       "         0.09594347, 0.08365102, 0.06092447, 0.07695915, 0.07671774,\n",
       "         0.07378625, 0.09241703, 0.06238288, 0.08127878, 0.06842941,\n",
       "         0.07427989, 0.07017569, 0.09372095, 0.0800331 , 0.05820438,\n",
       "         0.07162783, 0.07576718, 0.08360206, 0.06725345, 0.09168125,\n",
       "         0.0949283 , 0.07897016, 0.08903446, 0.08564293, 0.06527237,\n",
       "         0.07833389, 0.08137768, 0.07475721, 0.07849996, 0.07939219,\n",
       "         0.05710912, 0.08446209, 0.08103569, 0.08896371, 0.08978167],\n",
       "        [0.14957285, 0.14489908, 0.13668371, 0.14139878, 0.14576708,\n",
       "         0.11764481, 0.14442042, 0.16223943, 0.13833365, 0.13601849,\n",
       "         0.13013759, 0.15063149, 0.16434698, 0.12887175, 0.1499983 ,\n",
       "         0.13896064, 0.15503861, 0.14205242, 0.14074355, 0.1553334 ,\n",
       "         0.16382758, 0.14103024, 0.13499097, 0.12260793, 0.15573955,\n",
       "         0.15004576, 0.13918176, 0.11482198, 0.14842229, 0.1494857 ,\n",
       "         0.1439416 , 0.13842922, 0.15239074, 0.13803984, 0.17622087,\n",
       "         0.16883616, 0.187757  , 0.13147209, 0.15109156, 0.14930673,\n",
       "         0.16121419, 0.21427378, 0.15064338, 0.14902849, 0.14513512,\n",
       "         0.16592963, 0.16018038, 0.13159731, 0.19040053, 0.19304377,\n",
       "         0.130628  , 0.13770389, 0.12786075, 0.17709154, 0.14112491,\n",
       "         0.1562656 , 0.19195871, 0.13518359, 0.15733254, 0.11708082,\n",
       "         0.14601416, 0.15019289, 0.16926251, 0.14517456, 0.15588241,\n",
       "         0.13011085, 0.16917518, 0.15743827, 0.11593467, 0.15503415,\n",
       "         0.14565119, 0.13511783, 0.14846148, 0.14304996, 0.15995632,\n",
       "         0.13772464, 0.13716798, 0.15069798, 0.18152769, 0.16655682,\n",
       "         0.15198902, 0.13931575, 0.15780662, 0.13308332, 0.14557801,\n",
       "         0.11774713, 0.15549835, 0.1475611 , 0.11870319, 0.16177386,\n",
       "         0.12276078, 0.1221248 , 0.17370763, 0.13708037, 0.15213571,\n",
       "         0.20531915, 0.14431376, 0.15821931, 0.14696631, 0.14220455],\n",
       "        [0.11436872, 0.1082068 , 0.10924449, 0.11380776, 0.10730878,\n",
       "         0.12053319, 0.13707823, 0.11601199, 0.08784823, 0.08705532,\n",
       "         0.10582975, 0.11411382, 0.11874174, 0.12208565, 0.10911936,\n",
       "         0.11424428, 0.10278802, 0.09152115, 0.1163749 , 0.12684833,\n",
       "         0.13553724, 0.12703174, 0.10268571, 0.11199934, 0.11166366,\n",
       "         0.11120252, 0.10328248, 0.10050047, 0.11265416, 0.13282347,\n",
       "         0.12287208, 0.10921207, 0.12741232, 0.10221126, 0.12121099,\n",
       "         0.11476944, 0.10911425, 0.10270913, 0.10762558, 0.10828083,\n",
       "         0.11595401, 0.09896942, 0.11540496, 0.10283523, 0.12042159,\n",
       "         0.09533397, 0.10346485, 0.10325427, 0.12044014, 0.10308714,\n",
       "         0.11111838, 0.10404631, 0.11498715, 0.1081776 , 0.10749724,\n",
       "         0.12614506, 0.10395622, 0.10399296, 0.11168201, 0.10178203,\n",
       "         0.15974674, 0.12702179, 0.09918731, 0.09859509, 0.12401258,\n",
       "         0.10501698, 0.10514776, 0.10020645, 0.11281691, 0.10579708,\n",
       "         0.11355067, 0.12574436, 0.14730298, 0.12591628, 0.1068809 ,\n",
       "         0.1340082 , 0.10924581, 0.12570357, 0.1095633 , 0.14366947,\n",
       "         0.09251337, 0.11928248, 0.1112465 , 0.11321446, 0.10722751,\n",
       "         0.11217881, 0.119648  , 0.12319049, 0.11807792, 0.09125068,\n",
       "         0.12139437, 0.10359553, 0.11119187, 0.1136617 , 0.1238994 ,\n",
       "         0.11392198, 0.13458735, 0.09268009, 0.09927883, 0.09695648],\n",
       "        [0.13174589, 0.09287194, 0.13976698, 0.11622297, 0.10797085,\n",
       "         0.12247764, 0.10162217, 0.09482629, 0.11553722, 0.11572034,\n",
       "         0.10699236, 0.11389135, 0.1107601 , 0.09898524, 0.09622632,\n",
       "         0.09652219, 0.13635659, 0.10771673, 0.12518426, 0.10036786,\n",
       "         0.10507089, 0.14350055, 0.11171225, 0.12755302, 0.10860486,\n",
       "         0.13636989, 0.15674723, 0.13015924, 0.11511263, 0.11957498,\n",
       "         0.12711964, 0.12231437, 0.06765451, 0.12058963, 0.11277541,\n",
       "         0.12048169, 0.12602609, 0.12010768, 0.10489615, 0.10328702,\n",
       "         0.12171313, 0.12554921, 0.12619446, 0.13505787, 0.11189856,\n",
       "         0.11593245, 0.10449712, 0.09932061, 0.10771829, 0.13910127,\n",
       "         0.10201341, 0.13478832, 0.11338832, 0.1254172 , 0.12107728,\n",
       "         0.10902646, 0.14140913, 0.10910023, 0.1180501 , 0.13376886,\n",
       "         0.0974939 , 0.11501983, 0.10730285, 0.10429362, 0.12688094,\n",
       "         0.11745662, 0.10143474, 0.13146393, 0.10392275, 0.11287422,\n",
       "         0.1121776 , 0.09396903, 0.10421226, 0.09290928, 0.10534036,\n",
       "         0.11065084, 0.12793683, 0.10003018, 0.10858253, 0.11832302,\n",
       "         0.12456059, 0.1221055 , 0.10386026, 0.12437621, 0.10714894,\n",
       "         0.10230903, 0.11257   , 0.106211  , 0.11179741, 0.12385294,\n",
       "         0.12172744, 0.10825733, 0.12974341, 0.10809794, 0.10412303,\n",
       "         0.13857812, 0.10503256, 0.13182924, 0.12126249, 0.13123849],\n",
       "        [0.08597817, 0.08866473, 0.09711503, 0.08990604, 0.0858893 ,\n",
       "         0.10937639, 0.11669604, 0.12413131, 0.11720733, 0.11603855,\n",
       "         0.11057253, 0.12761275, 0.11562327, 0.1146428 , 0.11329771,\n",
       "         0.09221308, 0.10442426, 0.10506795, 0.09236006, 0.09840983,\n",
       "         0.10295073, 0.12519162, 0.1003494 , 0.11013724, 0.11065727,\n",
       "         0.0987474 , 0.08780997, 0.08618526, 0.10925971, 0.11134101,\n",
       "         0.09280479, 0.10207676, 0.10408284, 0.10292529, 0.10842618,\n",
       "         0.10506101, 0.08287671, 0.103307  , 0.11871215, 0.09194636,\n",
       "         0.09401949, 0.09465252, 0.10999363, 0.1277177 , 0.09944898,\n",
       "         0.1257457 , 0.07311302, 0.097498  , 0.09858881, 0.11650365,\n",
       "         0.09067054, 0.10988309, 0.10325   , 0.10393559, 0.09283855,\n",
       "         0.09031093, 0.10316922, 0.10122371, 0.10891261, 0.1206489 ,\n",
       "         0.08633561, 0.09442154, 0.08080929, 0.08432736, 0.08769427,\n",
       "         0.08219674, 0.10968958, 0.12325894, 0.09834187, 0.10375697,\n",
       "         0.11045751, 0.12303089, 0.11837501, 0.10341575, 0.10094661,\n",
       "         0.10592258, 0.09129061, 0.08464895, 0.09538997, 0.09558987,\n",
       "         0.10367145, 0.09718932, 0.09628037, 0.10498356, 0.10381126,\n",
       "         0.09079916, 0.11213154, 0.11027887, 0.10167584, 0.11046681,\n",
       "         0.10399893, 0.10118   , 0.0881045 , 0.09447228, 0.10983969,\n",
       "         0.11962979, 0.08641966, 0.09275881, 0.099298  , 0.09824627]]),\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7fdbdc790750>,\n",
       " 'register': <function ipywidgets.widgets.widget.register>,\n",
       " 'register_comm_target': <function ipywidgets.register_comm_target>,\n",
       " 'show_image': <function utils_assignment1.show_image>,\n",
       " 'sigma': 0.01,\n",
       " 'sns': <module 'seaborn' from '/usr/local/lib/python2.7/dist-packages/seaborn/__init__.pyc'>,\n",
       " 'softmax': <function __main__.softmax>,\n",
       " 'softmax1': <function __main__.softmax1>,\n",
       " 'softmax_column_by_column': <function __main__.softmax_column_by_column>,\n",
       " 'test_set_x': array([[0.61960784, 0.62352941, 0.64705882, ..., 0.48627451, 0.50588235,\n",
       "         0.43137255],\n",
       "        [0.92156863, 0.90588235, 0.90980392, ..., 0.69803922, 0.74901961,\n",
       "         0.78039216],\n",
       "        [0.61960784, 0.61960784, 0.54509804, ..., 0.03137255, 0.01176471,\n",
       "         0.02745098],\n",
       "        ...,\n",
       "        [0.07843137, 0.0745098 , 0.05882353, ..., 0.19607843, 0.20784314,\n",
       "         0.18431373],\n",
       "        [0.09803922, 0.05882353, 0.09019608, ..., 0.31372549, 0.31764706,\n",
       "         0.31372549],\n",
       "        [0.28627451, 0.38431373, 0.38823529, ..., 0.36862745, 0.22745098,\n",
       "         0.10196078]]),\n",
       " 'test_set_y': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.]]),\n",
       " 'test_x': array([[0.61960784, 0.92156863, 0.61960784, ..., 0.07843137, 0.09803922,\n",
       "         0.28627451],\n",
       "        [0.62352941, 0.90588235, 0.61960784, ..., 0.0745098 , 0.05882353,\n",
       "         0.38431373],\n",
       "        [0.64705882, 0.90980392, 0.54509804, ..., 0.05882353, 0.09019608,\n",
       "         0.38823529],\n",
       "        ...,\n",
       "        [0.48627451, 0.69803922, 0.03137255, ..., 0.19607843, 0.31372549,\n",
       "         0.36862745],\n",
       "        [0.50588235, 0.74901961, 0.01176471, ..., 0.20784314, 0.31764706,\n",
       "         0.22745098],\n",
       "        [0.43137255, 0.78039216, 0.02745098, ..., 0.18431373, 0.31372549,\n",
       "         0.10196078]]),\n",
       " 'test_y': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'train_set_x': array([[0.23137255, 0.16862745, 0.19607843, ..., 0.54901961, 0.32941176,\n",
       "         0.28235294],\n",
       "        [0.60392157, 0.49411765, 0.41176471, ..., 0.54509804, 0.55686275,\n",
       "         0.56470588],\n",
       "        [1.        , 0.99215686, 0.99215686, ..., 0.3254902 , 0.3254902 ,\n",
       "         0.32941176],\n",
       "        ...,\n",
       "        [0.27843137, 0.23529412, 0.29019608, ..., 0.26666667, 0.27058824,\n",
       "         0.26666667],\n",
       "        [0.98039216, 0.99607843, 0.82745098, ..., 0.84313725, 1.        ,\n",
       "         0.99607843],\n",
       "        [0.24313725, 0.23921569, 0.23529412, ..., 0.50980392, 0.50980392,\n",
       "         0.51372549]]),\n",
       " 'train_set_y': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'train_x': array([[0.23137255, 0.60392157, 1.        , ..., 0.27843137, 0.98039216,\n",
       "         0.24313725],\n",
       "        [0.16862745, 0.49411765, 0.99215686, ..., 0.23529412, 0.99607843,\n",
       "         0.23921569],\n",
       "        [0.19607843, 0.41176471, 0.99215686, ..., 0.29019608, 0.82745098,\n",
       "         0.23529412],\n",
       "        ...,\n",
       "        [0.54901961, 0.54509804, 0.3254902 , ..., 0.26666667, 0.84313725,\n",
       "         0.50980392],\n",
       "        [0.32941176, 0.55686275, 0.3254902 , ..., 0.27058824, 1.        ,\n",
       "         0.50980392],\n",
       "        [0.28235294, 0.56470588, 0.32941176, ..., 0.26666667, 0.99607843,\n",
       "         0.51372549]]),\n",
       " 'train_y': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 1., ..., 0., 0., 0.]]),\n",
       " 'trait_types': <module 'ipywidgets.widgets.trait_types' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/trait_types.pyc'>,\n",
       " 'update_params': <function utils_assignment1.update_params>,\n",
       " 'valid_set_x': array([[0.1372549 , 0.10588235, 0.09803922, ..., 0.6627451 , 0.65882353,\n",
       "         0.65882353],\n",
       "        [0.07843137, 0.07843137, 0.07058824, ..., 0.43529412, 0.38039216,\n",
       "         0.2       ],\n",
       "        [0.45490196, 0.45098039, 0.60784314, ..., 0.07058824, 0.32941176,\n",
       "         0.48627451],\n",
       "        ...,\n",
       "        [0.49803922, 0.54509804, 0.60784314, ..., 0.77254902, 0.75294118,\n",
       "         0.74901961],\n",
       "        [0.74509804, 0.78431373, 0.81568627, ..., 0.63921569, 0.71372549,\n",
       "         0.75294118],\n",
       "        [0.69411765, 0.68235294, 0.71372549, ..., 0.46666667, 0.49803922,\n",
       "         0.53333333]]),\n",
       " 'valid_set_y': array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'valid_x': array([[0.1372549 , 0.07843137, 0.45490196, ..., 0.49803922, 0.74509804,\n",
       "         0.69411765],\n",
       "        [0.10588235, 0.07843137, 0.45098039, ..., 0.54509804, 0.78431373,\n",
       "         0.68235294],\n",
       "        [0.09803922, 0.07058824, 0.60784314, ..., 0.60784314, 0.81568627,\n",
       "         0.71372549],\n",
       "        ...,\n",
       "        [0.6627451 , 0.43529412, 0.07058824, ..., 0.77254902, 0.63921569,\n",
       "         0.46666667],\n",
       "        [0.65882353, 0.38039216, 0.32941176, ..., 0.75294118, 0.71372549,\n",
       "         0.49803922],\n",
       "        [0.65882353, 0.2       , 0.48627451, ..., 0.74901961, 0.75294118,\n",
       "         0.53333333]]),\n",
       " 'valid_y': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'valuewidget': <module 'ipywidgets.widgets.valuewidget' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/valuewidget.pyc'>,\n",
       " 'version_info': (7, 4, 2, 'final', 0),\n",
       " 'widget': <module 'ipywidgets.widgets.widget' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget.pyc'>,\n",
       " 'widget_bool': <module 'ipywidgets.widgets.widget_bool' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_bool.pyc'>,\n",
       " 'widget_box': <module 'ipywidgets.widgets.widget_box' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_box.pyc'>,\n",
       " 'widget_button': <module 'ipywidgets.widgets.widget_button' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_button.pyc'>,\n",
       " 'widget_color': <module 'ipywidgets.widgets.widget_color' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_color.pyc'>,\n",
       " 'widget_controller': <module 'ipywidgets.widgets.widget_controller' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_controller.pyc'>,\n",
       " 'widget_core': <module 'ipywidgets.widgets.widget_core' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_core.pyc'>,\n",
       " 'widget_date': <module 'ipywidgets.widgets.widget_date' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_date.pyc'>,\n",
       " 'widget_description': <module 'ipywidgets.widgets.widget_description' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_description.pyc'>,\n",
       " 'widget_float': <module 'ipywidgets.widgets.widget_float' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_float.pyc'>,\n",
       " 'widget_int': <module 'ipywidgets.widgets.widget_int' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_int.pyc'>,\n",
       " 'widget_layout': <module 'ipywidgets.widgets.widget_layout' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_layout.pyc'>,\n",
       " 'widget_link': <module 'ipywidgets.widgets.widget_link' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_link.pyc'>,\n",
       " 'widget_media': <module 'ipywidgets.widgets.widget_media' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_media.pyc'>,\n",
       " 'widget_output': <module 'ipywidgets.widgets.widget_output' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_output.pyc'>,\n",
       " 'widget_selection': <module 'ipywidgets.widgets.widget_selection' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_selection.pyc'>,\n",
       " 'widget_selectioncontainer': <module 'ipywidgets.widgets.widget_selectioncontainer' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_selectioncontainer.pyc'>,\n",
       " 'widget_serialization': {'from_json': <function ipywidgets.widgets.widget._json_to_widget>,\n",
       "  'to_json': <function ipywidgets.widgets.widget._widget_to_json>},\n",
       " 'widget_string': <module 'ipywidgets.widgets.widget_string' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_string.pyc'>,\n",
       " 'widget_style': <module 'ipywidgets.widgets.widget_style' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/widget_style.pyc'>,\n",
       " 'widgets': <module 'ipywidgets.widgets' from '/usr/local/lib/python2.7/dist-packages/ipywidgets/widgets/__init__.pyc'>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
