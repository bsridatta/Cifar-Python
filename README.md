# Neural-Networks-from-Scratch

Multi-Layer and Recurrent Neural Networks from scratch using NumPy

## [Single Layer NN](Single%20Layer%20NN/Single_Layer_NN.pdf)
- Explore and prepare CIFAR-10
- Methods for layer initialization, forward pass
- Numerical gradient descent with finite difference method.
- Mini-Batch gradient descent
- Visualize weights and feature map
- Tuning and analysing learning rate and regularization (weight decay)

## [Two Layer NN](Two%20Layer%20NN/Two_layer_NN.pdf)
- Implement and analyse the effects of Cyclic learning rate
- Coarse to fine random search
- Model with 50% accuracy

## [K Layer NN with Batch Normalization](K%20Layer%20NN%20with%20Batch%20Normalization/K_Layer_NN_w_BN.pdf)
- Multi Layer implementation
- Analyse effects varying layers 3-9 
- Batch Norm implementation
- Larger coase-to-fine hyperparameter search

### [Character Level RNN](Character%20Level%20RNN/Char_RNN.pdf)
- Prepare text from Harry Potter and the Goblet of Fire
- Implement sequence generation layer and methods
- Implement forward and backward pass
- Train with AdaGrad
- Analyse evolution of generated text
- Generate paragraphs from the best model

Best viewed by downloading the PDFs.

